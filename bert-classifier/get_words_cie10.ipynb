{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Spanish language model if not already installed\n",
    "%pip install spacy nltk pandas\n",
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download Spanish stop words if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get Spanish stop words\n",
    "spanish_stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../csv_import_scripts/cie10-es-diagnoses.csv')\n",
    "\n",
    "# Create a dictionary to store words and their associated codes\n",
    "word_code_dict = {}\n",
    "\n",
    "# Process each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text and code from each row\n",
    "    text = str(row['description']).lower()\n",
    "    code = row['code']\n",
    "\n",
    "    # Clean text and split into words\n",
    "    words = re.findall(r'\\b[\\w-]+\\b', text)\n",
    "\n",
    "    # Add non-stop words and their codes to dictionary\n",
    "    for word in words:\n",
    "        if word not in spanish_stop_words:  # Only process if not a stop word\n",
    "            if word not in word_code_dict:\n",
    "                word_code_dict[word] = {'count': 0, 'codes': set()}\n",
    "            word_code_dict[word]['count'] += 1\n",
    "            word_code_dict[word]['codes'].add(code)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "result_df = pd.DataFrame([\n",
    "    {\n",
    "        'word': word,\n",
    "        'count': data['count'],\n",
    "        'codes': ', '.join(sorted(data['codes'])),\n",
    "        'num_codes': len(data['codes']),\n",
    "        'main_categories': ', '.join(sorted(set(code[0] for code in data['codes'])))\n",
    "    }\n",
    "    for word, data in word_code_dict.items()\n",
    "])\n",
    "\n",
    "# Sort by count in descending order\n",
    "result_df = result_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Total unique words found: {len(result_df)}\")\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Add lemmatization support\n",
    "# Load Spanish language model\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "# Add POS tag and dependency analysis\n",
    "result_df['pos_tag'] = result_df['word'].apply(lambda x: nlp(x)[0].pos_)\n",
    "result_df['spacy_lemma'] = result_df['word'].apply(lambda x: nlp(x)[0].lemma_)\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Add lemmatized words to result dataframe\n",
    "result_df['root_word'] = result_df['word'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "# Show updated dataframe\n",
    "result_df.to_csv('cie10_word_analysis.csv', index=False, encoding='utf-8')\n",
    "result_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
