{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/david/.pyenv/versions/3.12.8/lib/python3.12/site-packages (1.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# ====================\n",
    "#  CONFIGURACIÓN\n",
    "# ====================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class Config:\n",
    "    USE_FP16 = False\n",
    "    STRIDE = 128  # Overlap between windows\n",
    "    MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "    #MODEL_NAME = 'PlanTL-GOB-ES/bsc-bio-ehr-es'\n",
    "    #MODEL_NAME = 'IIC/bsc-bio-ehr-es-caresA'\n",
    "    #MODEL_NAME = 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es'\n",
    "    #MODEL_NAME = 'IIC/bert-base-spanish-wwm-cased-ctebmsp'\n",
    "    #MODEL_NAME = 'IIC/xlm-roberta-large-ehealth_kd'\n",
    "    THRESHOLD_TUNING_INTERVAL = 3  # Cada cuántas épocas ajustar umbrales\n",
    "    USE_FEATURE_PYRAMID = True # Usar Feature Pyramid Network\n",
    "    FEATURE_LAYER_WEIGHTS = [0.1, 0.3, 0.6]  # Pesos para las últimas 3 capas\n",
    "    CLASS_WEIGHT_SMOOTHING = 0.1  # Suavizado para pesos de clases\n",
    "    EARLY_STOP_PATIENCE = 50  # Número de épocas sin mejora para parar\n",
    "    IMPROVEMENT_MARGIN = 0.0005\n",
    "    MAX_LENGTH = 512 # Máxima longitud de secuencia, definida en el BERT pre-entrenado\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    VAL_BATCH_SIZE = 16\n",
    "    TEST_BATCH_SIZE = 32\n",
    "    EPOCHS = 1000\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    WARMUP_EPOCHS = 2\n",
    "    HIERARCHICAL_WEIGHTS = {'parent': 1.5, 'child': 1.0}\n",
    "    LEARNING_RATE = 2e-5 # 2e-2\n",
    "    DATA_PATHS = {\n",
    "        'train': 'codiesp_csvs/codiesp_D_source_train.csv',\n",
    "        'test': 'codiesp_csvs/codiesp_D_source_test.csv',\n",
    "        'val': 'codiesp_csvs/codiesp_D_source_validation.csv'\n",
    "    }\n",
    "    SAVE_TOKENIZER_PATH = 'snapshots/cie10_tokenizer'\n",
    "    SAVE_PATH = 'snapshots/best_hierarchical_model'\n",
    "    SAVE_STATE_PATH = 'snapshots/best_hierarchical_model_state.bin'\n",
    "    SAVE_MLB_PARENT_PATH = 'snapshots/best_hierarchical_model_mlb_parent'\n",
    "    SAVE_MLB_CHILD_PATH = 'snapshots/best_hierarchical_model_mlb_child'\n",
    "    THRESHOLDS = {'parent': 0.041, 'child': 0.12}\n",
    "    PRETRAIN_EPOCHS = 10\n",
    "    PRETRAIN_BATCH_SIZE = 8\n",
    "    PRETRAIN_DATA_PATH = '../csv_import_scripts/cie10-es-diagnoses-expanded.csv'\n",
    "    FORCE_NEW_MODEL = True\n",
    "    MLB_ALL_CODES=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PREPROCESAMIENTO\n",
    "# ====================\n",
    "\n",
    "def parse_code(code):\n",
    "    \"\"\"Divide el código en niveles jerárquicos\"\"\"\n",
    "    parts = code.upper().split('.')\n",
    "    hierarchy = []\n",
    "    if len(parts) >= 1:\n",
    "        parent = parts[0]  # Primera categoría (ej: S62)\n",
    "        hierarchy.append(parent)\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            child_part = parts[1]\n",
    "            child = f\"{parent}.{child_part}\"  # Segunda categoría (ej: S62.14S)\n",
    "            hierarchy.append(child)\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def calculate_mlb_classes():\n",
    "    if not Config.MLB_ALL_CODES:\n",
    "        return calculate_mlb_classes_from_training_data()\n",
    "\n",
    "    df = pd.read_csv(Config.PRETRAIN_DATA_PATH)\n",
    "    # Try to load saved MLBs first\n",
    "    try:\n",
    "        mlb_parent = joblib.load(Config.SAVE_MLB_PARENT_PATH)\n",
    "        mlb_child = joblib.load(Config.SAVE_MLB_CHILD_PATH)\n",
    "        print(\"Loaded saved MLBs\")\n",
    "        return mlb_parent, mlb_child\n",
    "    except:\n",
    "        print(\"Creating new MLBs\")\n",
    "\n",
    "        # Construir jerarquía de códigos\n",
    "        all_parents = set()\n",
    "        all_children = set()\n",
    "\n",
    "        for code in df['code']:\n",
    "            levels = parse_code(code)\n",
    "            if len(levels) >= 1: all_parents.add(levels[0])\n",
    "            if len(levels) >= 2: all_children.add(levels[1])\n",
    "\n",
    "        # Inicializar MLB\n",
    "        mlb_parent = MultiLabelBinarizer().fit([all_parents])\n",
    "        mlb_child = MultiLabelBinarizer().fit([all_children])\n",
    "\n",
    "        # Save MLBs\n",
    "        joblib.dump(mlb_parent, Config.SAVE_MLB_PARENT_PATH)\n",
    "        joblib.dump(mlb_child, Config.SAVE_MLB_CHILD_PATH)\n",
    "\n",
    "    print(f\"Padres: {len(mlb_parent.classes_)} - Hijos: {len(mlb_child.classes_)}\")\n",
    "    return mlb_parent, mlb_child\n",
    "\n",
    "def calculate_mlb_classes_from_training_data():\n",
    "    # Construir jerarquía de códigos\n",
    "    all_parents = set()\n",
    "    all_children = set()\n",
    "\n",
    "    for _, path in Config.DATA_PATHS.items():\n",
    "        df = pd.read_csv(path)\n",
    "        df['labels'].apply(eval)\n",
    "\n",
    "        for codes in df['labels'].apply(eval): # FIXME: unsafe eval\n",
    "            for code in codes:\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: all_parents.add(levels[0])\n",
    "                if len(levels) >= 2: all_children.add(levels[1])\n",
    "\n",
    "        # Inicializar MLB\n",
    "        mlb_parent = MultiLabelBinarizer().fit([all_parents])\n",
    "        mlb_child = MultiLabelBinarizer().fit([all_children])\n",
    "\n",
    "    print(f\"Padres: {len(mlb_parent.classes_)} - Hijos: {len(mlb_child.classes_)}\")\n",
    "    return mlb_parent, mlb_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PLT DE MÉTRICAS\n",
    "# ====================\n",
    "\n",
    "def plot_metrics():\n",
    "    # Load metrics\n",
    "    metrics_history = pd.read_csv('training_metrics.csv')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['loss'], label='Loss')\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['f1_micro'], label='F1 Micro')\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['f1_macro'], label='F1 Macro')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training Metrics Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MODELO JERÁRQUICO\n",
    "# ====================\n",
    "\n",
    "class HierarchicalBERT(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            self.bert = AutoModel.from_pretrained(Config.SAVE_PATH)\n",
    "            print(\"Loaded saved BERT model\")\n",
    "        except:\n",
    "            self.bert = AutoModel.from_pretrained(Config.MODEL_NAME)\n",
    "            print(\"Using default BERT model\")\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Clasificación padre\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "\n",
    "        # Clasificación hijo con contexto de padres\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "# Función de pérdida\n",
    "# ====================\n",
    "def hierarchical_loss(parent_logits, child_logits,\n",
    "                      parent_labels, child_labels):\n",
    "\n",
    "    loss_parent = torch.nn.BCEWithLogitsLoss()(parent_logits, parent_labels)\n",
    "    loss_child = torch.nn.BCEWithLogitsLoss()(child_logits, child_labels)\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padres: 983 - Hijos: 2462\n",
      "Loaded saved tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 603/603 [00:48<00:00, 12.40it/s, loss=5.05, lr=1.5e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 8.2307 | LR: 1.50E-05\n",
      "\n",
      "Example Validation Results:\n",
      "Expected parent: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent: ['A01', 'A02', 'A03', 'A04', 'A06', 'A08', 'A09', 'A15', 'A17', 'A18', 'A19', 'A23', 'A28', 'A31', 'A32', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A48', 'A49', 'A53', 'A54', 'A55', 'A60', 'A63', 'A64', 'A69', 'A74', 'A78', 'A79', 'A80', 'A86', 'A87', 'A90', 'A91', 'B00', 'B01', 'B02', 'B05', 'B06', 'B07', 'B08', 'B10', 'B15', 'B17', 'B18', 'B19', 'B20', 'B25', 'B26', 'B27', 'B30', 'B33', 'B34', 'B35', 'B37', 'B44', 'B45', 'B46', 'B48', 'B49', 'B55', 'B57', 'B58', 'B59', 'B65', 'B67', 'B69', 'B74', 'B81', 'B83', 'B85', 'B91', 'B95', 'B96', 'B97', 'B99', 'C02', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'C15', 'C16', 'C17', 'C18', 'C20', 'C21', 'C22', 'C25', 'C26', 'C30', 'C31', 'C32', 'C34', 'C38', 'C40', 'C41', 'C43', 'C44', 'C47', 'C48', 'C49', 'C50', 'C52', 'C53', 'C54', 'C55', 'C56', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C83', 'C84', 'C85', 'C88', 'C90', 'C91', 'C92', 'C94', 'C96', 'D09', 'D10', 'D11', 'D12', 'D13', 'D15', 'D16', 'D17', 'D18', 'D21', 'D22', 'D23', 'D24', 'D25', 'D27', 'D29', 'D30', 'D31', 'D32', 'D33', 'D35', 'D36', 'D37', 'D38', 'D3A', 'D40', 'D41', 'D43', 'D44', 'D46', 'D47', 'D48', 'D49', 'D50', 'D53', 'D56', 'D57', 'D58', 'D61', 'D62', 'D63', 'D64', 'D65', 'D66', 'D68', 'D69', 'D70', 'D72', 'D73', 'D74', 'D75', 'D76', 'D80', 'D84', 'D86', 'D89', 'E03', 'E04', 'E05', 'E06', 'E07', 'E10', 'E11', 'E13', 'E16', 'E21', 'E22', 'E23', 'E24', 'E27', 'E28', 'E29', 'E31', 'E43', 'E44', 'E46', 'E50', 'E51', 'E53', 'E54', 'E55', 'E56', 'E61', 'E63', 'E66', 'E70', 'E72', 'E73', 'E74', 'E75', 'E77', 'E78', 'E79', 'E80', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'F02', 'F05', 'F09', 'F10', 'F11', 'F12', 'F14', 'F17', 'F19', 'F20', 'F23', 'F25', 'F29', 'F30', 'F31', 'F32', 'F34', 'F39', 'F40', 'F41', 'F43', 'F45', 'F50', 'F60', 'F63', 'F65', 'F70', 'F71', 'F72', 'F73', 'F79', 'F80', 'F84', 'F90', 'F91', 'F98', 'F99', 'G00', 'G03', 'G04', 'G06', 'G08', 'G12', 'G20', 'G24', 'G25', 'G30', 'G31', 'G35', 'G36', 'G37', 'G40', 'G43', 'G44', 'G45', 'G47', 'G50', 'G51', 'G54', 'G56', 'G57', 'G58', 'G60', 'G61', 'G62', 'G70', 'G71', 'G72', 'G81', 'G82', 'G83', 'G89', 'G90', 'G91', 'G93', 'G95', 'G96', 'G97', 'H00', 'H01', 'H02', 'H04', 'H05', 'H10', 'H11', 'H15', 'H16', 'H17', 'H18', 'H20', 'H21', 'H25', 'H26', 'H27', 'H30', 'H31', 'H33', 'H34', 'H35', 'H40', 'H43', 'H44', 'H46', 'H47', 'H49', 'H50', 'H51', 'H52', 'H53', 'H54', 'H55', 'H57', 'H59', 'H60', 'H66', 'H80', 'H90', 'H91', 'H92', 'H93', 'I05', 'I07', 'I08', 'I10', 'I11', 'I12', 'I15', 'I16', 'I20', 'I21', 'I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I30', 'I31', 'I33', 'I34', 'I35', 'I37', 'I38', 'I42', 'I43', 'I44', 'I45', 'I46', 'I47', 'I48', 'I49', 'I50', 'I51', 'I60', 'I61', 'I62', 'I63', 'I65', 'I66', 'I67', 'I69', 'I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I80', 'I81', 'I82', 'I83', 'I85', 'I86', 'I87', 'I88', 'I89', 'I95', 'I96', 'I99', 'J00', 'J01', 'J02', 'J03', 'J06', 'J10', 'J11', 'J12', 'J15', 'J16', 'J18', 'J21', 'J22', 'J30', 'J32', 'J33', 'J34', 'J35', 'J38', 'J39', 'J42', 'J43', 'J44', 'J45', 'J47', 'J60', 'J62', 'J64', 'J69', 'J80', 'J81', 'J82', 'J84', 'J85', 'J86', 'J90', 'J91', 'J93', 'J94', 'J95', 'J96', 'J98', 'J99', 'K00', 'K01', 'K02', 'K04', 'K05', 'K06', 'K08', 'K09', 'K11', 'K12', 'K13', 'K14', 'K20', 'K21', 'K22', 'K25', 'K26', 'K27', 'K29', 'K30', 'K31', 'K35', 'K36', 'K37', 'K38', 'K40', 'K42', 'K43', 'K44', 'K46', 'K50', 'K51', 'K52', 'K55', 'K56', 'K57', 'K59', 'K60', 'K61', 'K62', 'K63', 'K64', 'K65', 'K66', 'K68', 'K70', 'K71', 'K72', 'K73', 'K74', 'K75', 'K76', 'K80', 'K81', 'K82', 'K83', 'K85', 'K86', 'K90', 'K91', 'K92', 'L00', 'L02', 'L03', 'L08', 'L10', 'L11', 'L20', 'L21', 'L25', 'L27', 'L28', 'L29', 'L30', 'L40', 'L42', 'L43', 'L50', 'L51', 'L52', 'L53', 'L57', 'L58', 'L60', 'L65', 'L68', 'L70', 'L71', 'L72', 'L74', 'L76', 'L80', 'L81', 'L83', 'L85', 'L88', 'L89', 'L90', 'L91', 'L92', 'L93', 'L94', 'L95', 'L97', 'L98', 'M00', 'M04', 'M05', 'M06', 'M08', 'M13', 'M15', 'M16', 'M17', 'M19', 'M21', 'M24', 'M25', 'M26', 'M27', 'M30', 'M31', 'M32', 'M33', 'M34', 'M35', 'M41', 'M43', 'M45', 'M46', 'M47', 'M48', 'M50', 'M51', 'M53', 'M54', 'M60', 'M61', 'M62', 'M65', 'M67', 'M70', 'M71', 'M72', 'M75', 'M79', 'M80', 'M81', 'M83', 'M84', 'M85', 'M86', 'M87', 'M88', 'M89', 'M92', 'M94', 'M95', 'M96', 'N00', 'N01', 'N02', 'N03', 'N04', 'N05', 'N11', 'N12', 'N13', 'N14', 'N15', 'N17', 'N18', 'N19', 'N20', 'N21', 'N23', 'N25', 'N26', 'N28', 'N29', 'N30', 'N31', 'N32', 'N34', 'N35', 'N36', 'N39', 'N40', 'N41', 'N42', 'N43', 'N44', 'N45', 'N46', 'N48', 'N49', 'N50', 'N52', 'N53', 'N60', 'N62', 'N63', 'N64', 'N76', 'N80', 'N81', 'N82', 'N83', 'N85', 'N89', 'N90', 'N91', 'N92', 'N93', 'N94', 'N96', 'N97', 'O02', 'O03', 'O14', 'O21', 'O24', 'O30', 'O32', 'O36', 'O40', 'O41', 'O42', 'O45', 'O47', 'O60', 'O72', 'O75', 'O80', 'O92', 'O99', 'P07', 'P52', 'P81', 'P83', 'P84', 'P91', 'Q02', 'Q03', 'Q04', 'Q05', 'Q07', 'Q10', 'Q12', 'Q13', 'Q14', 'Q18', 'Q21', 'Q23', 'Q24', 'Q25', 'Q27', 'Q28', 'Q30', 'Q31', 'Q32', 'Q35', 'Q37', 'Q38', 'Q42', 'Q43', 'Q44', 'Q45', 'Q51', 'Q52', 'Q53', 'Q55', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64', 'Q67', 'Q68', 'Q69', 'Q70', 'Q71', 'Q74', 'Q75', 'Q76', 'Q78', 'Q79', 'Q82', 'Q83', 'Q85', 'Q87', 'Q89', 'Q90', 'Q98', 'R00', 'R01', 'R04', 'R05', 'R06', 'R07', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R22', 'R23', 'R25', 'R26', 'R27', 'R29', 'R30', 'R31', 'R32', 'R33', 'R34', 'R35', 'R36', 'R39', 'R40', 'R41', 'R42', 'R43', 'R44', 'R45', 'R46', 'R47', 'R48', 'R49', 'R50', 'R51', 'R52', 'R53', 'R55', 'R56', 'R57', 'R58', 'R59', 'R60', 'R61', 'R62', 'R63', 'R64', 'R65', 'R68', 'R69', 'R71', 'R73', 'R74', 'R76', 'R78', 'R79', 'R80', 'R81', 'R82', 'R86', 'R89', 'R90', 'R91', 'R97', 'R99', 'S00', 'S01', 'S02', 'S03', 'S05', 'S06', 'S09', 'S10', 'S11', 'S19', 'S20', 'S21', 'S22', 'S23', 'S25', 'S27', 'S30', 'S31', 'S32', 'S34', 'S35', 'S36', 'S37', 'S39', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S50', 'S52', 'S53', 'S60', 'S61', 'S62', 'S63', 'S69', 'S70', 'S71', 'S72', 'S80', 'S81', 'S82', 'S83', 'S84', 'S85', 'S86', 'S89', 'S90', 'S91', 'S92', 'S93', 'S96', 'T07', 'T14', 'T17', 'T18', 'T19', 'T20', 'T21', 'T24', 'T30', 'T31', 'T36', 'T38', 'T48', 'T50', 'T56', 'T68', 'T74', 'T75', 'T78', 'T79', 'T80', 'T81', 'T82', 'T85', 'T86', 'V00', 'V19', 'V23', 'V29', 'V37', 'V87', 'V89', 'V99', 'W01', 'W16', 'W19', 'W21', 'W32', 'W34', 'W40', 'W55', 'W56', 'W57', 'W86', 'X08', 'X58', 'X73', 'Y09', 'Y84', 'Y95', 'Z16', 'Z17', 'Z20', 'Z21', 'Z37', 'Z3A', 'Z51', 'Z53', 'Z56', 'Z60', 'Z65', 'Z67', 'Z68', 'Z74', 'Z75', 'Z76', 'Z77', 'Z79', 'Z80', 'Z82', 'Z83', 'Z85', 'Z86', 'Z87', 'Z88', 'Z90', 'Z91', 'Z92', 'Z93', 'Z94', 'Z95', 'Z96', 'Z97', 'Z98', 'Z99']\n",
      "Expected child: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child: ['A01.00', 'A01.4', 'A02.9', 'A03.8', 'A04.5', 'A04.8', 'A15.0', 'A15.4', 'A15.6', 'A15.9', 'A17.81', 'A18.09', 'A18.10', 'A18.12', 'A18.15', 'A18.2', 'A18.4', 'A18.50', 'A23.9', 'A28.1', 'A31.9', 'A32.11', 'A32.9', 'A38.9', 'A40.3', 'A41.02', 'A41.1', 'A41.81', 'A41.9', 'A43.9', 'A44.0', 'A44.9', 'A48.1', 'A49.1', 'A49.8', 'A49.9', 'A53.9', 'A54.9', 'A63.0', 'A69.20', 'A74.9', 'A79.9', 'A80.9', 'A87.2', 'B00.9', 'B01.9', 'B02.21', 'B02.29', 'B02.39', 'B05.9', 'B06.9', 'B07.9', 'B08.3', 'B10.89', 'B15.9', 'B17.9', 'B18.2', 'B19.10', 'B19.20', 'B19.9', 'B25.9', 'B27.00', 'B27.90', 'B27.99', 'B30.0', 'B30.9', 'B33.3', 'B34.1', 'B34.8', 'B35.2', 'B35.9', 'B37.0', 'B37.2', 'B37.3', 'B37.81', 'B37.9', 'B44.9', 'B45.2', 'B45.9', 'B46.5', 'B48.8', 'B55.9', 'B57.2', 'B58.2', 'B58.9', 'B65.0', 'B65.9', 'B67.90', 'B67.99', 'B69.0', 'B69.9', 'B74.9', 'B83.0', 'B95.0', 'B95.2', 'B95.4', 'B95.5', 'B95.61', 'B95.62', 'B95.7', 'B95.8', 'B96.0', 'B96.1', 'B96.20', 'B96.3', 'B96.4', 'B96.5', 'B96.7', 'B96.81', 'B96.89', 'B97.0', 'B97.10', 'B97.12', 'B97.29', 'B97.4', 'B97.6', 'B97.7', 'B99.9', 'C02.1', 'C02.9', 'C04.9', 'C08.0', 'C10.9', 'C15.9', 'C16.0', 'C16.9', 'C17.1', 'C18.6', 'C18.7', 'C18.9', 'C21.0', 'C22.0', 'C22.1', 'C26.0', 'C31.2', 'C34.9', 'C34.90', 'C34.91', 'C40.20', 'C41.0', 'C41.1', 'C43.9', 'C44.311', 'C44.49', 'C48.0', 'C48.2', 'C49.21', 'C49.5', 'C49.A', 'C49.A2', 'C50.312', 'C50.519', 'C50.9', 'C50.91', 'C50.912', 'C50.919', 'C53.9', 'C54.1', 'C56.2', 'C60.1', 'C62.9', 'C62.90', 'C62.91', 'C62.92', 'C63.10', 'C63.8', 'C64.1', 'C64.2', 'C64.9', 'C65.9', 'C66.9', 'C67.2', 'C67.7', 'C67.9', 'C68.0', 'C69.32', 'C70.9', 'C71.0', 'C72.30', 'C74.90', 'C74.92', 'C75.0', 'C76.3', 'C77.2', 'C77.3', 'C77.8', 'C77.9', 'C78.00', 'C78.01', 'C78.02', 'C78.1', 'C78.39', 'C78.5', 'C78.6', 'C78.7', 'C78.89', 'C79.00', 'C79.02', 'C79.2', 'C79.31', 'C79.49', 'C79.51', 'C79.52', 'C79.70', 'C79.71', 'C79.72', 'C79.81', 'C79.82', 'C79.89', 'C79.9', 'C80.0', 'C80.1', 'C81.1', 'C83.0', 'C83.1', 'C83.30', 'C83.39', 'C83.70', 'C84.49', 'C84.A', 'C85.9', 'C85.90', 'C85.99', 'C88.4', 'C90.0', 'C90.3', 'C91.90', 'C92.10', 'C94.6', 'C96.6', 'D09.0', 'D10.30', 'D11.0', 'D12.1', 'D12.6', 'D15.1', 'D16.22', 'D16.5', 'D17.23', 'D17.9', 'D18.00', 'D18.03', 'D18.09', 'D18.1', 'D22.9', 'D23.11', 'D23.9', 'D24.1', 'D24.9', 'D29.1', 'D29.30', 'D29.31', 'D30.00', 'D30.01', 'D30.02', 'D30.3', 'D31.62', 'D31.9', 'D32.0', 'D32.9', 'D35.00', 'D35.02', 'D35.1', 'D36.11', 'D37.030', 'D37.8', 'D40.10', 'D40.11', 'D40.8', 'D41.00', 'D41.02', 'D43.1', 'D44.10', 'D44.12', 'D44.2', 'D44.6', 'D44.7', 'D47.1', 'D47.2', 'D47.3', 'D47.9', 'D48.0', 'D48.1', 'D49.0', 'D49.1', 'D49.2', 'D49.3', 'D49.4', 'D49.511', 'D49.512', 'D49.519', 'D49.59', 'D49.7', 'D49.89', 'D50.0', 'D50.9', 'D53.1', 'D57.1', 'D58.9', 'D61.818', 'D63.1', 'D64.9', 'D68.51', 'D68.59', 'D68.61', 'D68.9', 'D69.0', 'D69.2', 'D69.3', 'D69.41', 'D69.6', 'D69.9', 'D70.9', 'D72.0', 'D72.1', 'D72.810', 'D72.819', 'D72.821', 'D72.822', 'D72.829', 'D73.1', 'D73.3', 'D73.5', 'D73.89', 'D74.8', 'D75.89', 'D76.3', 'D80.1', 'D84.9', 'D86.9', 'D89.2', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E04.9', 'E05.00', 'E05.80', 'E05.90', 'E06.3', 'E07.9', 'E10.10', 'E10.9', 'E11.22', 'E11.319', 'E11.359', 'E11.42', 'E11.622', 'E11.628', 'E11.9', 'E13.9', 'E16.2', 'E21.0', 'E21.3', 'E21.5', 'E23.0', 'E23.2', 'E27.0', 'E27.8', 'E27.9', 'E29.1', 'E44.0', 'E44.1', 'E50.9', 'E51.9', 'E53.1', 'E53.8', 'E56.9', 'E63.9', 'E66.01', 'E66.3', 'E66.9', 'E72.01', 'E74.00', 'E74.04', 'E75.29', 'E75.5', 'E77.8', 'E78.00', 'E78.1', 'E78.5', 'E79.0', 'E80.20', 'E80.7', 'E83.119', 'E83.39', 'E83.41', 'E83.42', 'E83.51', 'E83.52', 'E83.59', 'E83.81', 'E85.4', 'E85.9', 'E86.0', 'E87.0', 'E87.1', 'E87.2', 'E87.3', 'E87.5', 'E87.6', 'E88.09', 'E88.9', 'E89.0', 'F02.80', 'F10.10', 'F10.20', 'F10.21', 'F10.23', 'F11.20', 'F12.10', 'F12.20', 'F14.10', 'F14.20', 'F17.200', 'F17.210', 'F17.290', 'F19.20', 'F19.21', 'F20.5', 'F25.0', 'F30.9', 'F31.9', 'F32.9', 'F34.1', 'F40.9', 'F41.8', 'F41.9', 'F43.20', 'F43.9', 'F45.29', 'F50.2', 'F60.3', 'F63.9', 'F65.3', 'F80.81', 'F84.0', 'F90.9', 'F98.8', 'G00.1', 'G03.9', 'G04.1', 'G04.90', 'G06.0', 'G06.2', 'G12.20', 'G12.21', 'G24.5', 'G25.0', 'G25.3', 'G30.9', 'G31.84', 'G40.109', 'G40.119', 'G40.309', 'G40.401', 'G40.409', 'G40.909', 'G44.209', 'G45.9', 'G47.00', 'G47.30', 'G47.32', 'G47.33', 'G47.9', 'G50.0', 'G51.0', 'G54.6', 'G56.92', 'G57.00', 'G57.01', 'G57.42', 'G58.9', 'G61.0', 'G62.9', 'G70.00', 'G71.8', 'G72.3', 'G72.49', 'G72.9', 'G81.14', 'G81.90', 'G81.91', 'G81.94', 'G82.20', 'G82.21', 'G83.14', 'G83.2', 'G83.9', 'G89.29', 'G90.2', 'G90.50', 'G90.511', 'G91.8', 'G91.9', 'G93.0', 'G93.2', 'G93.40', 'G93.5', 'G93.6', 'G93.9', 'G95.20', 'G95.9', 'G96.0', 'G97.2', 'H00.19', 'H01.009', 'H01.8', 'H02.40', 'H02.401', 'H02.402', 'H02.409', 'H02.411', 'H02.841', 'H02.843', 'H02.844', 'H02.846', 'H02.849', 'H02.9', 'H04.20', 'H04.202', 'H05.2', 'H05.20', 'H05.221', 'H05.231', 'H10.9', 'H11.001', 'H11.002', 'H11.42', 'H11.421', 'H11.422', 'H11.429', 'H11.439', 'H15.00', 'H15.032', 'H15.092', 'H16.0', 'H16.001', 'H16.012', 'H16.07', 'H16.071', 'H16.9', 'H17.12', 'H17.9', 'H18.20', 'H18.89', 'H18.891', 'H18.9', 'H20.052', 'H20.059', 'H20.9', 'H21.26', 'H21.309', 'H21.42', 'H21.50', 'H21.501', 'H21.509', 'H21.519', 'H21.542', 'H21.9', 'H25.1', 'H26.8', 'H26.9', 'H27.10', 'H30.14', 'H30.89', 'H30.93', 'H31.101', 'H31.103', 'H31.309', 'H31.32', 'H33.10', 'H33.101', 'H33.102', 'H33.19', 'H33.20', 'H33.21', 'H33.22', 'H35.00', 'H35.349', 'H35.50', 'H35.6', 'H35.61', 'H35.62', 'H35.81', 'H35.89', 'H35.9', 'H40.10', 'H40.11', 'H40.83', 'H40.9', 'H43.1', 'H43.12', 'H43.13', 'H44.00', 'H44.001', 'H44.002', 'H44.009', 'H44.139', 'H44.40', 'H44.511', 'H46.00', 'H46.10', 'H46.9', 'H47.091', 'H47.092', 'H47.10', 'H49.02', 'H49.11', 'H49.21', 'H49.22', 'H49.23', 'H49.9', 'H50.011', 'H50.012', 'H50.10', 'H50.112', 'H50.21', 'H50.22', 'H50.9', 'H52.1', 'H52.7', 'H53.14', 'H53.142', 'H53.143', 'H53.149', 'H53.15', 'H53.2', 'H53.30', 'H53.40', 'H53.462', 'H53.8', 'H53.9', 'H54.0', 'H54.2', 'H54.3', 'H54.61', 'H54.62', 'H54.7', 'H55.00', 'H55.01', 'H57.02', 'H57.03', 'H57.04', 'H57.1', 'H57.10', 'H57.11', 'H57.12', 'H57.13', 'H57.8', 'H57.9', 'H59.03', 'H59.033', 'H60.91', 'H80.90', 'H90.12', 'H90.5', 'H91.20', 'H91.3', 'H91.90', 'H91.91', 'H91.93', 'H92.01', 'H92.09', 'H93.11', 'H93.19', 'I05.0', 'I05.9', 'I11.9', 'I12.0', 'I12.9', 'I20.8', 'I20.9', 'I21.11', 'I21.19', 'I21.29', 'I21.3', 'I23.7', 'I24.9', 'I25.10', 'I25.2', 'I25.9', 'I26.99', 'I27.2', 'I28.1', 'I30.9', 'I31.2', 'I31.3', 'I31.4', 'I33.0', 'I34.0', 'I34.1', 'I35.0', 'I35.8', 'I35.9', 'I37.0', 'I42.0', 'I42.9', 'I44.2', 'I44.30', 'I45.10', 'I46.9', 'I47.1', 'I48.0', 'I48.2', 'I48.91', 'I48.92', 'I49.01', 'I49.3', 'I49.9', 'I50.9', 'I51.4', 'I51.7', 'I51.9', 'I60.9', 'I61.5', 'I61.8', 'I61.9', 'I62.00', 'I62.01', 'I62.03', 'I62.9', 'I63.512', 'I63.519', 'I63.9', 'I65.21', 'I65.8', 'I66.9', 'I67.1', 'I67.82', 'I69.354', 'I70.0', 'I70.90', 'I71.00', 'I71.2', 'I71.4', 'I72.9', 'I73.8', 'I74.8', 'I74.9', 'I75.81', 'I75.89', 'I77.0', 'I77.1', 'I77.6', 'I77.9', 'I78.1', 'I80.8', 'I82.0', 'I82.210', 'I82.220', 'I82.3', 'I82.40', 'I82.401', 'I82.402', 'I82.409', 'I82.411', 'I82.431', 'I82.439', 'I82.90', 'I83.90', 'I83.93', 'I85.00', 'I85.01', 'I86.1', 'I86.4', 'I87.1', 'I87.2', 'I87.8', 'I88.1', 'I88.9', 'I89.0', 'I89.1', 'I89.8', 'I95.9', 'I99.8', 'J01.90', 'J02.9', 'J03.90', 'J03.91', 'J11.0', 'J11.1', 'J12.0', 'J18.1', 'J18.9', 'J21.8', 'J21.9', 'J30.1', 'J32.0', 'J32.4', 'J32.9', 'J34.1', 'J34.3', 'J34.89', 'J35.1', 'J38.1', 'J38.4', 'J39.8', 'J43.9', 'J44.9', 'J45.909', 'J47.9', 'J62.8', 'J69.0', 'J81.0', 'J81.1', 'J84.01', 'J84.10', 'J84.89', 'J85.2', 'J86.0', 'J93.9', 'J94.0', 'J94.2', 'J94.8', 'J96.00', 'J96.90', 'J96.92', 'J98.01', 'J98.11', 'J98.2', 'J98.4', 'J98.51', 'J98.8', 'K00.0', 'K00.1', 'K01.0', 'K02.9', 'K04.7', 'K05.10', 'K06.1', 'K06.2', 'K06.8', 'K08.109', 'K08.89', 'K08.9', 'K09.0', 'K11.1', 'K11.20', 'K11.6', 'K11.7', 'K11.9', 'K12.0', 'K12.1', 'K12.2', 'K13.0', 'K13.21', 'K13.29', 'K13.70', 'K13.79', 'K14.5', 'K14.6', 'K14.9', 'K20.9', 'K21.9', 'K22.2', 'K22.70', 'K22.8', 'K22.9', 'K25.9', 'K26.4', 'K26.9', 'K27.9', 'K31.1', 'K31.5', 'K31.7', 'K31.819', 'K31.89', 'K31.9', 'K35.2', 'K35.80', 'K38.8', 'K40.20', 'K40.30', 'K40.90', 'K42.9', 'K44.9', 'K46.9', 'K50.00', 'K50.90', 'K51.00', 'K51.90', 'K52.3', 'K52.81', 'K52.82', 'K52.9', 'K55.049', 'K55.059', 'K55.1', 'K55.20', 'K55.21', 'K55.9', 'K56.1', 'K56.2', 'K56.41', 'K56.60', 'K56.69', 'K57.00', 'K57.10', 'K57.20', 'K57.32', 'K57.90', 'K57.92', 'K59.00', 'K59.39', 'K59.8', 'K59.9', 'K60.3', 'K62.1', 'K62.5', 'K62.6', 'K62.7', 'K62.89', 'K63.1', 'K63.2', 'K63.5', 'K63.9', 'K64.8', 'K64.9', 'K65.0', 'K65.1', 'K65.3', 'K65.8', 'K65.9', 'K66.0', 'K66.1', 'K66.8', 'K68.12', 'K68.19', 'K68.9', 'K70.10', 'K70.30', 'K70.9', 'K71.6', 'K72.00', 'K72.90', 'K74.0', 'K74.60', 'K75.0', 'K75.4', 'K75.9', 'K76.0', 'K76.6', 'K76.7', 'K76.81', 'K76.89', 'K76.9', 'K80.10', 'K80.20', 'K80.5', 'K80.50', 'K80.51', 'K81.0', 'K82.8', 'K83.0', 'K83.1', 'K83.3', 'K83.8', 'K85.20', 'K85.90', 'K86.0', 'K86.1', 'K86.3', 'K86.89', 'K86.9', 'K90.0', 'K90.9', 'K91.7', 'K92.0', 'K92.1', 'K92.2', 'L02.01', 'L02.211', 'L02.31', 'L02.41', 'L02.91', 'L03.115', 'L03.90', 'L08.9', 'L10.0', 'L11.9', 'L21.9', 'L25.9', 'L28.2', 'L29.0', 'L29.9', 'L30.9', 'L40.53', 'L40.9', 'L43.9', 'L51.1', 'L51.2', 'L51.9', 'L53.9', 'L57.0', 'L60.8', 'L60.9', 'L65.9', 'L68.0', 'L71.9', 'L74.0', 'L76.3', 'L81.9', 'L85.8', 'L85.9', 'L89.150', 'L90.5', 'L90.8', 'L91.0', 'L92.9', 'L93.0', 'L97.909', 'L97.919', 'L98.0', 'L98.49', 'L98.8', 'L98.9', 'M00.071', 'M00.9', 'M05.9', 'M06.9', 'M08.90', 'M08.961', 'M08.99', 'M13.0', 'M15.9', 'M16.0', 'M19.90', 'M21.959', 'M24.60', 'M24.641', 'M24.642', 'M25.40', 'M25.451', 'M25.50', 'M25.511', 'M25.512', 'M25.519', 'M25.531', 'M25.532', 'M25.541', 'M25.551', 'M25.552', 'M25.561', 'M25.562', 'M25.569', 'M25.571', 'M25.60', 'M25.9', 'M26.02', 'M26.04', 'M26.09', 'M26.30', 'M26.32', 'M26.601', 'M26.609', 'M26.619', 'M26.9', 'M27.2', 'M30.3', 'M31.0', 'M31.1', 'M31.6', 'M32.9', 'M35.3', 'M41.9', 'M43.6', 'M45.9', 'M46.40', 'M46.47', 'M46.90', 'M46.96', 'M47.895', 'M47.896', 'M47.9', 'M48.00', 'M48.04', 'M51.24', 'M54.2', 'M54.40', 'M54.41', 'M54.5', 'M54.6', 'M54.9', 'M60.059', 'M60.9', 'M61.10', 'M62.50', 'M62.81', 'M62.82', 'M62.838', 'M62.89', 'M65.9', 'M71.30', 'M75.90', 'M79.1', 'M79.2', 'M79.60', 'M79.604', 'M79.605', 'M79.621', 'M79.641', 'M79.643', 'M79.644', 'M79.645', 'M79.652', 'M79.659', 'M79.671', 'M79.672', 'M79.673', 'M79.7', 'M80.08X', 'M81.0', 'M83.9', 'M84.30X', 'M84.453', 'M84.529', 'M85.2', 'M85.50', 'M85.60', 'M85.8', 'M85.80', 'M86.60', 'M86.68', 'M86.9', 'M87.80', 'M88.832', 'M88.9', 'M89.072', 'M89.549', 'M89.59', 'M89.70', 'M89.8X', 'M89.8X9', 'M89.9', 'M94.359', 'N02.8', 'N03.9', 'N04.9', 'N05.5', 'N05.7', 'N05.9', 'N11.9', 'N13.30', 'N13.5', 'N13.6', 'N13.70', 'N13.8', 'N13.9', 'N17.0', 'N17.9', 'N18.2', 'N18.3', 'N18.4', 'N18.5', 'N18.6', 'N18.9', 'N20.0', 'N20.1', 'N20.9', 'N25.81', 'N26.1', 'N26.9', 'N28.0', 'N28.1', 'N28.82', 'N28.89', 'N28.9', 'N30.8', 'N30.90', 'N31.9', 'N32.0', 'N32.1', 'N32.3', 'N32.89', 'N32.9', 'N34.2', 'N36.0', 'N36.8', 'N39.0', 'N39.44', 'N40.0', 'N40.2', 'N41.0', 'N41.1', 'N43.3', 'N44.00', 'N44.2', 'N44.8', 'N45.1', 'N45.2', 'N45.3', 'N45.4', 'N48.21', 'N48.29', 'N48.30', 'N48.6', 'N48.81', 'N48.89', 'N48.9', 'N49.2', 'N50.0', 'N50.1', 'N50.3', 'N50.811', 'N50.812', 'N50.819', 'N50.82', 'N50.89', 'N50.9', 'N52.9', 'N53.12', 'N60.01', 'N60.02', 'N60.11', 'N60.12', 'N60.19', 'N64.4', 'N64.89', 'N64.9', 'N80.8', 'N80.9', 'N83.00', 'N83.9', 'N85.8', 'N89.5', 'N90.89', 'N91.0', 'N92.1', 'N93.9', 'N94.89', 'N97.9', 'O02.0', 'O03.9', 'O21.0', 'O24.419', 'O32.1', 'O40.9XX0', 'O41.0', 'O41.00X', 'O41.1290', 'O42.912', 'O42.92', 'O47.9', 'O72.1', 'O75.9', 'O99.11', 'P52.3', 'P81.9', 'P91.60', 'Q03.1', 'Q03.9', 'Q04.0', 'Q05.9', 'Q07.00', 'Q07.8', 'Q10.0', 'Q12.4', 'Q13.0', 'Q13.4', 'Q21.1', 'Q23.1', 'Q24.8', 'Q24.9', 'Q27.30', 'Q27.9', 'Q28.2', 'Q32.4', 'Q35.3', 'Q37.9', 'Q38.2', 'Q42.9', 'Q44.6', 'Q44.7', 'Q45.0', 'Q51.0', 'Q51.3', 'Q51.811', 'Q52.0', 'Q53.10', 'Q55.29', 'Q60.6', 'Q61.02', 'Q61.19', 'Q61.3', 'Q61.4', 'Q62.8', 'Q63.1', 'Q63.9', 'Q64.4', 'Q67.0', 'Q68.0', 'Q70.9', 'Q75.0', 'Q75.3', 'Q76.49', 'Q78.2', 'Q79.1', 'Q85.00', 'Q85.01', 'Q85.1', 'Q85.8', 'Q85.9', 'Q87.2', 'Q87.3', 'Q87.81', 'Q89.2', 'Q98.4', 'R00.0', 'R00.1', 'R00.2', 'R01.1', 'R04.0', 'R04.2', 'R04.89', 'R06.00', 'R06.01', 'R06.09', 'R06.1', 'R06.4', 'R06.6', 'R06.82', 'R06.89', 'R07.0', 'R07.1', 'R07.2', 'R07.81', 'R07.89', 'R07.9', 'R09.02', 'R09.2', 'R10.0', 'R10.10', 'R10.12', 'R10.13', 'R10.2', 'R10.30', 'R10.31', 'R10.32', 'R10.33', 'R10.811', 'R10.812', 'R10.813', 'R10.814', 'R10.815', 'R10.816', 'R10.817', 'R10.819', 'R10.83', 'R10.84', 'R10.9', 'R11.0', 'R11.10', 'R11.12', 'R11.14', 'R11.2', 'R13.10', 'R14.0', 'R16.0', 'R16.1', 'R16.2', 'R18.0', 'R18.8', 'R19.0', 'R19.00', 'R19.01', 'R19.02', 'R19.03', 'R19.04', 'R19.06', 'R19.2', 'R19.5', 'R19.7', 'R19.8', 'R20.0', 'R20.1', 'R20.2', 'R20.8', 'R20.9', 'R22.0', 'R22.1', 'R22.2', 'R22.9', 'R23.0', 'R23.1', 'R23.3', 'R23.4', 'R23.8', 'R25.1', 'R25.2', 'R25.3', 'R26.0', 'R26.2', 'R26.81', 'R26.89', 'R26.9', 'R27.0', 'R27.8', 'R29.2', 'R29.6', 'R29.723', 'R29.810', 'R29.898', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R33.9', 'R35.0', 'R35.1', 'R35.8', 'R36.1', 'R39.12', 'R39.14', 'R39.15', 'R39.198', 'R39.89', 'R39.9', 'R40.0', 'R40.1', 'R40.20', 'R40.241', 'R40.2410', 'R40.2420', 'R40.2421', 'R40.243', 'R40.2430', 'R40.3', 'R40.4', 'R41.0', 'R41.3', 'R41.840', 'R41.843', 'R43.2', 'R43.9', 'R44.0', 'R44.1', 'R44.3', 'R44.9', 'R45.0', 'R45.1', 'R45.3', 'R45.4', 'R46.0', 'R47.01', 'R47.02', 'R47.1', 'R47.89', 'R47.9', 'R48.2', 'R49.0', 'R49.1', 'R50.81', 'R50.82', 'R50.9', 'R53.1', 'R53.81', 'R53.83', 'R56.00', 'R56.9', 'R57.0', 'R57.1', 'R57.9', 'R59.0', 'R59.9', 'R60.0', 'R60.1', 'R60.9', 'R63.0', 'R63.1', 'R63.3', 'R63.4', 'R65.20', 'R65.21', 'R68.83', 'R68.84', 'R68.89', 'R73.9', 'R74.0', 'R76.0', 'R76.11', 'R78.81', 'R79.82', 'R80.9', 'R82.2', 'R82.3', 'R91.1', 'R91.8', 'R97.1', 'R97.20', 'R97.8', 'S00.32X', 'S00.33X', 'S00.522', 'S00.532', 'S01.101', 'S01.20X', 'S01.501', 'S01.512', 'S01.80X', 'S01.90X', 'S01.95X', 'S02.0XX', 'S02.19X', 'S02.2XX', 'S02.5XX', 'S02.601', 'S02.652', 'S03.00X', 'S03.2XX', 'S05.8X2', 'S05.9', 'S05.92X', 'S06.0X0', 'S06.2X', 'S06.36', 'S06.9', 'S06.9X', 'S06.9X0', 'S09.90X', 'S09.92X', 'S11.80X', 'S11.90X', 'S11.91X', 'S19.9X', 'S20.92X', 'S22.39X', 'S22.4', 'S23.163', 'S27.329', 'S27.899', 'S30.0XX', 'S30.1XX', 'S30.23', 'S31.809', 'S32.019', 'S32.50', 'S32.502', 'S32.602', 'S32.9', 'S34.139', 'S35.229', 'S36.00X', 'S36.029', 'S36.09X', 'S37.009', 'S37.039', 'S37.812', 'S39.92', 'S39.94', 'S39.94X', 'S42.402', 'S42.409', 'S42.90X', 'S43.006', 'S43.40', 'S50.10X', 'S52.002', 'S52.102', 'S52.92X', 'S53.101', 'S60.229', 'S60.511', 'S61.209', 'S70.312', 'S71.101', 'S80.821', 'S91.051', 'S92.312', 'S93.402', 'T14.8', 'T14.90', 'T14.91', 'T18.3XX', 'T19.2XX', 'T20.00', 'T20.02X', 'T20.04X', 'T24.009', 'T24.011', 'T24.012', 'T30.0', 'T31.2', 'T56.0', 'T74.2', 'T79.7', 'T79.7XX', 'T81.30', 'T81.4', 'T81.49X', 'T82.01X', 'T82.7XX', 'T82.868', 'T85.41X', 'T86.11', 'T86.822', 'V00.121', 'V29.99X', 'V37.0', 'V87.9', 'V89.9XX', 'V99.XXX', 'W16.011', 'W19.XXX', 'W34.00', 'W34.00X', 'W55.03', 'W55.22X', 'W56.81X', 'W57.XXX', 'W86.8XX', 'X08.8XX', 'X58.XXX', 'Y84.2', 'Z16.11', 'Z16.19', 'Z16.24', 'Z16.35', 'Z17.0', 'Z20.818', 'Z37.0', 'Z3A.16', 'Z3A.18', 'Z3A.20', 'Z3A.24', 'Z3A.35', 'Z3A.38', 'Z3A.39', 'Z51.5', 'Z53.29', 'Z53.31', 'Z65.1', 'Z67.10', 'Z67.11', 'Z68.1', 'Z68.26', 'Z68.34', 'Z68.36', 'Z68.4', 'Z68.41', 'Z68.42', 'Z68.45', 'Z74.01', 'Z75.1', 'Z76.82', 'Z77.29', 'Z79.01', 'Z79.02', 'Z79.1', 'Z79.3', 'Z79.4', 'Z79.52', 'Z79.82', 'Z79.83', 'Z79.84', 'Z79.890', 'Z80.0', 'Z80.3', 'Z82.41', 'Z83.3', 'Z85.3', 'Z86.11', 'Z86.61', 'Z86.718', 'Z86.73', 'Z87.01', 'Z87.440', 'Z87.442', 'Z87.81', 'Z87.891', 'Z88.0', 'Z88.1', 'Z88.2', 'Z88.8', 'Z90.3', 'Z90.49', 'Z90.5', 'Z90.710', 'Z90.721', 'Z90.79', 'Z90.81', 'Z91.041', 'Z92.0', 'Z92.21', 'Z92.3', 'Z93.0', 'Z93.2', 'Z94.0', 'Z95.1', 'Z95.2', 'Z96.0', 'Z96.1', 'Z97.4', 'Z98.52', 'Z98.84', 'Z98.85', 'Z99.2', 'Z99.3', 'Z99.81', 'Z99.89']\n",
      "Parent Accuracy: 100.00% | Child Accuracy: 90.91%\n",
      "F1 Validation | Micro: 0.01498 | Macro: 0.01372 | Best: 0.01372 | Epochs without improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▌ | 515/603 [00:41<00:07, 12.43it/s, loss=5.32, lr=2.78e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 509\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    507\u001b[0m     best_thresholds \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mTHRESHOLDS\n\u001b[0;32m--> 509\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_thresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# Cargar datos de test\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mDATA_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 291\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(best_thresholds)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m Config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    293\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/amp/grad_scaler.py:380\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/optim/adamw.py:612\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    609\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_params, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 612\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v3 (Sliding Window)\n",
    "# ====================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  Modelo v3\n",
    "# ====================\n",
    "class HierarchicalBERTv2(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(Config.MODEL_NAME, output_hidden_states=True)\n",
    "        self.bert = AutoModel.from_pretrained(Config.MODEL_NAME, config=config)\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if Config.USE_FEATURE_PYRAMID:\n",
    "            # Combine last 3 layers' [CLS] embeddings\n",
    "            hidden_states = outputs.hidden_states[-3:]  # Get last 3 layers\n",
    "            # Stack [CLS] embeddings (shape: [3, batch_size, hidden_size])\n",
    "            pooled = torch.stack([state[:, 0] for state in hidden_states])\n",
    "            # Weighted combination of layers (weights should sum to 1)\n",
    "            if sum(Config.FEATURE_LAYER_WEIGHTS) != 1:\n",
    "                raise ValueError(\"FEATURE_LAYER_WEIGHTS must sum to 1\")\n",
    "\n",
    "            # Apply weights to layers\n",
    "            pooled = torch.einsum('lbd,l->bd', pooled,\n",
    "                                torch.tensor(Config.FEATURE_LAYER_WEIGHTS).to(pooled.device))\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Jerarquía de clasificación\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "#  FUNCIÓN DE PÉRDIDA MEJORADA\n",
    "# ====================\n",
    "def hierarchical_lossv2(parent_logits, child_logits,\n",
    "                     parent_labels, child_labels,\n",
    "                     parent_weights, child_weights):\n",
    "\n",
    "    loss_parent = F.binary_cross_entropy_with_logits(\n",
    "        parent_logits,\n",
    "        parent_labels,\n",
    "        pos_weight=parent_weights\n",
    "    )\n",
    "\n",
    "    loss_child = F.binary_cross_entropy_with_logits(\n",
    "        child_logits,\n",
    "        child_labels,\n",
    "        pos_weight=child_weights\n",
    "    )\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)\n",
    "\n",
    "# ====================\n",
    "#  AJUSTE DINÁMICO DE UMBRALES\n",
    "# ====================\n",
    "def calculate_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = {}\n",
    "    for i in range(y_probs.shape[1]):\n",
    "        if np.sum(y_true[:, i]) > 0:  # Solo clases presentes\n",
    "            precision, recall, threshs = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_idx = np.nanargmax(f1_scores)\n",
    "            thresholds[i] = threshs[best_idx]\n",
    "    return thresholds\n",
    "\n",
    "# ====================\n",
    "#  DATASET v3 (Sliding Window)\n",
    "# ====================\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        # Same label processing as before\n",
    "        for codes in df['labels'].apply(eval): # FIXME: unsafe eval\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        # Generate sliding windows for each text\n",
    "        for idx, text in enumerate(self.texts):\n",
    "            # Tokenize whole text\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                truncation=False,\n",
    "                return_offsets_mapping=True,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            # Generate sliding windows\n",
    "            window_size = Config.MAX_LENGTH - 2  # Account for [CLS] and [SEP]\n",
    "            stride = Config.STRIDE\n",
    "\n",
    "            for i in range(0, len(tokens['input_ids']), stride):\n",
    "                # Extract window\n",
    "                window_start = i\n",
    "                window_end = min(i + window_size, len(tokens['input_ids']))\n",
    "\n",
    "                # Add special tokens\n",
    "                input_ids = (\n",
    "                    [self.tokenizer.cls_token_id] +\n",
    "                    tokens['input_ids'][window_start:window_end] +\n",
    "                    [self.tokenizer.sep_token_id]\n",
    "                )\n",
    "\n",
    "                attention_mask = [1] * len(input_ids)\n",
    "\n",
    "                # Pad if necessary\n",
    "                padding_length = Config.MAX_LENGTH - len(input_ids)\n",
    "                if padding_length > 0:\n",
    "                    input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "                    attention_mask += [0] * padding_length\n",
    "\n",
    "                self.examples.append({\n",
    "                    'input_ids': torch.tensor(input_ids),\n",
    "                    'attention_mask': torch.tensor(attention_mask),\n",
    "                    'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                    'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "                    'text_id': idx  # To group windows later\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO con Sliding Window\n",
    "# ====================\n",
    "def train(best_thresholds=Config.THRESHOLDS):\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERTv2(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.path.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "        else:\n",
    "            print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "\n",
    "    # Calcular pesos de clases\n",
    "    parent_counts = np.sum(train_dataset.parent_labels, axis=0)\n",
    "    parent_weights = (len(train_dataset) - parent_counts) / (parent_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    parent_weights = torch.tensor(parent_weights).to(device)\n",
    "\n",
    "    child_counts = np.sum(train_dataset.child_labels, axis=0)\n",
    "    child_weights = (len(train_dataset) - child_counts) / (child_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    child_weights = torch.tensor(child_weights).to(device)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Ajuste periódico de umbrales\n",
    "        if (epoch + 1) % Config.THRESHOLD_TUNING_INTERVAL == 0:\n",
    "            val_probs, val_labels = get_validation_probabilities(model, val_loader, device)\n",
    "\n",
    "            # Calcular mejores umbrales por clase\n",
    "            parent_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['parent'], val_probs['parent']\n",
    "            )\n",
    "            child_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['child'], val_probs['child']\n",
    "            )\n",
    "\n",
    "            # Actualizar umbrales globales\n",
    "            best_thresholds['parent'] = np.mean(list(parent_thresholds.values()))\n",
    "            best_thresholds['child'] = np.mean(list(child_thresholds.values()))\n",
    "            print(f\"Nuevos umbrales: Parent={best_thresholds['parent']:.3f}, Child={best_thresholds['child']:.3f}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        text_predictions = defaultdict(lambda: {'parent': [], 'child': []})\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                parent_logits, child_logits, _ = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask']\n",
    "                )\n",
    "\n",
    "                # Store predictions by original text\n",
    "                for i, text_id in enumerate(batch['text_id'].cpu().numpy()):\n",
    "                    text_predictions[text_id]['parent'].append(parent_logits[i])\n",
    "                    text_predictions[text_id]['child'].append(child_logits[i])\n",
    "\n",
    "                # Immediate window-level loss\n",
    "                loss = hierarchical_lossv2(\n",
    "                    parent_logits,\n",
    "                    child_logits,\n",
    "                    batch['parent_labels'],  # Use batch labels directly\n",
    "                    batch['child_labels'],   # Not the dataset's labels\n",
    "                    parent_weights,\n",
    "                    child_weights\n",
    "                )\n",
    "\n",
    "            # Backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # After epoch completes, calculate aggregated loss\n",
    "        agg_loss = 0\n",
    "        for text_id in text_predictions:\n",
    "            if text_id >= len(train_dataset.parent_labels):\n",
    "                continue  # Skip invalid text_ids\n",
    "\n",
    "            # Aggregate predictions\n",
    "            parent_agg = torch.stack(text_predictions[text_id]['parent']).max(dim=0)[0]\n",
    "            child_agg = torch.stack(text_predictions[text_id]['child']).max(dim=0)[0]\n",
    "\n",
    "            # Get true labels from dataset\n",
    "            parent_label = torch.FloatTensor(train_dataset.parent_labels[text_id]).to(device)\n",
    "            child_label = torch.FloatTensor(train_dataset.child_labels[text_id]).to(device)\n",
    "\n",
    "            # Calculate aggregated loss\n",
    "            agg_loss += hierarchical_lossv2(\n",
    "                parent_agg.unsqueeze(0),\n",
    "                child_agg.unsqueeze(0),\n",
    "                parent_label.unsqueeze(0),\n",
    "                child_label.unsqueeze(0),\n",
    "                parent_weights,\n",
    "                child_weights\n",
    "            )\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss += agg_loss.item() / len(text_predictions)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device, mlb_parent, mlb_child, best_thresholds)\n",
    "\n",
    "        # Store metrics\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if epoch == 0:\n",
    "            best_f1 = loss_metric\n",
    "\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN):\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_3\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'model': \"v3\",\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'parent_threshold': best_thresholds['parent'],\n",
    "            'child_threshold': best_thresholds['child']\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "# ====================\n",
    "#  FUNCIONES AUXILIARES (Con sliding windows)\n",
    "# ====================\n",
    "def get_validation_probabilities(model, dataloader, device):\n",
    "    model.eval()\n",
    "    text_predictions = defaultdict(lambda: {'parent': [], 'child': []})\n",
    "    parent_labels = {}\n",
    "    child_labels = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            text_ids = batch['text_id'].numpy()\n",
    "\n",
    "            # Store labels by text_id\n",
    "            for i, text_id in enumerate(text_ids):\n",
    "                if text_id not in parent_labels:\n",
    "                    parent_labels[text_id] = batch['parent_labels'][i].numpy()\n",
    "                    child_labels[text_id] = batch['child_labels'][i].numpy()\n",
    "\n",
    "            # Get predictions\n",
    "            p_logits, c_logits, _ = model(**inputs)\n",
    "\n",
    "            # Store predictions by text_id\n",
    "            for i, text_id in enumerate(text_ids):\n",
    "                text_predictions[text_id]['parent'].append(p_logits[i].cpu())\n",
    "                text_predictions[text_id]['child'].append(c_logits[i].cpu())\n",
    "\n",
    "    # Aggregate predictions per text\n",
    "    parent_probs, child_probs = [], []\n",
    "    final_parent_labels, final_child_labels = [], []\n",
    "\n",
    "    for text_id in text_predictions:\n",
    "        # Aggregate using max pooling (same as training)\n",
    "        parent_agg = torch.stack(text_predictions[text_id]['parent']).max(dim=0)[0]\n",
    "        child_agg = torch.stack(text_predictions[text_id]['child']).max(dim=0)[0]\n",
    "\n",
    "        parent_probs.append(torch.sigmoid(parent_agg).numpy())\n",
    "        child_probs.append(torch.sigmoid(child_agg).numpy())\n",
    "\n",
    "        # Get original labels\n",
    "        final_parent_labels.append(parent_labels[text_id])\n",
    "        final_child_labels.append(child_labels[text_id])\n",
    "\n",
    "    return {\n",
    "        'parent': np.array(parent_probs),\n",
    "        'child': np.array(child_probs)\n",
    "    }, {\n",
    "        'parent': np.array(final_parent_labels),\n",
    "        'child': np.array(final_child_labels)\n",
    "    }\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN Con sliding windows\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child, thresholds):\n",
    "    val_probs, val_labels = get_validation_probabilities(model, dataloader, device)\n",
    "\n",
    "    # Convert probabilities to predictions\n",
    "    parent_preds = (val_probs['parent'] > thresholds['parent']).astype(int)\n",
    "    child_preds = (val_probs['child'] > thresholds['child']).astype(int)\n",
    "\n",
    "    # Print example comparison\n",
    "    if len(val_labels['parent']) > 0:\n",
    "        idx = 0  # First example\n",
    "        parent_true = np.array(mlb_parent.classes_)[val_labels['parent'][idx].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds[idx].astype(bool)]\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[val_labels['child'][idx].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds[idx].astype(bool)]\n",
    "\n",
    "        print(\"\\nExample Validation Results:\")\n",
    "        print(f\"Expected parent: {sorted(parent_true)}\")\n",
    "        print(f\"Predicted parent: {sorted(parent_pred)}\")\n",
    "        print(f\"Expected child: {sorted(child_true)}\")\n",
    "        print(f\"Predicted child: {sorted(child_pred)}\")\n",
    "\n",
    "        common_parent = len(set(parent_true) & set(parent_pred))\n",
    "        common_child = len(set(child_true) & set(child_pred))\n",
    "        print(f\"Parent Accuracy: {common_parent/len(parent_true):.2%} | Child Accuracy: {common_child/len(child_true):.2%}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(val_labels['parent'], parent_preds, average='micro', zero_division=0),\n",
    "        'f1_macro_parent': f1_score(val_labels['parent'], parent_preds, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(val_labels['child'], child_preds, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(val_labels['child'], child_preds, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Weighted averages\n",
    "    total_weight = sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "    metrics['f1_micro'] = (Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "                          Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']) / total_weight\n",
    "\n",
    "    metrics['f1_macro'] = (Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "                          Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']) / total_weight\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device, thresholds):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > thresholds['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > thresholds['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    best_thresholds = Config.THRESHOLDS\n",
    "\n",
    "    train(best_thresholds)\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.path.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device, best_thresholds)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padres: 983 - Hijos: 2462\n",
      "Loaded saved tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s, loss=0.951, lr=1.5e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['A01', 'A02', 'A03', 'A04', 'A06', 'A08', 'A09', 'A15', 'A17', 'A18', 'A19', 'A23', 'A28', 'A31', 'A32', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A48', 'A49', 'A53', 'A54', 'A55', 'A60', 'A63', 'A64', 'A69', 'A74', 'A78', 'A79', 'A80', 'A86', 'A87', 'A90', 'A91', 'B00', 'B01', 'B02', 'B05', 'B06', 'B07', 'B08', 'B10', 'B15', 'B17', 'B18', 'B19', 'B20', 'B25', 'B26', 'B27', 'B30', 'B33', 'B34', 'B35', 'B37', 'B44', 'B45', 'B46', 'B48', 'B49', 'B55', 'B57', 'B58', 'B59', 'B65', 'B67', 'B69', 'B74', 'B81', 'B83', 'B85', 'B91', 'B95', 'B96', 'B97', 'B99', 'C02', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'C15', 'C16', 'C17', 'C18', 'C20', 'C21', 'C22', 'C25', 'C26', 'C30', 'C31', 'C32', 'C34', 'C38', 'C40', 'C41', 'C43', 'C44', 'C47', 'C48', 'C49', 'C50', 'C52', 'C53', 'C54', 'C55', 'C56', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C83', 'C84', 'C85', 'C88', 'C90', 'C91', 'C92', 'C94', 'C96', 'D09', 'D10', 'D11', 'D12', 'D13', 'D15', 'D16', 'D17', 'D18', 'D21', 'D22', 'D23', 'D24', 'D25', 'D27', 'D29', 'D30', 'D31', 'D32', 'D33', 'D35', 'D36', 'D37', 'D38', 'D3A', 'D40', 'D41', 'D43', 'D44', 'D46', 'D47', 'D48', 'D49', 'D50', 'D53', 'D56', 'D57', 'D58', 'D61', 'D62', 'D63', 'D64', 'D65', 'D66', 'D68', 'D69', 'D70', 'D72', 'D73', 'D74', 'D75', 'D76', 'D80', 'D84', 'D86', 'D89', 'E03', 'E04', 'E05', 'E06', 'E07', 'E10', 'E11', 'E13', 'E16', 'E21', 'E22', 'E23', 'E24', 'E27', 'E28', 'E29', 'E31', 'E43', 'E44', 'E46', 'E50', 'E51', 'E53', 'E54', 'E55', 'E56', 'E61', 'E63', 'E66', 'E70', 'E72', 'E73', 'E74', 'E75', 'E77', 'E78', 'E79', 'E80', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'F02', 'F05', 'F09', 'F10', 'F11', 'F12', 'F14', 'F17', 'F19', 'F20', 'F23', 'F25', 'F29', 'F30', 'F31', 'F32', 'F34', 'F39', 'F40', 'F41', 'F43', 'F45', 'F50', 'F60', 'F63', 'F65', 'F70', 'F71', 'F72', 'F73', 'F79', 'F80', 'F84', 'F90', 'F91', 'F98', 'F99', 'G00', 'G03', 'G04', 'G06', 'G08', 'G12', 'G20', 'G24', 'G25', 'G30', 'G31', 'G35', 'G36', 'G37', 'G40', 'G43', 'G44', 'G45', 'G47', 'G50', 'G51', 'G54', 'G56', 'G57', 'G58', 'G60', 'G61', 'G62', 'G70', 'G71', 'G72', 'G81', 'G82', 'G83', 'G89', 'G90', 'G91', 'G93', 'G95', 'G96', 'G97', 'H00', 'H01', 'H02', 'H04', 'H05', 'H10', 'H11', 'H15', 'H16', 'H17', 'H18', 'H20', 'H21', 'H25', 'H26', 'H27', 'H30', 'H31', 'H33', 'H34', 'H35', 'H40', 'H43', 'H44', 'H46', 'H47', 'H49', 'H50', 'H51', 'H52', 'H53', 'H54', 'H55', 'H57', 'H59', 'H60', 'H66', 'H80', 'H90', 'H91', 'H92', 'H93', 'I05', 'I07', 'I08', 'I10', 'I11', 'I12', 'I15', 'I16', 'I20', 'I21', 'I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I30', 'I31', 'I33', 'I34', 'I35', 'I37', 'I38', 'I42', 'I43', 'I44', 'I45', 'I46', 'I47', 'I48', 'I49', 'I50', 'I51', 'I60', 'I61', 'I62', 'I63', 'I65', 'I66', 'I67', 'I69', 'I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I80', 'I81', 'I82', 'I83', 'I85', 'I86', 'I87', 'I88', 'I89', 'I95', 'I96', 'I99', 'J00', 'J01', 'J02', 'J03', 'J06', 'J10', 'J11', 'J12', 'J15', 'J16', 'J18', 'J21', 'J22', 'J30', 'J32', 'J33', 'J34', 'J35', 'J38', 'J39', 'J42', 'J43', 'J44', 'J45', 'J47', 'J60', 'J62', 'J64', 'J69', 'J80', 'J81', 'J82', 'J84', 'J85', 'J86', 'J90', 'J91', 'J93', 'J94', 'J95', 'J96', 'J98', 'J99', 'K00', 'K01', 'K02', 'K04', 'K05', 'K06', 'K08', 'K09', 'K11', 'K12', 'K13', 'K14', 'K20', 'K21', 'K22', 'K25', 'K26', 'K27', 'K29', 'K30', 'K31', 'K35', 'K36', 'K37', 'K38', 'K40', 'K42', 'K43', 'K44', 'K46', 'K50', 'K51', 'K52', 'K55', 'K56', 'K57', 'K59', 'K60', 'K61', 'K62', 'K63', 'K64', 'K65', 'K66', 'K68', 'K70', 'K71', 'K72', 'K73', 'K74', 'K75', 'K76', 'K80', 'K81', 'K82', 'K83', 'K85', 'K86', 'K90', 'K91', 'K92', 'L00', 'L02', 'L03', 'L08', 'L10', 'L11', 'L20', 'L21', 'L25', 'L27', 'L28', 'L29', 'L30', 'L40', 'L42', 'L43', 'L50', 'L51', 'L52', 'L53', 'L57', 'L58', 'L60', 'L65', 'L68', 'L70', 'L71', 'L72', 'L74', 'L76', 'L80', 'L81', 'L83', 'L85', 'L88', 'L89', 'L90', 'L91', 'L92', 'L93', 'L94', 'L95', 'L97', 'L98', 'M00', 'M04', 'M05', 'M06', 'M08', 'M13', 'M15', 'M16', 'M17', 'M19', 'M21', 'M24', 'M25', 'M26', 'M27', 'M30', 'M31', 'M32', 'M33', 'M34', 'M35', 'M41', 'M43', 'M45', 'M46', 'M47', 'M48', 'M50', 'M51', 'M53', 'M54', 'M60', 'M61', 'M62', 'M65', 'M67', 'M70', 'M71', 'M72', 'M75', 'M79', 'M80', 'M81', 'M83', 'M84', 'M85', 'M86', 'M87', 'M88', 'M89', 'M92', 'M94', 'M95', 'M96', 'N00', 'N01', 'N02', 'N03', 'N04', 'N05', 'N11', 'N12', 'N13', 'N14', 'N15', 'N17', 'N18', 'N19', 'N20', 'N21', 'N23', 'N25', 'N26', 'N28', 'N29', 'N30', 'N31', 'N32', 'N34', 'N35', 'N36', 'N39', 'N40', 'N41', 'N42', 'N43', 'N44', 'N45', 'N46', 'N48', 'N49', 'N50', 'N52', 'N53', 'N60', 'N62', 'N63', 'N64', 'N76', 'N80', 'N81', 'N82', 'N83', 'N85', 'N89', 'N90', 'N91', 'N92', 'N93', 'N94', 'N96', 'N97', 'O02', 'O03', 'O14', 'O21', 'O24', 'O30', 'O32', 'O36', 'O40', 'O41', 'O42', 'O45', 'O47', 'O60', 'O72', 'O75', 'O80', 'O92', 'O99', 'P07', 'P52', 'P81', 'P83', 'P84', 'P91', 'Q02', 'Q03', 'Q04', 'Q05', 'Q07', 'Q10', 'Q12', 'Q13', 'Q14', 'Q18', 'Q21', 'Q23', 'Q24', 'Q25', 'Q27', 'Q28', 'Q30', 'Q31', 'Q32', 'Q35', 'Q37', 'Q38', 'Q42', 'Q43', 'Q44', 'Q45', 'Q51', 'Q52', 'Q53', 'Q55', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64', 'Q67', 'Q68', 'Q69', 'Q70', 'Q71', 'Q74', 'Q75', 'Q76', 'Q78', 'Q79', 'Q82', 'Q83', 'Q85', 'Q87', 'Q89', 'Q90', 'Q98', 'R00', 'R01', 'R04', 'R05', 'R06', 'R07', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R22', 'R23', 'R25', 'R26', 'R27', 'R29', 'R30', 'R31', 'R32', 'R33', 'R34', 'R35', 'R36', 'R39', 'R40', 'R41', 'R42', 'R43', 'R44', 'R45', 'R46', 'R47', 'R48', 'R49', 'R50', 'R51', 'R52', 'R53', 'R55', 'R56', 'R57', 'R58', 'R59', 'R60', 'R61', 'R62', 'R63', 'R64', 'R65', 'R68', 'R69', 'R71', 'R73', 'R74', 'R76', 'R78', 'R79', 'R80', 'R81', 'R82', 'R86', 'R89', 'R90', 'R91', 'R97', 'R99', 'S00', 'S01', 'S02', 'S03', 'S05', 'S06', 'S09', 'S10', 'S11', 'S19', 'S20', 'S21', 'S22', 'S23', 'S25', 'S27', 'S30', 'S31', 'S32', 'S34', 'S35', 'S36', 'S37', 'S39', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S50', 'S52', 'S53', 'S60', 'S61', 'S62', 'S63', 'S69', 'S70', 'S71', 'S72', 'S80', 'S81', 'S82', 'S83', 'S84', 'S85', 'S86', 'S89', 'S90', 'S91', 'S92', 'S93', 'S96', 'T07', 'T14', 'T17', 'T18', 'T19', 'T20', 'T21', 'T24', 'T30', 'T31', 'T36', 'T38', 'T48', 'T50', 'T56', 'T68', 'T74', 'T75', 'T78', 'T79', 'T80', 'T81', 'T82', 'T85', 'T86', 'V00', 'V19', 'V23', 'V29', 'V37', 'V87', 'V89', 'V99', 'W01', 'W16', 'W19', 'W21', 'W32', 'W34', 'W40', 'W55', 'W56', 'W57', 'W86', 'X08', 'X58', 'X73', 'Y09', 'Y84', 'Y95', 'Z16', 'Z17', 'Z20', 'Z21', 'Z37', 'Z3A', 'Z51', 'Z53', 'Z56', 'Z60', 'Z65', 'Z67', 'Z68', 'Z74', 'Z75', 'Z76', 'Z77', 'Z79', 'Z80', 'Z82', 'Z83', 'Z85', 'Z86', 'Z87', 'Z88', 'Z90', 'Z91', 'Z92', 'Z93', 'Z94', 'Z95', 'Z96', 'Z97', 'Z98', 'Z99']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A01.00', 'A01.4', 'A02.9', 'A03.8', 'A03.9', 'A04.5', 'A04.7', 'A04.8', 'A06.9', 'A08.0', 'A15.0', 'A15.4', 'A15.6', 'A15.9', 'A17.81', 'A18.09', 'A18.10', 'A18.11', 'A18.12', 'A18.15', 'A18.2', 'A18.4', 'A18.50', 'A19.9', 'A23.9', 'A28.1', 'A31.9', 'A32.11', 'A32.9', 'A38.9', 'A39.0', 'A39.1', 'A39.4', 'A40.3', 'A41.02', 'A41.1', 'A41.51', 'A41.52', 'A41.81', 'A41.9', 'A42.1', 'A42.9', 'A43.9', 'A44.0', 'A44.9', 'A48.1', 'A49.1', 'A49.8', 'A49.9', 'A53.9', 'A54.9', 'A60.00', 'A60.9', 'A63.0', 'A69.20', 'A74.9', 'A79.9', 'A80.9', 'A87.2', 'B00.1', 'B00.9', 'B01.12', 'B01.9', 'B02.21', 'B02.29', 'B02.30', 'B02.39', 'B02.9', 'B05.9', 'B06.9', 'B07.9', 'B08.3', 'B10.81', 'B10.89', 'B15.9', 'B17.9', 'B18.1', 'B18.2', 'B19.10', 'B19.20', 'B19.9', 'B25.9', 'B26.9', 'B27.00', 'B27.90', 'B27.99', 'B30.0', 'B30.9', 'B33.3', 'B34.1', 'B34.8', 'B34.9', 'B35.2', 'B35.9', 'B37.0', 'B37.2', 'B37.3', 'B37.7', 'B37.81', 'B37.9', 'B44.9', 'B45.2', 'B45.9', 'B46.5', 'B46.9', 'B48.8', 'B55.0', 'B55.9', 'B57.2', 'B58.00', 'B58.2', 'B58.9', 'B65.0', 'B65.1', 'B65.9', 'B67.8', 'B67.90', 'B67.99', 'B69.0', 'B69.9', 'B74.9', 'B81.0', 'B83.0', 'B85.2', 'B95.0', 'B95.2', 'B95.3', 'B95.4', 'B95.5', 'B95.6', 'B95.61', 'B95.62', 'B95.7', 'B95.8', 'B96.0', 'B96.1', 'B96.20', 'B96.29', 'B96.3', 'B96.4', 'B96.5', 'B96.7', 'B96.81', 'B96.89', 'B97.0', 'B97.10', 'B97.12', 'B97.29', 'B97.4', 'B97.6', 'B97.7', 'B99.9', 'C02.0', 'C02.1', 'C02.9', 'C04.9', 'C05.1', 'C05.9', 'C06.0', 'C06.9', 'C08.0', 'C09.9', 'C10.9', 'C15.9', 'C16.0', 'C16.9', 'C17.1', 'C18.0', 'C18.6', 'C18.7', 'C18.9', 'C21.0', 'C22.0', 'C22.1', 'C22.9', 'C25.0', 'C25.9', 'C26.0', 'C30.0', 'C31.2', 'C32.1', 'C32.9', 'C34.2', 'C34.9', 'C34.90', 'C34.91', 'C38.4', 'C40.20', 'C41.0', 'C41.1', 'C41.9', 'C43.31', 'C43.62', 'C43.9', 'C44.192', 'C44.199', 'C44.311', 'C44.42', 'C44.49', 'C44.699', 'C47.9', 'C48.0', 'C48.2', 'C49.12', 'C49.21', 'C49.4', 'C49.5', 'C49.9', 'C49.A', 'C49.A2', 'C50.312', 'C50.412', 'C50.519', 'C50.9', 'C50.91', 'C50.912', 'C50.919', 'C53.9', 'C54.1', 'C56.2', 'C56.9', 'C60.1', 'C60.9', 'C62.9', 'C62.90', 'C62.91', 'C62.92', 'C63.1', 'C63.10', 'C63.7', 'C63.8', 'C64.1', 'C64.2', 'C64.9', 'C65.9', 'C66.1', 'C66.2', 'C66.9', 'C67.2', 'C67.7', 'C67.9', 'C68.0', 'C69.30', 'C69.32', 'C69.40', 'C70.0', 'C70.9', 'C71.0', 'C71.9', 'C72.30', 'C72.32', 'C74.02', 'C74.90', 'C74.92', 'C75.0', 'C76.3', 'C77.0', 'C77.1', 'C77.2', 'C77.3', 'C77.8', 'C77.9', 'C78.00', 'C78.01', 'C78.02', 'C78.1', 'C78.39', 'C78.4', 'C78.5', 'C78.6', 'C78.7', 'C78.89', 'C79.0', 'C79.00', 'C79.02', 'C79.2', 'C79.31', 'C79.49', 'C79.51', 'C79.52', 'C79.60', 'C79.70', 'C79.71', 'C79.72', 'C79.81', 'C79.82', 'C79.89', 'C79.9', 'C80.0', 'C80.1', 'C81.1', 'C81.10', 'C81.29', 'C81.90', 'C83.0', 'C83.00', 'C83.1', 'C83.30', 'C83.39', 'C83.70', 'C84.49', 'C84.99', 'C84.A', 'C85.1', 'C85.9', 'C85.90', 'C85.99', 'C88.0', 'C88.4', 'C90.0', 'C90.00', 'C90.1', 'C90.3', 'C90.30', 'C91.00', 'C91.01', 'C91.1', 'C91.10', 'C91.90', 'C92.10', 'C92.3', 'C94.6', 'C96.6', 'C96.A', 'D09.0', 'D10.30', 'D11.0', 'D12.1', 'D12.3', 'D12.6', 'D12.8', 'D13.0', 'D13.1', 'D13.5', 'D13.6', 'D13.9', 'D15.1', 'D16.10', 'D16.21', 'D16.22', 'D16.5', 'D16.9', 'D17.23', 'D17.5', 'D17.71', 'D17.9', 'D18.00', 'D18.01', 'D18.02', 'D18.03', 'D18.09', 'D18.1', 'D21.10', 'D21.20', 'D22.9', 'D23.10', 'D23.11', 'D23.30', 'D23.9', 'D24.1', 'D24.9', 'D25.9', 'D27.9', 'D29.1', 'D29.30', 'D29.31', 'D29.8', 'D30.00', 'D30.01', 'D30.02', 'D30.3', 'D30.4', 'D31.62', 'D31.9', 'D32.0', 'D32.9', 'D33.3', 'D35.00', 'D35.02', 'D35.1', 'D35.2', 'D36.10', 'D36.11', 'D37.030', 'D37.8', 'D38.3', 'D3A.8', 'D40.0', 'D40.1', 'D40.10', 'D40.11', 'D40.8', 'D41.00', 'D41.01', 'D41.02', 'D41.3', 'D41.4', 'D43.1', 'D43.2', 'D43.4', 'D44.10', 'D44.12', 'D44.2', 'D44.4', 'D44.6', 'D44.7', 'D46.9', 'D47.0', 'D47.1', 'D47.2', 'D47.3', 'D47.9', 'D48.0', 'D48.1', 'D49.0', 'D49.1', 'D49.2', 'D49.3', 'D49.4', 'D49.511', 'D49.512', 'D49.519', 'D49.59', 'D49.6', 'D49.7', 'D49.89', 'D50.0', 'D50.9', 'D53.1', 'D56.1', 'D56.3', 'D57.00', 'D57.1', 'D58.9', 'D61.818', 'D61.82', 'D63.1', 'D64.9', 'D68.1', 'D68.51', 'D68.52', 'D68.59', 'D68.61', 'D68.9', 'D69.0', 'D69.2', 'D69.3', 'D69.41', 'D69.6', 'D69.9', 'D70.9', 'D72.0', 'D72.1', 'D72.810', 'D72.819', 'D72.820', 'D72.821', 'D72.822', 'D72.829', 'D73.1', 'D73.3', 'D73.4', 'D73.5', 'D73.89', 'D74.8', 'D75.81', 'D75.89', 'D76.3', 'D80.0', 'D80.1', 'D84.9', 'D86.9', 'D89.1', 'D89.2', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E04.9', 'E05.00', 'E05.80', 'E05.90', 'E05.91', 'E06.1', 'E06.3', 'E07.9', 'E10.10', 'E10.9', 'E11.22', 'E11.319', 'E11.359', 'E11.42', 'E11.622', 'E11.628', 'E11.64', 'E11.65', 'E11.9', 'E13.9', 'E16.2', 'E21.0', 'E21.3', 'E21.5', 'E22.0', 'E22.2', 'E23.0', 'E23.2', 'E24.9', 'E27.0', 'E27.40', 'E27.49', 'E27.8', 'E27.9', 'E28.39', 'E29.1', 'E31.20', 'E31.21', 'E44.0', 'E44.1', 'E50.9', 'E51.9', 'E53.1', 'E53.8', 'E55.9', 'E56.1', 'E56.9', 'E61.1', 'E63.9', 'E66.01', 'E66.3', 'E66.9', 'E70.39', 'E72.01', 'E72.09', 'E73.9', 'E74.00', 'E74.04', 'E75.29', 'E75.5', 'E77.8', 'E78.00', 'E78.1', 'E78.5', 'E79.0', 'E80.20', 'E80.21', 'E80.4', 'E80.7', 'E83.01', 'E83.119', 'E83.19', 'E83.39', 'E83.41', 'E83.42', 'E83.51', 'E83.52', 'E83.59', 'E83.81', 'E84.9', 'E85.2', 'E85.3', 'E85.4', 'E85.9', 'E86.0', 'E87.0', 'E87.1', 'E87.2', 'E87.3', 'E87.5', 'E87.6', 'E87.70', 'E88.09', 'E88.89', 'E88.9', 'E89.0', 'F02.80', 'F10.10', 'F10.20', 'F10.21', 'F10.23', 'F10.921', 'F10.96', 'F11.20', 'F11.93', 'F12.10', 'F12.20', 'F14.10', 'F14.20', 'F17.200', 'F17.210', 'F17.219', 'F17.290', 'F19.10', 'F19.20', 'F19.21', 'F20.0', 'F20.2', 'F20.5', 'F25.0', 'F30.9', 'F31.81', 'F31.9', 'F32.3', 'F32.9', 'F34.1', 'F40.00', 'F40.10', 'F40.9', 'F41.0', 'F41.8', 'F41.9', 'F43.20', 'F43.9', 'F45.21', 'F45.29', 'F50.2', 'F60.1', 'F60.3', 'F60.6', 'F60.7', 'F60.9', 'F63.9', 'F65.3', 'F80.81', 'F84.0', 'F84.5', 'F90.9', 'F91.9', 'F98.8', 'G00.1', 'G03.9', 'G04.02', 'G04.1', 'G04.90', 'G06.0', 'G06.2', 'G12.20', 'G12.21', 'G24.5', 'G24.9', 'G25.0', 'G25.2', 'G25.3', 'G30.9', 'G31.84', 'G31.9', 'G36.0', 'G37.3', 'G40.109', 'G40.119', 'G40.309', 'G40.401', 'G40.409', 'G40.901', 'G40.909', 'G43.829', 'G43.909', 'G44.209', 'G44.89', 'G45.9', 'G47.00', 'G47.30', 'G47.32', 'G47.33', 'G47.63', 'G47.9', 'G50.0', 'G51.0', 'G51.3', 'G54.6', 'G56.00', 'G56.20', 'G56.31', 'G56.92', 'G57.00', 'G57.01', 'G57.42', 'G58.9', 'G60.8', 'G61.0', 'G62.81', 'G62.9', 'G70.00', 'G71.11', 'G71.8', 'G72.2', 'G72.3', 'G72.49', 'G72.9', 'G81.14', 'G81.90', 'G81.91', 'G81.94', 'G82.20', 'G82.21', 'G82.50', 'G83.1', 'G83.14', 'G83.2', 'G83.21', 'G83.24', 'G83.9', 'G89.18', 'G89.29', 'G90.2', 'G90.50', 'G90.511', 'G90.9', 'G91.8', 'G91.9', 'G93.0', 'G93.2', 'G93.40', 'G93.41', 'G93.5', 'G93.6', 'G93.82', 'G93.9', 'G95.19', 'G95.20', 'G95.9', 'G96.0', 'G96.19', 'G97.2', 'H00.11', 'H00.19', 'H01.001', 'H01.006', 'H01.009', 'H01.8', 'H01.9', 'H02.30', 'H02.40', 'H02.401', 'H02.402', 'H02.409', 'H02.411', 'H02.539', 'H02.726', 'H02.841', 'H02.843', 'H02.844', 'H02.845', 'H02.846', 'H02.849', 'H02.89', 'H02.9', 'H04.20', 'H04.201', 'H04.202', 'H04.203', 'H04.209', 'H05.11', 'H05.111', 'H05.2', 'H05.20', 'H05.221', 'H05.231', 'H05.233', 'H10.30', 'H10.32', 'H10.9', 'H11.001', 'H11.002', 'H11.133', 'H11.143', 'H11.42', 'H11.421', 'H11.422', 'H11.423', 'H11.429', 'H11.431', 'H11.432', 'H11.439', 'H11.9', 'H15.00', 'H15.032', 'H15.033', 'H15.092', 'H15.10', 'H16.0', 'H16.00', 'H16.001', 'H16.012', 'H16.07', 'H16.071', 'H16.079', 'H16.409', 'H16.442', 'H16.449', 'H16.9', 'H17.0', 'H17.12', 'H17.9', 'H18.2', 'H18.20', 'H18.89', 'H18.891', 'H18.9', 'H20.05', 'H20.052', 'H20.059', 'H20.9', 'H21.26', 'H21.309', 'H21.42', 'H21.50', 'H21.501', 'H21.509', 'H21.519', 'H21.542', 'H21.9', 'H25.1', 'H25.13', 'H26.8', 'H26.9', 'H27.10', 'H30.14', 'H30.89', 'H30.9', 'H30.92', 'H30.93', 'H31.101', 'H31.103', 'H31.309', 'H31.32', 'H31.40', 'H33.10', 'H33.101', 'H33.102', 'H33.19', 'H33.20', 'H33.21', 'H33.22', 'H33.309', 'H34.9', 'H35.00', 'H35.042', 'H35.06', 'H35.063', 'H35.30', 'H35.33', 'H35.349', 'H35.383', 'H35.50', 'H35.52', 'H35.6', 'H35.60', 'H35.61', 'H35.62', 'H35.81', 'H35.89', 'H35.9', 'H40.05', 'H40.052', 'H40.059', 'H40.10', 'H40.11', 'H40.83', 'H40.9', 'H43.1', 'H43.10', 'H43.11', 'H43.12', 'H43.13', 'H43.81', 'H43.812', 'H44.00', 'H44.001', 'H44.002', 'H44.009', 'H44.139', 'H44.40', 'H44.511', 'H46.00', 'H46.10', 'H46.9', 'H47.03', 'H47.039', 'H47.091', 'H47.092', 'H47.093', 'H47.10', 'H47.20', 'H47.322', 'H49.01', 'H49.02', 'H49.11', 'H49.21', 'H49.22', 'H49.23', 'H49.9', 'H50.00', 'H50.011', 'H50.012', 'H50.10', 'H50.112', 'H50.21', 'H50.22', 'H50.52', 'H50.9', 'H51.21', 'H52.01', 'H52.02', 'H52.1', 'H52.10', 'H52.13', 'H52.7', 'H53.00', 'H53.002', 'H53.14', 'H53.142', 'H53.143', 'H53.149', 'H53.15', 'H53.2', 'H53.30', 'H53.40', 'H53.411', 'H53.413', 'H53.419', 'H53.461', 'H53.462', 'H53.47', 'H53.60', 'H53.7', 'H53.8', 'H53.9', 'H54.0', 'H54.2', 'H54.3', 'H54.61', 'H54.62', 'H54.7', 'H55.00', 'H55.01', 'H57.02', 'H57.03', 'H57.04', 'H57.1', 'H57.10', 'H57.11', 'H57.12', 'H57.13', 'H57.8', 'H57.9', 'H59.03', 'H59.031', 'H59.033', 'H60.91', 'H66.90', 'H80.90', 'H90.12', 'H90.5', 'H91.20', 'H91.3', 'H91.9', 'H91.90', 'H91.91', 'H91.93', 'H92.01', 'H92.09', 'H92.20', 'H92.21', 'H93.11', 'H93.19', 'I05.0', 'I05.9', 'I07.1', 'I08.1', 'I08.3', 'I11.9', 'I12.0', 'I12.9', 'I15.9', 'I16.9', 'I20.0', 'I20.8', 'I20.9', 'I21.09', 'I21.11', 'I21.19', 'I21.29', 'I21.3', 'I23.0', 'I23.7', 'I24.0', 'I24.9', 'I25.10', 'I25.2', 'I25.9', 'I26.99', 'I27.2', 'I28.1', 'I30.9', 'I31.2', 'I31.3', 'I31.4', 'I33.0', 'I34.0', 'I34.1', 'I35.0', 'I35.1', 'I35.8', 'I35.9', 'I37.0', 'I42.0', 'I42.6', 'I42.8', 'I42.9', 'I44.2', 'I44.30', 'I44.60', 'I44.7', 'I45.10', 'I45.2', 'I45.6', 'I46.9', 'I47.1', 'I47.2', 'I48.0', 'I48.2', 'I48.91', 'I48.92', 'I49.01', 'I49.3', 'I49.9', 'I50.9', 'I51.4', 'I51.5', 'I51.7', 'I51.9', 'I60.9', 'I61.1', 'I61.3', 'I61.4', 'I61.5', 'I61.8', 'I61.9', 'I62.00', 'I62.01', 'I62.03', 'I62.9', 'I63.511', 'I63.512', 'I63.519', 'I63.9', 'I65.21', 'I65.22', 'I65.8', 'I66.9', 'I67.1', 'I67.2', 'I67.82', 'I67.89', 'I69.354', 'I70.0', 'I70.1', 'I70.203', 'I70.90', 'I71.00', 'I71.1', 'I71.2', 'I71.4', 'I71.9', 'I72.1', 'I72.2', 'I72.3', 'I72.8', 'I72.9', 'I73.1', 'I73.8', 'I74.3', 'I74.8', 'I74.9', 'I75.81', 'I75.89', 'I77.0', 'I77.1', 'I77.6', 'I77.70', 'I77.71', 'I77.819', 'I77.9', 'I78.1', 'I80.8', 'I82.0', 'I82.2', 'I82.210', 'I82.220', 'I82.3', 'I82.40', 'I82.401', 'I82.402', 'I82.409', 'I82.411', 'I82.431', 'I82.439', 'I82.619', 'I82.90', 'I82.A19', 'I83.90', 'I83.93', 'I85.00', 'I85.01', 'I86.1', 'I86.4', 'I87.1', 'I87.2', 'I87.8', 'I88.0', 'I88.1', 'I88.9', 'I89.0', 'I89.1', 'I89.8', 'I95.9', 'I99.8', 'I99.9', 'J01.30', 'J01.90', 'J02.9', 'J03.90', 'J03.91', 'J06.9', 'J10.1', 'J11.0', 'J11.1', 'J12.0', 'J15.0', 'J15.211', 'J15.6', 'J16.0', 'J18.1', 'J18.9', 'J21.8', 'J21.9', 'J30.1', 'J30.2', 'J30.81', 'J32.0', 'J32.3', 'J32.4', 'J32.9', 'J33.8', 'J33.9', 'J34.1', 'J34.3', 'J34.89', 'J34.9', 'J35.1', 'J38.1', 'J38.4', 'J39.2', 'J39.8', 'J43.9', 'J44.9', 'J45.909', 'J47.9', 'J62.8', 'J69.0', 'J69.8', 'J81.0', 'J81.1', 'J84.01', 'J84.10', 'J84.112', 'J84.89', 'J84.9', 'J85.2', 'J86.0', 'J91.0', 'J93.9', 'J94.0', 'J94.2', 'J94.8', 'J95.821', 'J95.851', 'J96.00', 'J96.90', 'J96.92', 'J98.01', 'J98.11', 'J98.2', 'J98.4', 'J98.51', 'J98.6', 'J98.8', 'K00.0', 'K00.1', 'K00.6', 'K01.0', 'K02.9', 'K04.1', 'K04.7', 'K04.8', 'K05.10', 'K05.6', 'K06.1', 'K06.2', 'K06.8', 'K06.9', 'K08.109', 'K08.89', 'K08.9', 'K09.0', 'K09.8', 'K11.1', 'K11.2', 'K11.20', 'K11.4', 'K11.6', 'K11.7', 'K11.9', 'K12.0', 'K12.1', 'K12.2', 'K13.0', 'K13.21', 'K13.29', 'K13.70', 'K13.79', 'K14.5', 'K14.6', 'K14.8', 'K14.9', 'K20.9', 'K21.9', 'K22.0', 'K22.10', 'K22.2', 'K22.6', 'K22.70', 'K22.8', 'K22.9', 'K25.9', 'K26.4', 'K26.9', 'K27.9', 'K29.50', 'K29.70', 'K29.80', 'K31.1', 'K31.5', 'K31.7', 'K31.819', 'K31.84', 'K31.89', 'K31.9', 'K35.2', 'K35.80', 'K38.8', 'K40.20', 'K40.30', 'K40.90', 'K42.9', 'K43.9', 'K44.9', 'K46.9', 'K50.00', 'K50.10', 'K50.80', 'K50.90', 'K51.00', 'K51.90', 'K52.3', 'K52.81', 'K52.82', 'K52.9', 'K55.049', 'K55.059', 'K55.069', 'K55.1', 'K55.20', 'K55.21', 'K55.9', 'K56.0', 'K56.1', 'K56.2', 'K56.3', 'K56.41', 'K56.5', 'K56.60', 'K56.69', 'K56.7', 'K57.00', 'K57.10', 'K57.11', 'K57.20', 'K57.30', 'K57.32', 'K57.80', 'K57.90', 'K57.92', 'K59.00', 'K59.39', 'K59.8', 'K59.9', 'K60.2', 'K60.3', 'K61.0', 'K61.1', 'K62.0', 'K62.1', 'K62.4', 'K62.5', 'K62.6', 'K62.7', 'K62.89', 'K62.9', 'K63.1', 'K63.2', 'K63.3', 'K63.5', 'K63.89', 'K63.9', 'K64.8', 'K64.9', 'K65.0', 'K65.1', 'K65.3', 'K65.4', 'K65.8', 'K65.9', 'K66.0', 'K66.1', 'K66.8', 'K68.12', 'K68.19', 'K68.9', 'K70.10', 'K70.30', 'K70.9', 'K71.6', 'K72.00', 'K72.9', 'K72.90', 'K73.9', 'K74.0', 'K74.60', 'K75.0', 'K75.4', 'K75.9', 'K76.0', 'K76.1', 'K76.6', 'K76.7', 'K76.81', 'K76.89', 'K76.9', 'K80.10', 'K80.20', 'K80.5', 'K80.50', 'K80.51', 'K81.0', 'K81.9', 'K82.8', 'K83.0', 'K83.1', 'K83.3', 'K83.8', 'K85.10', 'K85.20', 'K85.90', 'K86.0', 'K86.1', 'K86.2', 'K86.3', 'K86.89', 'K86.9', 'K90.0', 'K90.9', 'K91.2', 'K91.3', 'K91.7', 'K92.0', 'K92.1', 'K92.2', 'L02.01', 'L02.211', 'L02.31', 'L02.32', 'L02.41', 'L02.811', 'L02.91', 'L03.11', 'L03.115', 'L03.213', 'L03.90', 'L08.9', 'L10.0', 'L11.9', 'L20.9', 'L21.1', 'L21.9', 'L25.9', 'L27.0', 'L28.2', 'L29.0', 'L29.9', 'L30.9', 'L40.0', 'L40.53', 'L40.9', 'L43.9', 'L50.9', 'L51.1', 'L51.2', 'L51.9', 'L53.9', 'L57.0', 'L58.9', 'L60.8', 'L60.9', 'L65.9', 'L68.0', 'L70.1', 'L71.9', 'L72.0', 'L74.0', 'L74.52', 'L76.3', 'L81.9', 'L85.8', 'L85.9', 'L89.150', 'L89.159', 'L90.5', 'L90.8', 'L91.0', 'L92.9', 'L93.0', 'L94.0', 'L94.3', 'L95.9', 'L97.909', 'L97.919', 'L98.0', 'L98.49', 'L98.8', 'L98.9', 'M00.071', 'M00.9', 'M04.1', 'M05.9', 'M06.00', 'M06.9', 'M08.90', 'M08.961', 'M08.99', 'M13.0', 'M15.9', 'M16.0', 'M16.11', 'M17.9', 'M19.90', 'M21.059', 'M21.932', 'M21.959', 'M24.60', 'M24.641', 'M24.642', 'M25.40', 'M25.439', 'M25.451', 'M25.461', 'M25.47', 'M25.50', 'M25.511', 'M25.512', 'M25.519', 'M25.531', 'M25.532', 'M25.539', 'M25.541', 'M25.551', 'M25.552', 'M25.561', 'M25.562', 'M25.569', 'M25.571', 'M25.572', 'M25.60', 'M25.70', 'M25.9', 'M26.02', 'M26.04', 'M26.09', 'M26.19', 'M26.30', 'M26.32', 'M26.4', 'M26.52', 'M26.601', 'M26.609', 'M26.611', 'M26.619', 'M26.629', 'M26.69', 'M26.9', 'M27.2', 'M27.49', 'M27.8', 'M30.3', 'M31.0', 'M31.1', 'M31.6', 'M32.9', 'M33.90', 'M34.9', 'M35.00', 'M35.2', 'M35.3', 'M35.9', 'M41.9', 'M43.6', 'M45.9', 'M46.26', 'M46.34', 'M46.40', 'M46.44', 'M46.47', 'M46.90', 'M46.96', 'M47.816', 'M47.895', 'M47.896', 'M47.9', 'M48.00', 'M48.04', 'M48.06', 'M48.30', 'M50.221', 'M51.24', 'M51.26', 'M51.27', 'M53.2X6', 'M53.3', 'M54.2', 'M54.30', 'M54.31', 'M54.40', 'M54.41', 'M54.42', 'M54.5', 'M54.6', 'M54.9', 'M60.009', 'M60.059', 'M60.9', 'M61.10', 'M62.00', 'M62.3', 'M62.50', 'M62.81', 'M62.82', 'M62.838', 'M62.89', 'M65.9', 'M67.431', 'M70.51', 'M71.30', 'M72.0', 'M72.4', 'M72.6', 'M75.90', 'M79.0', 'M79.1', 'M79.2', 'M79.3', 'M79.5', 'M79.60', 'M79.603', 'M79.604', 'M79.605', 'M79.621', 'M79.641', 'M79.643', 'M79.644', 'M79.645', 'M79.646', 'M79.652', 'M79.659', 'M79.671', 'M79.672', 'M79.673', 'M79.7', 'M79.81', 'M79.89', 'M79.A11', 'M80.08X', 'M80.88X', 'M81.0', 'M83.9', 'M84.30X', 'M84.453', 'M84.48X', 'M84.529', 'M85.00', 'M85.2', 'M85.50', 'M85.60', 'M85.8', 'M85.80', 'M85.88', 'M86.161', 'M86.60', 'M86.68', 'M86.8X5', 'M86.9', 'M87.059', 'M87.80', 'M87.9', 'M88.832', 'M88.9', 'M89.061', 'M89.072', 'M89.549', 'M89.59', 'M89.70', 'M89.8X', 'M89.8X9', 'M89.9', 'M92.50', 'M94.359', 'M95.0', 'M95.2', 'M96.1', 'N00.9', 'N01.9', 'N02.1', 'N02.8', 'N03.9', 'N04.9', 'N05.1', 'N05.5', 'N05.7', 'N05.8', 'N05.9', 'N11.9', 'N13.30', 'N13.5', 'N13.6', 'N13.70', 'N13.8', 'N13.9', 'N14.0', 'N15.1', 'N17.0', 'N17.9', 'N18.2', 'N18.3', 'N18.4', 'N18.5', 'N18.6', 'N18.9', 'N20.0', 'N20.1', 'N20.9', 'N21.0', 'N25.81', 'N25.89', 'N26.1', 'N26.9', 'N28.0', 'N28.1', 'N28.82', 'N28.89', 'N28.9', 'N30.8', 'N30.80', 'N30.90', 'N31.1', 'N31.9', 'N32.0', 'N32.1', 'N32.3', 'N32.89', 'N32.9', 'N34.2', 'N35.9', 'N36.0', 'N36.1', 'N36.8', 'N39.0', 'N39.3', 'N39.41', 'N39.43', 'N39.44', 'N39.498', 'N40.0', 'N40.2', 'N41.0', 'N41.1', 'N41.2', 'N41.9', 'N42.30', 'N42.9', 'N43.3', 'N43.40', 'N44.00', 'N44.2', 'N44.8', 'N45.1', 'N45.2', 'N45.3', 'N45.4', 'N46.01', 'N46.11', 'N46.9', 'N48.1', 'N48.21', 'N48.29', 'N48.30', 'N48.5', 'N48.6', 'N48.81', 'N48.89', 'N48.9', 'N49.2', 'N50.0', 'N50.1', 'N50.3', 'N50.81', 'N50.811', 'N50.812', 'N50.819', 'N50.82', 'N50.89', 'N50.9', 'N52.9', 'N53.12', 'N60.01', 'N60.02', 'N60.1', 'N60.11', 'N60.12', 'N60.19', 'N60.92', 'N64.4', 'N64.89', 'N64.9', 'N76.0', 'N76.5', 'N80.1', 'N80.5', 'N80.8', 'N80.9', 'N81.2', 'N81.4', 'N82.1', 'N83.00', 'N83.9', 'N85.8', 'N85.9', 'N89.5', 'N90.89', 'N91.0', 'N92.1', 'N93.9', 'N94.10', 'N94.89', 'N94.9', 'N97.9', 'O02.0', 'O02.1', 'O03.9', 'O14.20', 'O21.0', 'O24.419', 'O30.00', 'O32.1', 'O36.4XX', 'O40.9XX0', 'O41.0', 'O41.00X', 'O41.129', 'O41.1290', 'O42.912', 'O42.92', 'O45.9', 'O47.9', 'O60.10X', 'O72.1', 'O75.9', 'O92.6', 'O99.11', 'P07.18', 'P07.39', 'P52.3', 'P81.9', 'P83.8', 'P91.60', 'Q03.0', 'Q03.1', 'Q03.9', 'Q04.0', 'Q04.3', 'Q04.8', 'Q04.9', 'Q05.9', 'Q07.00', 'Q07.8', 'Q07.9', 'Q10.0', 'Q12.4', 'Q13.0', 'Q13.4', 'Q14.1', 'Q18.0', 'Q18.8', 'Q21.1', 'Q23.1', 'Q24.8', 'Q24.9', 'Q25.6', 'Q27.30', 'Q27.9', 'Q28.2', 'Q30.9', 'Q31.5', 'Q32.4', 'Q35.3', 'Q37.9', 'Q38.2', 'Q42.9', 'Q43.0', 'Q43.8', 'Q43.9', 'Q44.6', 'Q44.7', 'Q45.0', 'Q51.0', 'Q51.3', 'Q51.811', 'Q52.0', 'Q53.10', 'Q53.20', 'Q53.9', 'Q55.21', 'Q55.22', 'Q55.29', 'Q55.4', 'Q55.69', 'Q60.2', 'Q60.5', 'Q60.6', 'Q61.02', 'Q61.19', 'Q61.3', 'Q61.4', 'Q62.10', 'Q62.11', 'Q62.4', 'Q62.8', 'Q63.1', 'Q63.2', 'Q63.9', 'Q64.0', 'Q64.10', 'Q64.12', 'Q64.4', 'Q64.74', 'Q67.0', 'Q67.3', 'Q68.0', 'Q69.9', 'Q70.9', 'Q71.91', 'Q74.0', 'Q75.0', 'Q75.2', 'Q75.3', 'Q76.49', 'Q78.2', 'Q78.8', 'Q79.1', 'Q79.9', 'Q82.8', 'Q83.9', 'Q85.00', 'Q85.01', 'Q85.1', 'Q85.8', 'Q85.9', 'Q87.2', 'Q87.3', 'Q87.40', 'Q87.81', 'Q87.89', 'Q89.2', 'Q89.3', 'Q90.9', 'Q98.3', 'Q98.4', 'R00.0', 'R00.1', 'R00.2', 'R01.0', 'R01.1', 'R04.0', 'R04.2', 'R04.89', 'R06.00', 'R06.01', 'R06.09', 'R06.1', 'R06.4', 'R06.6', 'R06.82', 'R06.89', 'R07.0', 'R07.1', 'R07.2', 'R07.81', 'R07.89', 'R07.9', 'R09.02', 'R09.2', 'R09.81', 'R10.0', 'R10.10', 'R10.11', 'R10.12', 'R10.13', 'R10.2', 'R10.30', 'R10.31', 'R10.32', 'R10.33', 'R10.811', 'R10.812', 'R10.813', 'R10.814', 'R10.815', 'R10.816', 'R10.817', 'R10.819', 'R10.83', 'R10.84', 'R10.9', 'R11.0', 'R11.10', 'R11.12', 'R11.14', 'R11.2', 'R13.10', 'R14.0', 'R15.9', 'R16.0', 'R16.1', 'R16.2', 'R18.0', 'R18.8', 'R19.0', 'R19.00', 'R19.01', 'R19.02', 'R19.03', 'R19.04', 'R19.06', 'R19.07', 'R19.09', 'R19.2', 'R19.32', 'R19.5', 'R19.7', 'R19.8', 'R20.0', 'R20.1', 'R20.2', 'R20.3', 'R20.8', 'R20.9', 'R22.0', 'R22.1', 'R22.2', 'R22.30', 'R22.9', 'R23.0', 'R23.1', 'R23.3', 'R23.4', 'R23.8', 'R25.1', 'R25.2', 'R25.3', 'R25.8', 'R26.0', 'R26.2', 'R26.81', 'R26.89', 'R26.9', 'R27.0', 'R27.8', 'R29.2', 'R29.6', 'R29.723', 'R29.810', 'R29.818', 'R29.898', 'R30.0', 'R30.1', 'R31.0', 'R31.29', 'R31.9', 'R33.9', 'R35.0', 'R35.1', 'R35.8', 'R36.1', 'R39.12', 'R39.13', 'R39.14', 'R39.15', 'R39.198', 'R39.89', 'R39.9', 'R40.0', 'R40.1', 'R40.20', 'R40.241', 'R40.2410', 'R40.2412', 'R40.242', 'R40.2420', 'R40.2421', 'R40.243', 'R40.2430', 'R40.3', 'R40.4', 'R41.0', 'R41.3', 'R41.840', 'R41.843', 'R43.2', 'R43.9', 'R44.0', 'R44.1', 'R44.3', 'R44.9', 'R45.0', 'R45.1', 'R45.3', 'R45.4', 'R46.0', 'R47.01', 'R47.02', 'R47.1', 'R47.89', 'R47.9', 'R48.2', 'R49.0', 'R49.1', 'R50.81', 'R50.82', 'R50.9', 'R53.1', 'R53.81', 'R53.83', 'R56.00', 'R56.9', 'R57.0', 'R57.1', 'R57.8', 'R57.9', 'R59.0', 'R59.1', 'R59.9', 'R60.0', 'R60.1', 'R60.9', 'R62.50', 'R62.52', 'R63.0', 'R63.1', 'R63.3', 'R63.4', 'R63.5', 'R65.10', 'R65.20', 'R65.21', 'R68.83', 'R68.84', 'R68.89', 'R71.0', 'R71.8', 'R73.01', 'R73.9', 'R74.0', 'R76.0', 'R76.11', 'R78.81', 'R79.82', 'R80.9', 'R82.0', 'R82.2', 'R82.3', 'R82.71', 'R86.9', 'R89.7', 'R90.0', 'R91.1', 'R91.8', 'R97.1', 'R97.20', 'R97.8', 'S00.32X', 'S00.33X', 'S00.522', 'S00.532', 'S01.00X', 'S01.101', 'S01.20X', 'S01.401', 'S01.501', 'S01.502', 'S01.512', 'S01.80X', 'S01.90X', 'S01.95X', 'S02.0XX', 'S02.109', 'S02.19X', 'S02.2XX', 'S02.401', 'S02.40F', 'S02.5XX', 'S02.601', 'S02.609', 'S02.652', 'S02.81X', 'S02.91X', 'S03.00X', 'S03.2XX', 'S05.8X2', 'S05.9', 'S05.90X', 'S05.92X', 'S06.0X0', 'S06.2X', 'S06.36', 'S06.370', 'S06.4X0', 'S06.4X9', 'S06.5X', 'S06.9', 'S06.9X', 'S06.9X0', 'S06.9X9', 'S09.90X', 'S09.92X', 'S09.93X', 'S10.0XX', 'S11.80X', 'S11.90X', 'S11.91X', 'S19.9X', 'S20.92X', 'S21.20', 'S22.20X', 'S22.39X', 'S22.4', 'S22.41X', 'S22.49XA', 'S22.5XX', 'S23.163', 'S25.02X', 'S27.329', 'S27.899', 'S30.0XX', 'S30.1XX', 'S30.23', 'S30.823', 'S31.104', 'S31.109', 'S31.40X', 'S31.809', 'S31.831', 'S32.019', 'S32.10', 'S32.313', 'S32.50', 'S32.501', 'S32.502', 'S32.591', 'S32.602', 'S32.9', 'S34.139', 'S35.229', 'S36.00X', 'S36.029', 'S36.09', 'S36.09X', 'S36.116', 'S37.009', 'S37.039', 'S37.20X', 'S37.29X', 'S37.812', 'S39.92', 'S39.94', 'S39.94X', 'S39.94XA', 'S41.10', 'S42.002', 'S42.009', 'S42.021', 'S42.30', 'S42.302', 'S42.322B', 'S42.402', 'S42.409', 'S42.90X', 'S43.006', 'S43.40', 'S45.10', 'S46.20', 'S47.2XX', 'S49.92X', 'S50.10X', 'S52.002', 'S52.102', 'S52.612', 'S52.92X', 'S53.101', 'S60.229', 'S60.511', 'S61.209', 'S61.402', 'S62.012', 'S62.12', 'S62.92X', 'S63.006', 'S63.071', 'S63.072', 'S69.91X', 'S69.92X', 'S70.312', 'S70.32', 'S71.101', 'S72.002', 'S72.302', 'S72.401', 'S72.401C', 'S80.821', 'S81.001', 'S82.101', 'S82.20', 'S82.40', 'S83.105', 'S83.106', 'S84.02X', 'S85.162', 'S86.102', 'S89.90X', 'S90.42', 'S91.051', 'S92.009', 'S92.312', 'S92.322', 'S93.401', 'S93.402', 'S96.002', 'T14.8', 'T14.90', 'T14.91', 'T17.408', 'T17.810', 'T18.2XX', 'T18.3XX', 'T18.9XX', 'T19.2XX', 'T20.00', 'T20.02X', 'T20.04X', 'T20.30X', 'T21.22X', 'T24.009', 'T24.011', 'T24.012', 'T24.211', 'T30.0', 'T31.2', 'T31.20', 'T36.0X5', 'T38.0X5', 'T48.3X1', 'T50.8X5', 'T56.0', 'T68.XXX', 'T74.2', 'T75.1XX', 'T78.2XX', 'T78.3XX', 'T79.7', 'T79.7XX', 'T79.A0X', 'T80.810', 'T81.30', 'T81.31X', 'T81.4', 'T81.49X', 'T81.509', 'T81.83X', 'T82.01X', 'T82.510', 'T82.7XX', 'T82.858', 'T82.868', 'T85.41X', 'T85.44X', 'T86.11', 'T86.822', 'T86.829', 'V00.121', 'V19.3XX', 'V19.9XX', 'V23.9XX', 'V29.99X', 'V37.0', 'V87.9', 'V87.9XX', 'V89.2XX', 'V89.9XX', 'V99.XXX', 'W01.0XX', 'W16.011', 'W19.XXX', 'W21.00X', 'W32.0XX', 'W34.00', 'W34.00X', 'W40.9XX', 'W55.03', 'W55.22X', 'W56.81X', 'W57.XXX', 'W86.8XX', 'X08.8XX', 'X58.XXX', 'X73.0XX', 'Y84.2', 'Z16.11', 'Z16.19', 'Z16.20', 'Z16.24', 'Z16.35', 'Z17.0', 'Z17.1', 'Z20.818', 'Z37.0', 'Z3A.08', 'Z3A.16', 'Z3A.18', 'Z3A.19', 'Z3A.20', 'Z3A.21', 'Z3A.22', 'Z3A.23', 'Z3A.24', 'Z3A.27', 'Z3A.34', 'Z3A.35', 'Z3A.38', 'Z3A.39', 'Z3A.40', 'Z51.5', 'Z53.20', 'Z53.29', 'Z53.3', 'Z53.31', 'Z53.8', 'Z56.2', 'Z60.2', 'Z65.1', 'Z67.10', 'Z67.11', 'Z68.1', 'Z68.21', 'Z68.22', 'Z68.25', 'Z68.26', 'Z68.27', 'Z68.34', 'Z68.36', 'Z68.4', 'Z68.41', 'Z68.42', 'Z68.43', 'Z68.45', 'Z74.01', 'Z75.1', 'Z76.82', 'Z77.29', 'Z79.01', 'Z79.02', 'Z79.1', 'Z79.3', 'Z79.4', 'Z79.52', 'Z79.810', 'Z79.82', 'Z79.83', 'Z79.84', 'Z79.890', 'Z80.0', 'Z80.1', 'Z80.3', 'Z82.41', 'Z82.49', 'Z83.3', 'Z85.3', 'Z86.11', 'Z86.12', 'Z86.61', 'Z86.718', 'Z86.73', 'Z87.01', 'Z87.11', 'Z87.440', 'Z87.442', 'Z87.81', 'Z87.891', 'Z88.0', 'Z88.1', 'Z88.2', 'Z88.8', 'Z90.3', 'Z90.49', 'Z90.5', 'Z90.710', 'Z90.721', 'Z90.722', 'Z90.79', 'Z90.81', 'Z91.041', 'Z91.048', 'Z91.19', 'Z92.0', 'Z92.21', 'Z92.3', 'Z93.0', 'Z93.2', 'Z94.0', 'Z94.1', 'Z94.2', 'Z95.0', 'Z95.1', 'Z95.2', 'Z96.0', 'Z96.1', 'Z97.4', 'Z97.5', 'Z98.2', 'Z98.52', 'Z98.84', 'Z98.85', 'Z99.2', 'Z99.3', 'Z99.81', 'Z99.89']\n",
      "Percentage of correct parent labels: 100.00% | 100.00%\n",
      "Epoch 1 | Loss: 1.4293 | LR: 1.50E-05\n",
      "F1 Validation | Micro: 0.01453 | Macro: 0.01407 | Best: 0.01407 | Epochs without improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 125/125 [00:10<00:00, 12.33it/s, loss=0.357, lr=3e-5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['A01', 'A02', 'A03', 'A04', 'A06', 'A08', 'A09', 'A15', 'A17', 'A18', 'A19', 'A23', 'A28', 'A31', 'A32', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A48', 'A49', 'A53', 'A54', 'A55', 'A60', 'A63', 'A64', 'A69', 'A74', 'A78', 'A79', 'A80', 'A86', 'A87', 'A90', 'A91', 'B00', 'B01', 'B02', 'B05', 'B06', 'B07', 'B08', 'B10', 'B15', 'B17', 'B18', 'B19', 'B20', 'B25', 'B26', 'B27', 'B30', 'B33', 'B34', 'B35', 'B37', 'B44', 'B45', 'B46', 'B48', 'B49', 'B55', 'B57', 'B58', 'B59', 'B65', 'B67', 'B69', 'B74', 'B81', 'B83', 'B85', 'B91', 'B95', 'B96', 'B97', 'B99', 'C02', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'C15', 'C16', 'C17', 'C18', 'C20', 'C21', 'C22', 'C25', 'C26', 'C30', 'C31', 'C32', 'C34', 'C38', 'C40', 'C41', 'C43', 'C44', 'C47', 'C48', 'C49', 'C50', 'C52', 'C53', 'C54', 'C55', 'C56', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C83', 'C84', 'C85', 'C88', 'C90', 'C91', 'C92', 'C94', 'C96', 'D09', 'D10', 'D11', 'D12', 'D13', 'D15', 'D16', 'D17', 'D18', 'D21', 'D22', 'D23', 'D24', 'D25', 'D27', 'D29', 'D30', 'D31', 'D32', 'D33', 'D35', 'D36', 'D37', 'D38', 'D3A', 'D40', 'D41', 'D43', 'D44', 'D46', 'D47', 'D48', 'D49', 'D50', 'D53', 'D56', 'D57', 'D58', 'D61', 'D62', 'D63', 'D64', 'D65', 'D66', 'D68', 'D69', 'D70', 'D72', 'D73', 'D74', 'D75', 'D76', 'D80', 'D84', 'D86', 'D89', 'E03', 'E04', 'E05', 'E06', 'E07', 'E10', 'E11', 'E13', 'E16', 'E21', 'E22', 'E23', 'E24', 'E27', 'E28', 'E29', 'E31', 'E43', 'E44', 'E46', 'E50', 'E51', 'E53', 'E54', 'E55', 'E56', 'E61', 'E63', 'E66', 'E70', 'E72', 'E73', 'E74', 'E75', 'E77', 'E78', 'E79', 'E80', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'F02', 'F05', 'F09', 'F10', 'F11', 'F12', 'F14', 'F17', 'F19', 'F20', 'F23', 'F25', 'F29', 'F30', 'F31', 'F32', 'F34', 'F39', 'F40', 'F41', 'F43', 'F45', 'F50', 'F60', 'F63', 'F65', 'F70', 'F71', 'F72', 'F73', 'F79', 'F80', 'F84', 'F90', 'F91', 'F98', 'F99', 'G00', 'G03', 'G04', 'G06', 'G08', 'G12', 'G20', 'G24', 'G25', 'G30', 'G31', 'G35', 'G36', 'G37', 'G40', 'G43', 'G44', 'G45', 'G47', 'G50', 'G51', 'G54', 'G56', 'G57', 'G58', 'G60', 'G61', 'G62', 'G70', 'G71', 'G72', 'G81', 'G82', 'G83', 'G89', 'G90', 'G91', 'G93', 'G95', 'G96', 'G97', 'H00', 'H01', 'H02', 'H04', 'H05', 'H10', 'H11', 'H15', 'H16', 'H17', 'H18', 'H20', 'H21', 'H25', 'H26', 'H27', 'H30', 'H31', 'H33', 'H34', 'H35', 'H40', 'H43', 'H44', 'H46', 'H47', 'H49', 'H50', 'H51', 'H52', 'H53', 'H54', 'H55', 'H57', 'H59', 'H60', 'H66', 'H80', 'H90', 'H91', 'H92', 'H93', 'I05', 'I07', 'I08', 'I10', 'I11', 'I12', 'I15', 'I16', 'I20', 'I21', 'I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I30', 'I31', 'I33', 'I34', 'I35', 'I37', 'I38', 'I42', 'I43', 'I44', 'I45', 'I46', 'I47', 'I48', 'I49', 'I50', 'I51', 'I60', 'I61', 'I62', 'I63', 'I65', 'I66', 'I67', 'I69', 'I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I80', 'I81', 'I82', 'I83', 'I85', 'I86', 'I87', 'I88', 'I89', 'I95', 'I96', 'I99', 'J00', 'J01', 'J02', 'J03', 'J06', 'J10', 'J11', 'J12', 'J15', 'J16', 'J18', 'J21', 'J22', 'J30', 'J32', 'J33', 'J34', 'J35', 'J38', 'J39', 'J42', 'J43', 'J44', 'J45', 'J47', 'J60', 'J62', 'J64', 'J69', 'J80', 'J81', 'J82', 'J84', 'J85', 'J86', 'J90', 'J91', 'J93', 'J94', 'J95', 'J96', 'J98', 'J99', 'K00', 'K01', 'K02', 'K04', 'K05', 'K06', 'K08', 'K09', 'K11', 'K12', 'K13', 'K14', 'K20', 'K21', 'K22', 'K25', 'K26', 'K27', 'K29', 'K30', 'K31', 'K35', 'K36', 'K37', 'K38', 'K40', 'K42', 'K43', 'K44', 'K46', 'K50', 'K51', 'K52', 'K55', 'K56', 'K57', 'K59', 'K60', 'K61', 'K62', 'K63', 'K64', 'K65', 'K66', 'K68', 'K70', 'K71', 'K72', 'K73', 'K74', 'K75', 'K76', 'K80', 'K81', 'K82', 'K83', 'K85', 'K86', 'K90', 'K91', 'K92', 'L00', 'L02', 'L03', 'L08', 'L10', 'L11', 'L20', 'L21', 'L25', 'L27', 'L28', 'L29', 'L30', 'L40', 'L42', 'L43', 'L50', 'L51', 'L52', 'L53', 'L57', 'L58', 'L60', 'L65', 'L68', 'L70', 'L71', 'L72', 'L74', 'L76', 'L80', 'L81', 'L83', 'L85', 'L88', 'L89', 'L90', 'L91', 'L92', 'L93', 'L94', 'L95', 'L97', 'L98', 'M00', 'M04', 'M05', 'M06', 'M08', 'M13', 'M15', 'M16', 'M17', 'M19', 'M21', 'M24', 'M25', 'M26', 'M27', 'M30', 'M31', 'M32', 'M33', 'M34', 'M35', 'M41', 'M43', 'M45', 'M46', 'M47', 'M48', 'M50', 'M51', 'M53', 'M54', 'M60', 'M61', 'M62', 'M65', 'M67', 'M70', 'M71', 'M72', 'M75', 'M79', 'M80', 'M81', 'M83', 'M84', 'M85', 'M86', 'M87', 'M88', 'M89', 'M92', 'M94', 'M95', 'M96', 'N00', 'N01', 'N02', 'N03', 'N04', 'N05', 'N11', 'N12', 'N13', 'N14', 'N15', 'N17', 'N18', 'N19', 'N20', 'N21', 'N23', 'N25', 'N26', 'N28', 'N29', 'N30', 'N31', 'N32', 'N34', 'N35', 'N36', 'N39', 'N40', 'N41', 'N42', 'N43', 'N44', 'N45', 'N46', 'N48', 'N49', 'N50', 'N52', 'N53', 'N60', 'N62', 'N63', 'N64', 'N76', 'N80', 'N81', 'N82', 'N83', 'N85', 'N89', 'N90', 'N91', 'N92', 'N93', 'N94', 'N96', 'N97', 'O02', 'O03', 'O14', 'O21', 'O24', 'O30', 'O32', 'O36', 'O40', 'O41', 'O42', 'O45', 'O47', 'O60', 'O72', 'O75', 'O80', 'O92', 'O99', 'P07', 'P52', 'P81', 'P83', 'P84', 'P91', 'Q02', 'Q03', 'Q04', 'Q05', 'Q07', 'Q10', 'Q12', 'Q13', 'Q14', 'Q18', 'Q21', 'Q23', 'Q24', 'Q25', 'Q27', 'Q28', 'Q30', 'Q31', 'Q32', 'Q35', 'Q37', 'Q38', 'Q42', 'Q43', 'Q44', 'Q45', 'Q51', 'Q52', 'Q53', 'Q55', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64', 'Q67', 'Q68', 'Q69', 'Q70', 'Q71', 'Q74', 'Q75', 'Q76', 'Q78', 'Q79', 'Q82', 'Q83', 'Q85', 'Q87', 'Q89', 'Q90', 'Q98', 'R00', 'R01', 'R04', 'R05', 'R06', 'R07', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R22', 'R23', 'R25', 'R26', 'R27', 'R29', 'R30', 'R31', 'R32', 'R33', 'R34', 'R35', 'R36', 'R39', 'R40', 'R41', 'R42', 'R43', 'R44', 'R45', 'R46', 'R47', 'R48', 'R49', 'R50', 'R51', 'R52', 'R53', 'R55', 'R56', 'R57', 'R58', 'R59', 'R60', 'R61', 'R62', 'R63', 'R64', 'R65', 'R68', 'R69', 'R71', 'R73', 'R74', 'R76', 'R78', 'R79', 'R80', 'R81', 'R82', 'R86', 'R89', 'R90', 'R91', 'R97', 'R99', 'S00', 'S01', 'S02', 'S03', 'S05', 'S06', 'S09', 'S10', 'S11', 'S19', 'S20', 'S21', 'S22', 'S23', 'S25', 'S27', 'S30', 'S31', 'S32', 'S34', 'S35', 'S36', 'S37', 'S39', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S50', 'S52', 'S53', 'S60', 'S61', 'S62', 'S63', 'S69', 'S70', 'S71', 'S72', 'S80', 'S81', 'S82', 'S83', 'S84', 'S85', 'S86', 'S89', 'S90', 'S91', 'S92', 'S93', 'S96', 'T07', 'T14', 'T17', 'T18', 'T19', 'T20', 'T21', 'T24', 'T30', 'T31', 'T36', 'T38', 'T48', 'T50', 'T56', 'T68', 'T74', 'T75', 'T78', 'T79', 'T80', 'T81', 'T82', 'T85', 'T86', 'V00', 'V19', 'V23', 'V29', 'V37', 'V87', 'V89', 'V99', 'W01', 'W16', 'W19', 'W21', 'W32', 'W34', 'W40', 'W55', 'W56', 'W57', 'W86', 'X08', 'X58', 'X73', 'Y09', 'Y84', 'Y95', 'Z16', 'Z17', 'Z20', 'Z21', 'Z37', 'Z3A', 'Z51', 'Z53', 'Z56', 'Z60', 'Z65', 'Z67', 'Z68', 'Z74', 'Z75', 'Z76', 'Z77', 'Z79', 'Z80', 'Z82', 'Z83', 'Z85', 'Z86', 'Z87', 'Z88', 'Z90', 'Z91', 'Z92', 'Z93', 'Z94', 'Z95', 'Z96', 'Z97', 'Z98', 'Z99']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A01.00', 'A03.9', 'A04.5', 'A04.8', 'A06.9', 'A08.0', 'A15.9', 'A17.81', 'A18.10', 'A18.12', 'A23.9', 'A39.1', 'A40.3', 'A41.1', 'A42.9', 'A53.9', 'A60.00', 'A69.20', 'B01.9', 'B02.21', 'B02.30', 'B05.9', 'B07.9', 'B08.3', 'B10.89', 'B19.10', 'B27.00', 'B30.9', 'B34.9', 'B37.0', 'B37.2', 'B37.81', 'B37.9', 'B46.5', 'B46.9', 'B48.8', 'B57.2', 'B58.2', 'B58.9', 'B67.99', 'B83.0', 'B95.3', 'B95.5', 'B95.61', 'B96.20', 'B96.3', 'B96.7', 'B96.89', 'B97.4', 'B97.7', 'B99.9', 'C02.0', 'C02.1', 'C05.9', 'C06.0', 'C08.0', 'C17.1', 'C22.0', 'C30.0', 'C34.9', 'C40.20', 'C41.0', 'C41.9', 'C43.31', 'C43.9', 'C44.42', 'C44.699', 'C48.2', 'C49.21', 'C49.4', 'C49.5', 'C49.A2', 'C50.9', 'C50.919', 'C60.1', 'C62.90', 'C62.91', 'C63.10', 'C63.8', 'C64.1', 'C67.2', 'C67.7', 'C69.30', 'C70.0', 'C71.0', 'C75.0', 'C77.1', 'C77.8', 'C78.02', 'C78.1', 'C78.5', 'C78.7', 'C78.89', 'C79.0', 'C79.2', 'C79.49', 'C79.71', 'C79.72', 'C80.0', 'C81.90', 'C83.0', 'C84.99', 'C84.A', 'C85.9', 'C90.00', 'C90.3', 'C91.00', 'C91.01', 'C91.10', 'C92.3', 'C94.6', 'D09.0', 'D11.0', 'D13.6', 'D13.9', 'D17.23', 'D17.5', 'D17.71', 'D18.01', 'D18.03', 'D21.20', 'D22.9', 'D24.9', 'D29.31', 'D29.8', 'D30.02', 'D31.9', 'D35.00', 'D35.02', 'D37.8', 'D38.3', 'D40.10', 'D41.01', 'D41.3', 'D44.10', 'D44.12', 'D47.0', 'D47.9', 'D48.1', 'D49.3', 'D49.511', 'D49.519', 'D50.9', 'D56.1', 'D56.3', 'D64.9', 'D68.52', 'D69.2', 'D69.41', 'D72.822', 'D72.829', 'D73.1', 'D73.89', 'D74.8', 'D75.89', 'D80.0', 'E03.9', 'E04.2', 'E05.90', 'E06.1', 'E10.10', 'E11.628', 'E11.9', 'E21.3', 'E27.9', 'E50.9', 'E56.9', 'E66.3', 'E70.39', 'E73.9', 'E74.00', 'E74.04', 'E78.1', 'E80.20', 'E80.21', 'E80.4', 'E80.7', 'E83.01', 'E83.42', 'E83.51', 'E83.52', 'E83.59', 'E84.9', 'E85.2', 'E85.3', 'E85.4', 'E87.2', 'E87.3', 'E87.5', 'E88.09', 'E88.9', 'E89.0', 'F10.20', 'F10.921', 'F10.96', 'F12.10', 'F14.10', 'F17.200', 'F19.20', 'F19.21', 'F20.0', 'F20.2', 'F20.5', 'F41.0', 'F43.20', 'F43.9', 'F45.29', 'F60.3', 'F60.6', 'F60.7', 'F63.9', 'F65.3', 'F98.8', 'G00.1', 'G12.21', 'G25.2', 'G25.3', 'G30.9', 'G37.3', 'G40.109', 'G40.119', 'G40.309', 'G40.409', 'G44.89', 'G47.30', 'G56.20', 'G58.9', 'G82.20', 'G83.14', 'G83.9', 'G93.0', 'G93.2', 'G93.6', 'G93.82', 'G95.20', 'G96.19', 'H02.409', 'H02.411', 'H02.843', 'H02.849', 'H02.9', 'H04.20', 'H05.20', 'H11.001', 'H11.133', 'H11.42', 'H11.421', 'H11.429', 'H11.431', 'H11.432', 'H15.032', 'H15.10', 'H16.442', 'H17.9', 'H18.20', 'H18.891', 'H20.052', 'H20.9', 'H21.26', 'H21.309', 'H21.42', 'H21.519', 'H21.542', 'H21.9', 'H27.10', 'H31.101', 'H33.19', 'H33.20', 'H33.22', 'H34.9', 'H35.06', 'H35.33', 'H35.52', 'H35.6', 'H35.89', 'H40.05', 'H40.052', 'H40.9', 'H44.00', 'H46.10', 'H47.091', 'H49.01', 'H49.9', 'H50.00', 'H50.012', 'H50.112', 'H52.1', 'H52.10', 'H53.142', 'H53.47', 'H57.02', 'H57.03', 'H57.04', 'H57.1', 'H59.03', 'H66.90', 'H91.20', 'H93.19', 'I05.0', 'I05.9', 'I12.0', 'I23.0', 'I24.9', 'I25.10', 'I25.9', 'I31.4', 'I35.8', 'I42.9', 'I44.2', 'I45.10', 'I45.2', 'I48.92', 'I49.3', 'I50.9', 'I51.4', 'I51.7', 'I60.9', 'I61.1', 'I61.4', 'I61.8', 'I62.03', 'I63.511', 'I63.512', 'I63.519', 'I67.2', 'I69.354', 'I70.90', 'I71.00', 'I71.1', 'I71.4', 'I71.9', 'I72.1', 'I72.8', 'I77.0', 'I77.6', 'I77.70', 'I77.819', 'I77.9', 'I82.40', 'I82.402', 'I82.431', 'I82.90', 'I83.93', 'I85.00', 'I87.2', 'I89.1', 'I95.9', 'J02.9', 'J10.1', 'J11.0', 'J16.0', 'J18.9', 'J32.4', 'J34.3', 'J38.4', 'J43.9', 'J45.909', 'J69.8', 'J84.112', 'J84.89', 'J95.821', 'J98.4', 'K00.0', 'K01.0', 'K04.7', 'K05.10', 'K06.8', 'K06.9', 'K08.9', 'K11.9', 'K13.29', 'K13.70', 'K13.79', 'K14.5', 'K14.6', 'K21.9', 'K22.6', 'K22.8', 'K25.9', 'K26.9', 'K27.9', 'K29.80', 'K31.5', 'K31.9', 'K35.80', 'K44.9', 'K50.00', 'K51.00', 'K52.3', 'K55.069', 'K55.1', 'K55.20', 'K55.21', 'K56.0', 'K56.1', 'K56.5', 'K56.60', 'K56.7', 'K57.10', 'K57.11', 'K57.20', 'K57.30', 'K57.92', 'K59.00', 'K59.8', 'K59.9', 'K62.0', 'K62.4', 'K62.7', 'K63.1', 'K63.2', 'K63.3', 'K64.9', 'K65.9', 'K66.8', 'K68.19', 'K68.9', 'K70.10', 'K72.00', 'K73.9', 'K75.9', 'K76.1', 'K76.6', 'K83.3', 'K83.8', 'K86.9', 'K90.9', 'K91.3', 'K92.0', 'K92.2', 'L02.31', 'L02.32', 'L02.41', 'L02.91', 'L03.115', 'L10.0', 'L27.0', 'L28.2', 'L29.9', 'L43.9', 'L53.9', 'L58.9', 'L60.8', 'L60.9', 'L71.9', 'L76.3', 'L89.159', 'L90.5', 'L92.9', 'L97.919', 'L98.0', 'M16.0', 'M16.11', 'M21.932', 'M24.60', 'M25.541', 'M25.552', 'M25.561', 'M25.569', 'M26.02', 'M26.09', 'M26.30', 'M26.52', 'M26.601', 'M27.2', 'M27.49', 'M32.9', 'M35.00', 'M35.3', 'M43.6', 'M46.34', 'M46.40', 'M46.44', 'M47.896', 'M51.27', 'M53.3', 'M54.2', 'M54.5', 'M62.00', 'M62.81', 'M62.838', 'M65.9', 'M72.4', 'M79.0', 'M79.2', 'M79.604', 'M79.605', 'M79.621', 'M79.643', 'M79.644', 'M79.645', 'M79.646', 'M79.7', 'M79.81', 'M80.88X', 'M81.0', 'M84.30X', 'M84.529', 'M85.8', 'M85.80', 'M86.68', 'M86.9', 'M89.061', 'M89.549', 'M89.59', 'M92.50', 'M95.2', 'N01.9', 'N02.8', 'N05.5', 'N13.30', 'N13.8', 'N13.9', 'N14.0', 'N15.1', 'N17.0', 'N18.2', 'N18.3', 'N18.4', 'N18.5', 'N18.9', 'N26.1', 'N26.9', 'N28.1', 'N28.89', 'N28.9', 'N30.90', 'N31.1', 'N31.9', 'N35.9', 'N36.1', 'N39.3', 'N39.44', 'N40.0', 'N41.9', 'N42.30', 'N42.9', 'N44.00', 'N45.3', 'N46.01', 'N46.11', 'N48.21', 'N48.29', 'N48.81', 'N48.89', 'N50.3', 'N50.82', 'N50.89', 'N50.9', 'N60.02', 'N60.19', 'N81.2', 'N82.1', 'N83.00', 'N85.9', 'N89.5', 'N94.10', 'N97.9', 'O14.20', 'O24.419', 'O36.4XX', 'O40.9XX0', 'O41.0', 'O42.912', 'O42.92', 'O45.9', 'O47.9', 'O72.1', 'P07.18', 'P91.60', 'Q03.0', 'Q04.3', 'Q04.9', 'Q07.00', 'Q07.9', 'Q13.0', 'Q27.30', 'Q27.9', 'Q28.2', 'Q30.9', 'Q38.2', 'Q43.9', 'Q45.0', 'Q51.811', 'Q52.0', 'Q53.9', 'Q55.22', 'Q60.2', 'Q62.8', 'Q64.10', 'Q64.12', 'Q64.4', 'Q64.74', 'Q67.0', 'Q75.2', 'Q83.9', 'Q85.00', 'Q85.1', 'Q87.3', 'R00.0', 'R00.1', 'R04.2', 'R04.89', 'R06.00', 'R06.01', 'R06.1', 'R06.4', 'R07.1', 'R07.81', 'R09.02', 'R09.81', 'R10.0', 'R10.13', 'R10.2', 'R10.32', 'R10.817', 'R10.84', 'R10.9', 'R11.10', 'R11.2', 'R13.10', 'R16.1', 'R16.2', 'R19.2', 'R19.32', 'R19.5', 'R19.7', 'R20.0', 'R20.2', 'R20.3', 'R20.8', 'R22.0', 'R23.4', 'R29.2', 'R29.723', 'R29.810', 'R29.898', 'R31.0', 'R31.29', 'R31.9', 'R39.198', 'R39.9', 'R40.2410', 'R41.0', 'R41.3', 'R44.1', 'R45.1', 'R45.3', 'R45.4', 'R47.01', 'R47.1', 'R49.1', 'R50.82', 'R50.9', 'R53.1', 'R53.83', 'R56.9', 'R57.0', 'R57.1', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R62.50', 'R63.0', 'R63.4', 'R68.84', 'R71.8', 'R76.0', 'R82.2', 'R86.9', 'R90.0', 'R91.1', 'R91.8', 'S00.522', 'S00.532', 'S01.101', 'S01.502', 'S01.95X', 'S02.109', 'S02.19X', 'S02.5XX', 'S02.601', 'S02.609', 'S03.2XX', 'S05.8X2', 'S06.2X', 'S06.4X0', 'S06.9X0', 'S06.9X9', 'S09.93X', 'S20.92X', 'S22.41X', 'S23.163', 'S27.329', 'S30.0XX', 'S30.23', 'S31.831', 'S32.602', 'S34.139', 'S36.09', 'S37.812', 'S39.94', 'S42.409', 'S43.40', 'S50.10X', 'S62.012', 'S63.071', 'S70.312', 'S72.002', 'S72.302', 'S80.821', 'S82.101', 'S82.40', 'S83.106', 'S89.90X', 'S91.051', 'S92.312', 'S96.002', 'T14.8', 'T17.408', 'T18.2XX', 'T19.2XX', 'T38.0X5', 'T79.A0X', 'T80.810', 'T81.4', 'T81.509', 'T81.83X', 'T82.01X', 'T85.41X', 'T86.11', 'T86.829', 'V19.3XX', 'V19.9XX', 'V37.0', 'W19.XXX', 'W21.00X', 'W34.00', 'W55.22X', 'W86.8XX', 'X58.XXX', 'X73.0XX', 'Y84.2', 'Z20.818', 'Z3A.08', 'Z3A.16', 'Z3A.20', 'Z3A.21', 'Z3A.35', 'Z53.3', 'Z53.8', 'Z60.2', 'Z67.10', 'Z67.11', 'Z68.21', 'Z68.27', 'Z68.34', 'Z68.36', 'Z68.41', 'Z76.82', 'Z77.29', 'Z79.810', 'Z79.83', 'Z80.3', 'Z82.41', 'Z82.49', 'Z86.12', 'Z86.718', 'Z87.440', 'Z87.891', 'Z88.0', 'Z90.3', 'Z90.49', 'Z90.710', 'Z90.721', 'Z91.041', 'Z92.0', 'Z92.3', 'Z93.0', 'Z93.2', 'Z96.0', 'Z98.52', 'Z99.3', 'Z99.81']\n",
      "Percentage of correct parent labels: 100.00% | 45.45%\n",
      "Epoch 2 | Loss: 0.5935 | LR: 3.00E-05\n",
      "F1 Validation | Micro: 0.01584 | Macro: 0.01257 | Best: 0.01407 | Epochs without improvement: 3\n",
      "Nuevos umbrales: Parent=0.115, Child=0.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  55%|█████▌    | 69/125 [00:05<00:04, 12.52it/s, loss=0.21, lr=3e-5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 457\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    455\u001b[0m     best_thresholds \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mTHRESHOLDS\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_thresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Cargar datos de test\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mDATA_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 250\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(best_thresholds)\u001b[0m\n\u001b[1;32m    242\u001b[0m loss \u001b[38;5;241m=\u001b[39m hierarchical_loss(\n\u001b[1;32m    243\u001b[0m     outputs[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# parent_logits\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     outputs[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# child_logits\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     parent_labels,\n\u001b[1;32m    246\u001b[0m     child_labels\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m Config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS\n\u001b[0;32m--> 250\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m Config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    253\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# V2 modelo jerárquico\n",
    "# # ====================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  Modelo v2\n",
    "# ====================\n",
    "class HierarchicalBERTv2(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(Config.MODEL_NAME, output_hidden_states=True)\n",
    "        self.bert = AutoModel.from_pretrained(Config.MODEL_NAME, config=config)\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if Config.USE_FEATURE_PYRAMID:\n",
    "            # Combine last 3 layers' [CLS] embeddings\n",
    "            hidden_states = outputs.hidden_states[-3:]  # Get last 3 layers\n",
    "            # Stack [CLS] embeddings (shape: [3, batch_size, hidden_size])\n",
    "            pooled = torch.stack([state[:, 0] for state in hidden_states])\n",
    "            # Weighted combination of layers (weights should sum to 1)\n",
    "            pooled = torch.einsum('lbd,l->bd', pooled,\n",
    "                                torch.tensor(Config.FEATURE_LAYER_WEIGHTS).to(pooled.device))\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Jerarquía de clasificación\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "#  FUNCIÓN DE PÉRDIDA MEJORADA\n",
    "# ====================\n",
    "def hierarchical_lossv2(parent_logits, child_logits,\n",
    "                     parent_labels, child_labels,\n",
    "                     parent_weights, child_weights):\n",
    "\n",
    "    loss_parent = F.binary_cross_entropy_with_logits(\n",
    "        parent_logits,\n",
    "        parent_labels,\n",
    "        pos_weight=parent_weights\n",
    "    )\n",
    "\n",
    "    loss_child = F.binary_cross_entropy_with_logits(\n",
    "        child_logits,\n",
    "        child_labels,\n",
    "        pos_weight=child_weights\n",
    "    )\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)\n",
    "\n",
    "# ====================\n",
    "#  AJUSTE DINÁMICO DE UMBRALES\n",
    "# ====================\n",
    "def calculate_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = {}\n",
    "    for i in range(y_probs.shape[1]):\n",
    "        if np.sum(y_true[:, i]) > 0:  # Solo clases presentes\n",
    "            precision, recall, threshs = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_idx = np.nanargmax(f1_scores)\n",
    "            thresholds[i] = threshs[best_idx]\n",
    "    return thresholds\n",
    "\n",
    "# ====================\n",
    "#  Dataset\n",
    "# ====================\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        for codes in df['labels'].apply(eval): # FIXME: Unsafe eval\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        for idx in range(len(self.texts)):\n",
    "            encoding = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO\n",
    "# ====================\n",
    "def train(best_thresholds=Config.THRESHOLDS):\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERTv2(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.path.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "        else:\n",
    "            print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "\n",
    "    # Calcular pesos de clases\n",
    "    parent_counts = np.sum(train_dataset.parent_labels, axis=0)\n",
    "    parent_weights = (len(train_dataset) - parent_counts) / (parent_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    parent_weights = torch.tensor(parent_weights).to(device)\n",
    "\n",
    "    child_counts = np.sum(train_dataset.child_labels, axis=0)\n",
    "    child_weights = (len(train_dataset) - child_counts) / (child_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    child_weights = torch.tensor(child_weights).to(device)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Ajuste periódico de umbrales\n",
    "        if (epoch + 1) % Config.THRESHOLD_TUNING_INTERVAL == 0:\n",
    "            val_probs, val_labels = get_validation_probabilities(model, val_loader, device)\n",
    "\n",
    "            # Calcular mejores umbrales por clase\n",
    "            parent_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['parent'], val_probs['parent']\n",
    "            )\n",
    "            child_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['child'], val_probs['child']\n",
    "            )\n",
    "\n",
    "            # Actualizar umbrales globales\n",
    "            best_thresholds['parent'] = np.mean(list(parent_thresholds.values()))\n",
    "            best_thresholds['child'] = np.mean(list(child_thresholds.values()))\n",
    "            print(f\"Nuevos umbrales: Parent={best_thresholds['parent']:.3f}, Child={best_thresholds['child']:.3f}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                # Línea corregida\n",
    "                loss = hierarchical_loss(\n",
    "                    outputs[0],  # parent_logits\n",
    "                    outputs[1],  # child_logits\n",
    "                    parent_labels,\n",
    "                    child_labels\n",
    "                )\n",
    "\n",
    "                loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device, mlb_parent, mlb_child, best_thresholds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "        # Store metrics\n",
    "\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if epoch == 0:\n",
    "            best_f1 = loss_metric\n",
    "\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN):\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_3\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'model': \"v2\",\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'parent_threshold': best_thresholds['parent'],\n",
    "            'child_threshold': best_thresholds['child'],\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "# ====================\n",
    "#  FUNCIONES AUXILIARES\n",
    "# ====================\n",
    "def get_validation_probabilities(model, dataloader, device):\n",
    "    model.eval()\n",
    "    parent_probs, child_probs = [], []\n",
    "    parent_labels, child_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            p_logits, c_logits, _ = model(**inputs)\n",
    "\n",
    "            parent_probs.append(torch.sigmoid(p_logits).cpu().numpy())\n",
    "            child_probs.append(torch.sigmoid(c_logits).cpu().numpy())\n",
    "\n",
    "            parent_labels.append(batch['parent_labels'].numpy())\n",
    "            child_labels.append(batch['child_labels'].numpy())\n",
    "\n",
    "    return {\n",
    "        'parent': np.concatenate(parent_probs),\n",
    "        'child': np.concatenate(child_probs)\n",
    "    }, {\n",
    "        'parent': np.concatenate(parent_labels),\n",
    "        'child': np.concatenate(child_labels)\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child, thresholds):\n",
    "    model.eval()\n",
    "    parent_preds_all = []\n",
    "    child_preds_all = []\n",
    "    parent_labels_all = []\n",
    "    child_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            parent_labels = batch['parent_labels'].numpy()\n",
    "            child_labels = batch['child_labels'].numpy()\n",
    "\n",
    "            parent_logits, child_logits, pooled = model(**inputs)\n",
    "\n",
    "            # Convert to predictions\n",
    "            parent_preds = (torch.sigmoid(parent_logits).cpu().numpy() > thresholds['parent']).astype(int)\n",
    "            child_preds = (torch.sigmoid(child_logits).cpu().numpy() > thresholds['child']).astype(int)\n",
    "\n",
    "            # Append to lists\n",
    "            parent_preds_all.extend(parent_preds)\n",
    "            child_preds_all.extend(child_preds)\n",
    "            parent_labels_all.extend(parent_labels)\n",
    "            child_labels_all.extend(child_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    parent_preds_all = np.array(parent_preds_all)\n",
    "    child_preds_all = np.array(child_preds_all)\n",
    "    parent_labels_all = np.array(parent_labels_all)\n",
    "    child_labels_all = np.array(child_labels_all)\n",
    "\n",
    "    # Print example comparison for parent level\n",
    "    if len(parent_labels_all) > 0:\n",
    "        parent_true = np.array(mlb_parent.classes_)[parent_labels_all[0].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(parent_true) & set(parent_pred))\n",
    "        total_labels = len(set(parent_true))\n",
    "        accuracy_parent = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[child_labels_all[0].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(child_true) & set(child_pred))\n",
    "        total_labels = len(set(child_true))\n",
    "        accuracy_child = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        print(\"Expected parent labels:\", sorted(parent_true))\n",
    "        print(\"Predicted parent labels:\", sorted(parent_pred))\n",
    "        print(\"Expected child labels:\", sorted(child_true))\n",
    "        print(\"Predicted child labels:\", sorted(child_pred))\n",
    "\n",
    "        print(f\"Percentage of correct parent labels: {accuracy_parent:.2%} | {accuracy_child:.2%}\")\n",
    "\n",
    "    # Calculate F1 scores for each level\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(parent_labels_all, parent_preds_all, average='micro' , zero_division=0),\n",
    "        'f1_macro_parent': f1_score(parent_labels_all, parent_preds_all, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(child_labels_all, child_preds_all, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(child_labels_all, child_preds_all, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Calculate weighted average F1 scores\n",
    "    metrics['f1_micro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    metrics['f1_macro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device, thresholds):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > thresholds['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > thresholds['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    best_thresholds = Config.THRESHOLDS\n",
    "\n",
    "    train(best_thresholds)\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.path.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device, best_thresholds)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padres: 983 - Hijos: 2462\n",
      "Loaded saved tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default BERT model\n",
      "Starting training from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 125/125 [00:09<00:00, 12.52it/s, loss=0.99, lr=1.5e-5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['A01', 'A02', 'A03', 'A04', 'A06', 'A08', 'A09', 'A15', 'A17', 'A18', 'A19', 'A23', 'A28', 'A31', 'A32', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A48', 'A49', 'A53', 'A54', 'A55', 'A60', 'A63', 'A64', 'A69', 'A74', 'A78', 'A79', 'A80', 'A86', 'A87', 'A90', 'A91', 'B00', 'B01', 'B02', 'B05', 'B06', 'B07', 'B08', 'B10', 'B15', 'B17', 'B18', 'B19', 'B20', 'B25', 'B26', 'B27', 'B30', 'B33', 'B34', 'B35', 'B37', 'B44', 'B45', 'B46', 'B48', 'B49', 'B55', 'B57', 'B58', 'B59', 'B65', 'B67', 'B69', 'B74', 'B81', 'B83', 'B85', 'B91', 'B95', 'B96', 'B97', 'B99', 'C02', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'C15', 'C16', 'C17', 'C18', 'C20', 'C21', 'C22', 'C25', 'C26', 'C30', 'C31', 'C32', 'C34', 'C38', 'C40', 'C41', 'C43', 'C44', 'C47', 'C48', 'C49', 'C50', 'C52', 'C53', 'C54', 'C55', 'C56', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C83', 'C84', 'C85', 'C88', 'C90', 'C91', 'C92', 'C94', 'C96', 'D09', 'D10', 'D11', 'D12', 'D13', 'D15', 'D16', 'D17', 'D18', 'D21', 'D22', 'D23', 'D24', 'D25', 'D27', 'D29', 'D30', 'D31', 'D32', 'D33', 'D35', 'D36', 'D37', 'D38', 'D3A', 'D40', 'D41', 'D43', 'D44', 'D46', 'D47', 'D48', 'D49', 'D50', 'D53', 'D56', 'D57', 'D58', 'D61', 'D62', 'D63', 'D64', 'D65', 'D66', 'D68', 'D69', 'D70', 'D72', 'D73', 'D74', 'D75', 'D76', 'D80', 'D84', 'D86', 'D89', 'E03', 'E04', 'E05', 'E06', 'E07', 'E10', 'E11', 'E13', 'E16', 'E21', 'E22', 'E23', 'E24', 'E27', 'E28', 'E29', 'E31', 'E43', 'E44', 'E46', 'E50', 'E51', 'E53', 'E54', 'E55', 'E56', 'E61', 'E63', 'E66', 'E70', 'E72', 'E73', 'E74', 'E75', 'E77', 'E78', 'E79', 'E80', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'F02', 'F05', 'F09', 'F10', 'F11', 'F12', 'F14', 'F17', 'F19', 'F20', 'F23', 'F25', 'F29', 'F30', 'F31', 'F32', 'F34', 'F39', 'F40', 'F41', 'F43', 'F45', 'F50', 'F60', 'F63', 'F65', 'F70', 'F71', 'F72', 'F73', 'F79', 'F80', 'F84', 'F90', 'F91', 'F98', 'F99', 'G00', 'G03', 'G04', 'G06', 'G08', 'G12', 'G20', 'G24', 'G25', 'G30', 'G31', 'G35', 'G36', 'G37', 'G40', 'G43', 'G44', 'G45', 'G47', 'G50', 'G51', 'G54', 'G56', 'G57', 'G58', 'G60', 'G61', 'G62', 'G70', 'G71', 'G72', 'G81', 'G82', 'G83', 'G89', 'G90', 'G91', 'G93', 'G95', 'G96', 'G97', 'H00', 'H01', 'H02', 'H04', 'H05', 'H10', 'H11', 'H15', 'H16', 'H17', 'H18', 'H20', 'H21', 'H25', 'H26', 'H27', 'H30', 'H31', 'H33', 'H34', 'H35', 'H40', 'H43', 'H44', 'H46', 'H47', 'H49', 'H50', 'H51', 'H52', 'H53', 'H54', 'H55', 'H57', 'H59', 'H60', 'H66', 'H80', 'H90', 'H91', 'H92', 'H93', 'I05', 'I07', 'I08', 'I10', 'I11', 'I12', 'I15', 'I16', 'I20', 'I21', 'I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I30', 'I31', 'I33', 'I34', 'I35', 'I37', 'I38', 'I42', 'I43', 'I44', 'I45', 'I46', 'I47', 'I48', 'I49', 'I50', 'I51', 'I60', 'I61', 'I62', 'I63', 'I65', 'I66', 'I67', 'I69', 'I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I80', 'I81', 'I82', 'I83', 'I85', 'I86', 'I87', 'I88', 'I89', 'I95', 'I96', 'I99', 'J00', 'J01', 'J02', 'J03', 'J06', 'J10', 'J11', 'J12', 'J15', 'J16', 'J18', 'J21', 'J22', 'J30', 'J32', 'J33', 'J34', 'J35', 'J38', 'J39', 'J42', 'J43', 'J44', 'J45', 'J47', 'J60', 'J62', 'J64', 'J69', 'J80', 'J81', 'J82', 'J84', 'J85', 'J86', 'J90', 'J91', 'J93', 'J94', 'J95', 'J96', 'J98', 'J99', 'K00', 'K01', 'K02', 'K04', 'K05', 'K06', 'K08', 'K09', 'K11', 'K12', 'K13', 'K14', 'K20', 'K21', 'K22', 'K25', 'K26', 'K27', 'K29', 'K30', 'K31', 'K35', 'K36', 'K37', 'K38', 'K40', 'K42', 'K43', 'K44', 'K46', 'K50', 'K51', 'K52', 'K55', 'K56', 'K57', 'K59', 'K60', 'K61', 'K62', 'K63', 'K64', 'K65', 'K66', 'K68', 'K70', 'K71', 'K72', 'K73', 'K74', 'K75', 'K76', 'K80', 'K81', 'K82', 'K83', 'K85', 'K86', 'K90', 'K91', 'K92', 'L00', 'L02', 'L03', 'L08', 'L10', 'L11', 'L20', 'L21', 'L25', 'L27', 'L28', 'L29', 'L30', 'L40', 'L42', 'L43', 'L50', 'L51', 'L52', 'L53', 'L57', 'L58', 'L60', 'L65', 'L68', 'L70', 'L71', 'L72', 'L74', 'L76', 'L80', 'L81', 'L83', 'L85', 'L88', 'L89', 'L90', 'L91', 'L92', 'L93', 'L94', 'L95', 'L97', 'L98', 'M00', 'M04', 'M05', 'M06', 'M08', 'M13', 'M15', 'M16', 'M17', 'M19', 'M21', 'M24', 'M25', 'M26', 'M27', 'M30', 'M31', 'M32', 'M33', 'M34', 'M35', 'M41', 'M43', 'M45', 'M46', 'M47', 'M48', 'M50', 'M51', 'M53', 'M54', 'M60', 'M61', 'M62', 'M65', 'M67', 'M70', 'M71', 'M72', 'M75', 'M79', 'M80', 'M81', 'M83', 'M84', 'M85', 'M86', 'M87', 'M88', 'M89', 'M92', 'M94', 'M95', 'M96', 'N00', 'N01', 'N02', 'N03', 'N04', 'N05', 'N11', 'N12', 'N13', 'N14', 'N15', 'N17', 'N18', 'N19', 'N20', 'N21', 'N23', 'N25', 'N26', 'N28', 'N29', 'N30', 'N31', 'N32', 'N34', 'N35', 'N36', 'N39', 'N40', 'N41', 'N42', 'N43', 'N44', 'N45', 'N46', 'N48', 'N49', 'N50', 'N52', 'N53', 'N60', 'N62', 'N63', 'N64', 'N76', 'N80', 'N81', 'N82', 'N83', 'N85', 'N89', 'N90', 'N91', 'N92', 'N93', 'N94', 'N96', 'N97', 'O02', 'O03', 'O14', 'O21', 'O24', 'O30', 'O32', 'O36', 'O40', 'O41', 'O42', 'O45', 'O47', 'O60', 'O72', 'O75', 'O80', 'O92', 'O99', 'P07', 'P52', 'P81', 'P83', 'P84', 'P91', 'Q02', 'Q03', 'Q04', 'Q05', 'Q07', 'Q10', 'Q12', 'Q13', 'Q14', 'Q18', 'Q21', 'Q23', 'Q24', 'Q25', 'Q27', 'Q28', 'Q30', 'Q31', 'Q32', 'Q35', 'Q37', 'Q38', 'Q42', 'Q43', 'Q44', 'Q45', 'Q51', 'Q52', 'Q53', 'Q55', 'Q60', 'Q61', 'Q62', 'Q63', 'Q64', 'Q67', 'Q68', 'Q69', 'Q70', 'Q71', 'Q74', 'Q75', 'Q76', 'Q78', 'Q79', 'Q82', 'Q83', 'Q85', 'Q87', 'Q89', 'Q90', 'Q98', 'R00', 'R01', 'R04', 'R05', 'R06', 'R07', 'R09', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R22', 'R23', 'R25', 'R26', 'R27', 'R29', 'R30', 'R31', 'R32', 'R33', 'R34', 'R35', 'R36', 'R39', 'R40', 'R41', 'R42', 'R43', 'R44', 'R45', 'R46', 'R47', 'R48', 'R49', 'R50', 'R51', 'R52', 'R53', 'R55', 'R56', 'R57', 'R58', 'R59', 'R60', 'R61', 'R62', 'R63', 'R64', 'R65', 'R68', 'R69', 'R71', 'R73', 'R74', 'R76', 'R78', 'R79', 'R80', 'R81', 'R82', 'R86', 'R89', 'R90', 'R91', 'R97', 'R99', 'S00', 'S01', 'S02', 'S03', 'S05', 'S06', 'S09', 'S10', 'S11', 'S19', 'S20', 'S21', 'S22', 'S23', 'S25', 'S27', 'S30', 'S31', 'S32', 'S34', 'S35', 'S36', 'S37', 'S39', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S50', 'S52', 'S53', 'S60', 'S61', 'S62', 'S63', 'S69', 'S70', 'S71', 'S72', 'S80', 'S81', 'S82', 'S83', 'S84', 'S85', 'S86', 'S89', 'S90', 'S91', 'S92', 'S93', 'S96', 'T07', 'T14', 'T17', 'T18', 'T19', 'T20', 'T21', 'T24', 'T30', 'T31', 'T36', 'T38', 'T48', 'T50', 'T56', 'T68', 'T74', 'T75', 'T78', 'T79', 'T80', 'T81', 'T82', 'T85', 'T86', 'V00', 'V19', 'V23', 'V29', 'V37', 'V87', 'V89', 'V99', 'W01', 'W16', 'W19', 'W21', 'W32', 'W34', 'W40', 'W55', 'W56', 'W57', 'W86', 'X08', 'X58', 'X73', 'Y09', 'Y84', 'Y95', 'Z16', 'Z17', 'Z20', 'Z21', 'Z37', 'Z3A', 'Z51', 'Z53', 'Z56', 'Z60', 'Z65', 'Z67', 'Z68', 'Z74', 'Z75', 'Z76', 'Z77', 'Z79', 'Z80', 'Z82', 'Z83', 'Z85', 'Z86', 'Z87', 'Z88', 'Z90', 'Z91', 'Z92', 'Z93', 'Z94', 'Z95', 'Z96', 'Z97', 'Z98', 'Z99']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A01.00', 'A01.4', 'A02.9', 'A03.8', 'A03.9', 'A04.5', 'A04.7', 'A04.8', 'A06.9', 'A08.0', 'A15.0', 'A15.4', 'A15.6', 'A15.9', 'A17.81', 'A18.09', 'A18.10', 'A18.11', 'A18.12', 'A18.15', 'A18.2', 'A18.4', 'A18.50', 'A19.9', 'A23.9', 'A28.1', 'A31.9', 'A32.11', 'A32.9', 'A38.9', 'A39.0', 'A39.1', 'A39.4', 'A40.3', 'A41.02', 'A41.1', 'A41.51', 'A41.52', 'A41.81', 'A41.9', 'A42.1', 'A42.9', 'A43.9', 'A44.0', 'A44.9', 'A48.1', 'A49.1', 'A49.8', 'A49.9', 'A53.9', 'A54.9', 'A60.00', 'A60.9', 'A63.0', 'A69.20', 'A74.9', 'A79.9', 'A80.9', 'A87.2', 'B00.1', 'B00.9', 'B01.12', 'B01.9', 'B02.21', 'B02.29', 'B02.30', 'B02.39', 'B02.9', 'B05.9', 'B06.9', 'B07.9', 'B08.3', 'B10.81', 'B10.89', 'B15.9', 'B17.9', 'B18.1', 'B18.2', 'B19.10', 'B19.20', 'B19.9', 'B25.9', 'B26.9', 'B27.00', 'B27.90', 'B27.99', 'B30.0', 'B30.9', 'B33.3', 'B34.1', 'B34.8', 'B34.9', 'B35.2', 'B35.9', 'B37.0', 'B37.2', 'B37.3', 'B37.7', 'B37.81', 'B37.9', 'B44.9', 'B45.2', 'B45.9', 'B46.5', 'B46.9', 'B48.8', 'B55.0', 'B55.9', 'B57.2', 'B58.00', 'B58.2', 'B58.9', 'B65.0', 'B65.1', 'B65.9', 'B67.8', 'B67.90', 'B67.99', 'B69.0', 'B69.9', 'B74.9', 'B81.0', 'B83.0', 'B85.2', 'B95.0', 'B95.2', 'B95.3', 'B95.4', 'B95.5', 'B95.6', 'B95.61', 'B95.62', 'B95.7', 'B95.8', 'B96.0', 'B96.1', 'B96.20', 'B96.29', 'B96.3', 'B96.4', 'B96.5', 'B96.7', 'B96.81', 'B96.89', 'B97.0', 'B97.10', 'B97.12', 'B97.29', 'B97.4', 'B97.6', 'B97.7', 'B99.9', 'C02.0', 'C02.1', 'C02.9', 'C04.9', 'C05.1', 'C05.9', 'C06.0', 'C06.9', 'C08.0', 'C09.9', 'C10.9', 'C15.9', 'C16.0', 'C16.9', 'C17.1', 'C18.0', 'C18.6', 'C18.7', 'C18.9', 'C21.0', 'C22.0', 'C22.1', 'C22.9', 'C25.0', 'C25.9', 'C26.0', 'C30.0', 'C31.2', 'C32.1', 'C32.9', 'C34.2', 'C34.9', 'C34.90', 'C34.91', 'C38.4', 'C40.20', 'C41.0', 'C41.1', 'C41.9', 'C43.31', 'C43.62', 'C43.9', 'C44.192', 'C44.199', 'C44.311', 'C44.42', 'C44.49', 'C44.699', 'C47.9', 'C48.0', 'C48.2', 'C49.12', 'C49.21', 'C49.4', 'C49.5', 'C49.9', 'C49.A', 'C49.A2', 'C50.312', 'C50.412', 'C50.519', 'C50.9', 'C50.91', 'C50.912', 'C50.919', 'C53.9', 'C54.1', 'C56.2', 'C56.9', 'C60.1', 'C60.9', 'C62.9', 'C62.90', 'C62.91', 'C62.92', 'C63.1', 'C63.10', 'C63.7', 'C63.8', 'C64.1', 'C64.2', 'C64.9', 'C65.9', 'C66.1', 'C66.2', 'C66.9', 'C67.2', 'C67.7', 'C67.9', 'C68.0', 'C69.30', 'C69.32', 'C69.40', 'C70.0', 'C70.9', 'C71.0', 'C71.9', 'C72.30', 'C72.32', 'C74.02', 'C74.90', 'C74.92', 'C75.0', 'C76.3', 'C77.0', 'C77.1', 'C77.2', 'C77.3', 'C77.8', 'C77.9', 'C78.00', 'C78.01', 'C78.02', 'C78.1', 'C78.39', 'C78.4', 'C78.5', 'C78.6', 'C78.7', 'C78.89', 'C79.0', 'C79.00', 'C79.02', 'C79.2', 'C79.31', 'C79.49', 'C79.51', 'C79.52', 'C79.60', 'C79.70', 'C79.71', 'C79.72', 'C79.81', 'C79.82', 'C79.89', 'C79.9', 'C80.0', 'C80.1', 'C81.1', 'C81.10', 'C81.29', 'C81.90', 'C83.0', 'C83.00', 'C83.1', 'C83.30', 'C83.39', 'C83.70', 'C84.49', 'C84.99', 'C84.A', 'C85.1', 'C85.9', 'C85.90', 'C85.99', 'C88.0', 'C88.4', 'C90.0', 'C90.00', 'C90.1', 'C90.3', 'C90.30', 'C91.00', 'C91.01', 'C91.1', 'C91.10', 'C91.90', 'C92.10', 'C92.3', 'C94.6', 'C96.6', 'C96.A', 'D09.0', 'D10.30', 'D11.0', 'D12.1', 'D12.3', 'D12.6', 'D12.8', 'D13.0', 'D13.1', 'D13.5', 'D13.6', 'D13.9', 'D15.1', 'D16.10', 'D16.21', 'D16.22', 'D16.5', 'D16.9', 'D17.23', 'D17.5', 'D17.71', 'D17.9', 'D18.00', 'D18.01', 'D18.02', 'D18.03', 'D18.09', 'D18.1', 'D21.10', 'D21.20', 'D22.9', 'D23.10', 'D23.11', 'D23.30', 'D23.9', 'D24.1', 'D24.9', 'D25.9', 'D27.9', 'D29.1', 'D29.30', 'D29.31', 'D29.8', 'D30.00', 'D30.01', 'D30.02', 'D30.3', 'D30.4', 'D31.62', 'D31.9', 'D32.0', 'D32.9', 'D33.3', 'D35.00', 'D35.02', 'D35.1', 'D35.2', 'D36.10', 'D36.11', 'D37.030', 'D37.8', 'D38.3', 'D3A.8', 'D40.0', 'D40.1', 'D40.10', 'D40.11', 'D40.8', 'D41.00', 'D41.01', 'D41.02', 'D41.3', 'D41.4', 'D43.1', 'D43.2', 'D43.4', 'D44.10', 'D44.12', 'D44.2', 'D44.4', 'D44.6', 'D44.7', 'D46.9', 'D47.0', 'D47.1', 'D47.2', 'D47.3', 'D47.9', 'D48.0', 'D48.1', 'D49.0', 'D49.1', 'D49.2', 'D49.3', 'D49.4', 'D49.511', 'D49.512', 'D49.519', 'D49.59', 'D49.6', 'D49.7', 'D49.89', 'D50.0', 'D50.9', 'D53.1', 'D56.1', 'D56.3', 'D57.00', 'D57.1', 'D58.9', 'D61.818', 'D61.82', 'D63.1', 'D64.9', 'D68.1', 'D68.51', 'D68.52', 'D68.59', 'D68.61', 'D68.9', 'D69.0', 'D69.2', 'D69.3', 'D69.41', 'D69.6', 'D69.9', 'D70.9', 'D72.0', 'D72.1', 'D72.810', 'D72.819', 'D72.820', 'D72.821', 'D72.822', 'D72.829', 'D73.1', 'D73.3', 'D73.4', 'D73.5', 'D73.89', 'D74.8', 'D75.81', 'D75.89', 'D76.3', 'D80.0', 'D80.1', 'D84.9', 'D86.9', 'D89.1', 'D89.2', 'E03.8', 'E03.9', 'E04.1', 'E04.2', 'E04.9', 'E05.00', 'E05.80', 'E05.90', 'E05.91', 'E06.1', 'E06.3', 'E07.9', 'E10.10', 'E10.9', 'E11.22', 'E11.319', 'E11.359', 'E11.42', 'E11.622', 'E11.628', 'E11.64', 'E11.65', 'E11.9', 'E13.9', 'E16.2', 'E21.0', 'E21.3', 'E21.5', 'E22.0', 'E22.2', 'E23.0', 'E23.2', 'E24.9', 'E27.0', 'E27.40', 'E27.49', 'E27.8', 'E27.9', 'E28.39', 'E29.1', 'E31.20', 'E31.21', 'E44.0', 'E44.1', 'E50.9', 'E51.9', 'E53.1', 'E53.8', 'E55.9', 'E56.1', 'E56.9', 'E61.1', 'E63.9', 'E66.01', 'E66.3', 'E66.9', 'E70.39', 'E72.01', 'E72.09', 'E73.9', 'E74.00', 'E74.04', 'E75.29', 'E75.5', 'E77.8', 'E78.00', 'E78.1', 'E78.5', 'E79.0', 'E80.20', 'E80.21', 'E80.4', 'E80.7', 'E83.01', 'E83.119', 'E83.19', 'E83.39', 'E83.41', 'E83.42', 'E83.51', 'E83.52', 'E83.59', 'E83.81', 'E84.9', 'E85.2', 'E85.3', 'E85.4', 'E85.9', 'E86.0', 'E87.0', 'E87.1', 'E87.2', 'E87.3', 'E87.5', 'E87.6', 'E87.70', 'E88.09', 'E88.89', 'E88.9', 'E89.0', 'F02.80', 'F10.10', 'F10.20', 'F10.21', 'F10.23', 'F10.921', 'F10.96', 'F11.20', 'F11.93', 'F12.10', 'F12.20', 'F14.10', 'F14.20', 'F17.200', 'F17.210', 'F17.219', 'F17.290', 'F19.10', 'F19.20', 'F19.21', 'F20.0', 'F20.2', 'F20.5', 'F25.0', 'F30.9', 'F31.81', 'F31.9', 'F32.3', 'F32.9', 'F34.1', 'F40.00', 'F40.10', 'F40.9', 'F41.0', 'F41.8', 'F41.9', 'F43.20', 'F43.9', 'F45.21', 'F45.29', 'F50.2', 'F60.1', 'F60.3', 'F60.6', 'F60.7', 'F60.9', 'F63.9', 'F65.3', 'F80.81', 'F84.0', 'F84.5', 'F90.9', 'F91.9', 'F98.8', 'G00.1', 'G03.9', 'G04.02', 'G04.1', 'G04.90', 'G06.0', 'G06.2', 'G12.20', 'G12.21', 'G24.5', 'G24.9', 'G25.0', 'G25.2', 'G25.3', 'G30.9', 'G31.84', 'G31.9', 'G36.0', 'G37.3', 'G40.109', 'G40.119', 'G40.309', 'G40.401', 'G40.409', 'G40.901', 'G40.909', 'G43.829', 'G43.909', 'G44.209', 'G44.89', 'G45.9', 'G47.00', 'G47.30', 'G47.32', 'G47.33', 'G47.63', 'G47.9', 'G50.0', 'G51.0', 'G51.3', 'G54.6', 'G56.00', 'G56.20', 'G56.31', 'G56.92', 'G57.00', 'G57.01', 'G57.42', 'G58.9', 'G60.8', 'G61.0', 'G62.81', 'G62.9', 'G70.00', 'G71.11', 'G71.8', 'G72.2', 'G72.3', 'G72.49', 'G72.9', 'G81.14', 'G81.90', 'G81.91', 'G81.94', 'G82.20', 'G82.21', 'G82.50', 'G83.1', 'G83.14', 'G83.2', 'G83.21', 'G83.24', 'G83.9', 'G89.18', 'G89.29', 'G90.2', 'G90.50', 'G90.511', 'G90.9', 'G91.8', 'G91.9', 'G93.0', 'G93.2', 'G93.40', 'G93.41', 'G93.5', 'G93.6', 'G93.82', 'G93.9', 'G95.19', 'G95.20', 'G95.9', 'G96.0', 'G96.19', 'G97.2', 'H00.11', 'H00.19', 'H01.001', 'H01.006', 'H01.009', 'H01.8', 'H01.9', 'H02.30', 'H02.40', 'H02.401', 'H02.402', 'H02.409', 'H02.411', 'H02.539', 'H02.726', 'H02.841', 'H02.843', 'H02.844', 'H02.845', 'H02.846', 'H02.849', 'H02.89', 'H02.9', 'H04.20', 'H04.201', 'H04.202', 'H04.203', 'H04.209', 'H05.11', 'H05.111', 'H05.2', 'H05.20', 'H05.221', 'H05.231', 'H05.233', 'H10.30', 'H10.32', 'H10.9', 'H11.001', 'H11.002', 'H11.133', 'H11.143', 'H11.42', 'H11.421', 'H11.422', 'H11.423', 'H11.429', 'H11.431', 'H11.432', 'H11.439', 'H11.9', 'H15.00', 'H15.032', 'H15.033', 'H15.092', 'H15.10', 'H16.0', 'H16.00', 'H16.001', 'H16.012', 'H16.07', 'H16.071', 'H16.079', 'H16.409', 'H16.442', 'H16.449', 'H16.9', 'H17.0', 'H17.12', 'H17.9', 'H18.2', 'H18.20', 'H18.89', 'H18.891', 'H18.9', 'H20.05', 'H20.052', 'H20.059', 'H20.9', 'H21.26', 'H21.309', 'H21.42', 'H21.50', 'H21.501', 'H21.509', 'H21.519', 'H21.542', 'H21.9', 'H25.1', 'H25.13', 'H26.8', 'H26.9', 'H27.10', 'H30.14', 'H30.89', 'H30.9', 'H30.92', 'H30.93', 'H31.101', 'H31.103', 'H31.309', 'H31.32', 'H31.40', 'H33.10', 'H33.101', 'H33.102', 'H33.19', 'H33.20', 'H33.21', 'H33.22', 'H33.309', 'H34.9', 'H35.00', 'H35.042', 'H35.06', 'H35.063', 'H35.30', 'H35.33', 'H35.349', 'H35.383', 'H35.50', 'H35.52', 'H35.6', 'H35.60', 'H35.61', 'H35.62', 'H35.81', 'H35.89', 'H35.9', 'H40.05', 'H40.052', 'H40.059', 'H40.10', 'H40.11', 'H40.83', 'H40.9', 'H43.1', 'H43.10', 'H43.11', 'H43.12', 'H43.13', 'H43.81', 'H43.812', 'H44.00', 'H44.001', 'H44.002', 'H44.009', 'H44.139', 'H44.40', 'H44.511', 'H46.00', 'H46.10', 'H46.9', 'H47.03', 'H47.039', 'H47.091', 'H47.092', 'H47.093', 'H47.10', 'H47.20', 'H47.322', 'H49.01', 'H49.02', 'H49.11', 'H49.21', 'H49.22', 'H49.23', 'H49.9', 'H50.00', 'H50.011', 'H50.012', 'H50.10', 'H50.112', 'H50.21', 'H50.22', 'H50.52', 'H50.9', 'H51.21', 'H52.01', 'H52.02', 'H52.1', 'H52.10', 'H52.13', 'H52.7', 'H53.00', 'H53.002', 'H53.14', 'H53.142', 'H53.143', 'H53.149', 'H53.15', 'H53.2', 'H53.30', 'H53.40', 'H53.411', 'H53.413', 'H53.419', 'H53.461', 'H53.462', 'H53.47', 'H53.60', 'H53.7', 'H53.8', 'H53.9', 'H54.0', 'H54.2', 'H54.3', 'H54.61', 'H54.62', 'H54.7', 'H55.00', 'H55.01', 'H57.02', 'H57.03', 'H57.04', 'H57.1', 'H57.10', 'H57.11', 'H57.12', 'H57.13', 'H57.8', 'H57.9', 'H59.03', 'H59.031', 'H59.033', 'H60.91', 'H66.90', 'H80.90', 'H90.12', 'H90.5', 'H91.20', 'H91.3', 'H91.9', 'H91.90', 'H91.91', 'H91.93', 'H92.01', 'H92.09', 'H92.20', 'H92.21', 'H93.11', 'H93.19', 'I05.0', 'I05.9', 'I07.1', 'I08.1', 'I08.3', 'I11.9', 'I12.0', 'I12.9', 'I15.9', 'I16.9', 'I20.0', 'I20.8', 'I20.9', 'I21.09', 'I21.11', 'I21.19', 'I21.29', 'I21.3', 'I23.0', 'I23.7', 'I24.0', 'I24.9', 'I25.10', 'I25.2', 'I25.9', 'I26.99', 'I27.2', 'I28.1', 'I30.9', 'I31.2', 'I31.3', 'I31.4', 'I33.0', 'I34.0', 'I34.1', 'I35.0', 'I35.1', 'I35.8', 'I35.9', 'I37.0', 'I42.0', 'I42.6', 'I42.8', 'I42.9', 'I44.2', 'I44.30', 'I44.60', 'I44.7', 'I45.10', 'I45.2', 'I45.6', 'I46.9', 'I47.1', 'I47.2', 'I48.0', 'I48.2', 'I48.91', 'I48.92', 'I49.01', 'I49.3', 'I49.9', 'I50.9', 'I51.4', 'I51.5', 'I51.7', 'I51.9', 'I60.9', 'I61.1', 'I61.3', 'I61.4', 'I61.5', 'I61.8', 'I61.9', 'I62.00', 'I62.01', 'I62.03', 'I62.9', 'I63.511', 'I63.512', 'I63.519', 'I63.9', 'I65.21', 'I65.22', 'I65.8', 'I66.9', 'I67.1', 'I67.2', 'I67.82', 'I67.89', 'I69.354', 'I70.0', 'I70.1', 'I70.203', 'I70.90', 'I71.00', 'I71.1', 'I71.2', 'I71.4', 'I71.9', 'I72.1', 'I72.2', 'I72.3', 'I72.8', 'I72.9', 'I73.1', 'I73.8', 'I74.3', 'I74.8', 'I74.9', 'I75.81', 'I75.89', 'I77.0', 'I77.1', 'I77.6', 'I77.70', 'I77.71', 'I77.819', 'I77.9', 'I78.1', 'I80.8', 'I82.0', 'I82.2', 'I82.210', 'I82.220', 'I82.3', 'I82.40', 'I82.401', 'I82.402', 'I82.409', 'I82.411', 'I82.431', 'I82.439', 'I82.619', 'I82.90', 'I82.A19', 'I83.90', 'I83.93', 'I85.00', 'I85.01', 'I86.1', 'I86.4', 'I87.1', 'I87.2', 'I87.8', 'I88.0', 'I88.1', 'I88.9', 'I89.0', 'I89.1', 'I89.8', 'I95.9', 'I99.8', 'I99.9', 'J01.30', 'J01.90', 'J02.9', 'J03.90', 'J03.91', 'J06.9', 'J10.1', 'J11.0', 'J11.1', 'J12.0', 'J15.0', 'J15.211', 'J15.6', 'J16.0', 'J18.1', 'J18.9', 'J21.8', 'J21.9', 'J30.1', 'J30.2', 'J30.81', 'J32.0', 'J32.3', 'J32.4', 'J32.9', 'J33.8', 'J33.9', 'J34.1', 'J34.3', 'J34.89', 'J34.9', 'J35.1', 'J38.1', 'J38.4', 'J39.2', 'J39.8', 'J43.9', 'J44.9', 'J45.909', 'J47.9', 'J62.8', 'J69.0', 'J69.8', 'J81.0', 'J81.1', 'J84.01', 'J84.10', 'J84.112', 'J84.89', 'J84.9', 'J85.2', 'J86.0', 'J91.0', 'J93.9', 'J94.0', 'J94.2', 'J94.8', 'J95.821', 'J95.851', 'J96.00', 'J96.90', 'J96.92', 'J98.01', 'J98.11', 'J98.2', 'J98.4', 'J98.51', 'J98.6', 'J98.8', 'K00.0', 'K00.1', 'K00.6', 'K01.0', 'K02.9', 'K04.1', 'K04.7', 'K04.8', 'K05.10', 'K05.6', 'K06.1', 'K06.2', 'K06.8', 'K06.9', 'K08.109', 'K08.89', 'K08.9', 'K09.0', 'K09.8', 'K11.1', 'K11.2', 'K11.20', 'K11.4', 'K11.6', 'K11.7', 'K11.9', 'K12.0', 'K12.1', 'K12.2', 'K13.0', 'K13.21', 'K13.29', 'K13.70', 'K13.79', 'K14.5', 'K14.6', 'K14.8', 'K14.9', 'K20.9', 'K21.9', 'K22.0', 'K22.10', 'K22.2', 'K22.6', 'K22.70', 'K22.8', 'K22.9', 'K25.9', 'K26.4', 'K26.9', 'K27.9', 'K29.50', 'K29.70', 'K29.80', 'K31.1', 'K31.5', 'K31.7', 'K31.819', 'K31.84', 'K31.89', 'K31.9', 'K35.2', 'K35.80', 'K38.8', 'K40.20', 'K40.30', 'K40.90', 'K42.9', 'K43.9', 'K44.9', 'K46.9', 'K50.00', 'K50.10', 'K50.80', 'K50.90', 'K51.00', 'K51.90', 'K52.3', 'K52.81', 'K52.82', 'K52.9', 'K55.049', 'K55.059', 'K55.069', 'K55.1', 'K55.20', 'K55.21', 'K55.9', 'K56.0', 'K56.1', 'K56.2', 'K56.3', 'K56.41', 'K56.5', 'K56.60', 'K56.69', 'K56.7', 'K57.00', 'K57.10', 'K57.11', 'K57.20', 'K57.30', 'K57.32', 'K57.80', 'K57.90', 'K57.92', 'K59.00', 'K59.39', 'K59.8', 'K59.9', 'K60.2', 'K60.3', 'K61.0', 'K61.1', 'K62.0', 'K62.1', 'K62.4', 'K62.5', 'K62.6', 'K62.7', 'K62.89', 'K62.9', 'K63.1', 'K63.2', 'K63.3', 'K63.5', 'K63.89', 'K63.9', 'K64.8', 'K64.9', 'K65.0', 'K65.1', 'K65.3', 'K65.4', 'K65.8', 'K65.9', 'K66.0', 'K66.1', 'K66.8', 'K68.12', 'K68.19', 'K68.9', 'K70.10', 'K70.30', 'K70.9', 'K71.6', 'K72.00', 'K72.9', 'K72.90', 'K73.9', 'K74.0', 'K74.60', 'K75.0', 'K75.4', 'K75.9', 'K76.0', 'K76.1', 'K76.6', 'K76.7', 'K76.81', 'K76.89', 'K76.9', 'K80.10', 'K80.20', 'K80.5', 'K80.50', 'K80.51', 'K81.0', 'K81.9', 'K82.8', 'K83.0', 'K83.1', 'K83.3', 'K83.8', 'K85.10', 'K85.20', 'K85.90', 'K86.0', 'K86.1', 'K86.2', 'K86.3', 'K86.89', 'K86.9', 'K90.0', 'K90.9', 'K91.2', 'K91.3', 'K91.7', 'K92.0', 'K92.1', 'K92.2', 'L02.01', 'L02.211', 'L02.31', 'L02.32', 'L02.41', 'L02.811', 'L02.91', 'L03.11', 'L03.115', 'L03.213', 'L03.90', 'L08.9', 'L10.0', 'L11.9', 'L20.9', 'L21.1', 'L21.9', 'L25.9', 'L27.0', 'L28.2', 'L29.0', 'L29.9', 'L30.9', 'L40.0', 'L40.53', 'L40.9', 'L43.9', 'L50.9', 'L51.1', 'L51.2', 'L51.9', 'L53.9', 'L57.0', 'L58.9', 'L60.8', 'L60.9', 'L65.9', 'L68.0', 'L70.1', 'L71.9', 'L72.0', 'L74.0', 'L74.52', 'L76.3', 'L81.9', 'L85.8', 'L85.9', 'L89.150', 'L89.159', 'L90.5', 'L90.8', 'L91.0', 'L92.9', 'L93.0', 'L94.0', 'L94.3', 'L95.9', 'L97.909', 'L97.919', 'L98.0', 'L98.49', 'L98.8', 'L98.9', 'M00.071', 'M00.9', 'M04.1', 'M05.9', 'M06.00', 'M06.9', 'M08.90', 'M08.961', 'M08.99', 'M13.0', 'M15.9', 'M16.0', 'M16.11', 'M17.9', 'M19.90', 'M21.059', 'M21.932', 'M21.959', 'M24.60', 'M24.641', 'M24.642', 'M25.40', 'M25.439', 'M25.451', 'M25.461', 'M25.47', 'M25.50', 'M25.511', 'M25.512', 'M25.519', 'M25.531', 'M25.532', 'M25.539', 'M25.541', 'M25.551', 'M25.552', 'M25.561', 'M25.562', 'M25.569', 'M25.571', 'M25.572', 'M25.60', 'M25.70', 'M25.9', 'M26.02', 'M26.04', 'M26.09', 'M26.19', 'M26.30', 'M26.32', 'M26.4', 'M26.52', 'M26.601', 'M26.609', 'M26.611', 'M26.619', 'M26.629', 'M26.69', 'M26.9', 'M27.2', 'M27.49', 'M27.8', 'M30.3', 'M31.0', 'M31.1', 'M31.6', 'M32.9', 'M33.90', 'M34.9', 'M35.00', 'M35.2', 'M35.3', 'M35.9', 'M41.9', 'M43.6', 'M45.9', 'M46.26', 'M46.34', 'M46.40', 'M46.44', 'M46.47', 'M46.90', 'M46.96', 'M47.816', 'M47.895', 'M47.896', 'M47.9', 'M48.00', 'M48.04', 'M48.06', 'M48.30', 'M50.221', 'M51.24', 'M51.26', 'M51.27', 'M53.2X6', 'M53.3', 'M54.2', 'M54.30', 'M54.31', 'M54.40', 'M54.41', 'M54.42', 'M54.5', 'M54.6', 'M54.9', 'M60.009', 'M60.059', 'M60.9', 'M61.10', 'M62.00', 'M62.3', 'M62.50', 'M62.81', 'M62.82', 'M62.838', 'M62.89', 'M65.9', 'M67.431', 'M70.51', 'M71.30', 'M72.0', 'M72.4', 'M72.6', 'M75.90', 'M79.0', 'M79.1', 'M79.2', 'M79.3', 'M79.5', 'M79.60', 'M79.603', 'M79.604', 'M79.605', 'M79.621', 'M79.641', 'M79.643', 'M79.644', 'M79.645', 'M79.646', 'M79.652', 'M79.659', 'M79.671', 'M79.672', 'M79.673', 'M79.7', 'M79.81', 'M79.89', 'M79.A11', 'M80.08X', 'M80.88X', 'M81.0', 'M83.9', 'M84.30X', 'M84.453', 'M84.48X', 'M84.529', 'M85.00', 'M85.2', 'M85.50', 'M85.60', 'M85.8', 'M85.80', 'M85.88', 'M86.161', 'M86.60', 'M86.68', 'M86.8X5', 'M86.9', 'M87.059', 'M87.80', 'M87.9', 'M88.832', 'M88.9', 'M89.061', 'M89.072', 'M89.549', 'M89.59', 'M89.70', 'M89.8X', 'M89.8X9', 'M89.9', 'M92.50', 'M94.359', 'M95.0', 'M95.2', 'M96.1', 'N00.9', 'N01.9', 'N02.1', 'N02.8', 'N03.9', 'N04.9', 'N05.1', 'N05.5', 'N05.7', 'N05.8', 'N05.9', 'N11.9', 'N13.30', 'N13.5', 'N13.6', 'N13.70', 'N13.8', 'N13.9', 'N14.0', 'N15.1', 'N17.0', 'N17.9', 'N18.2', 'N18.3', 'N18.4', 'N18.5', 'N18.6', 'N18.9', 'N20.0', 'N20.1', 'N20.9', 'N21.0', 'N25.81', 'N25.89', 'N26.1', 'N26.9', 'N28.0', 'N28.1', 'N28.82', 'N28.89', 'N28.9', 'N30.8', 'N30.80', 'N30.90', 'N31.1', 'N31.9', 'N32.0', 'N32.1', 'N32.3', 'N32.89', 'N32.9', 'N34.2', 'N35.9', 'N36.0', 'N36.1', 'N36.8', 'N39.0', 'N39.3', 'N39.41', 'N39.43', 'N39.44', 'N39.498', 'N40.0', 'N40.2', 'N41.0', 'N41.1', 'N41.2', 'N41.9', 'N42.30', 'N42.9', 'N43.3', 'N43.40', 'N44.00', 'N44.2', 'N44.8', 'N45.1', 'N45.2', 'N45.3', 'N45.4', 'N46.01', 'N46.11', 'N46.9', 'N48.1', 'N48.21', 'N48.29', 'N48.30', 'N48.5', 'N48.6', 'N48.81', 'N48.89', 'N48.9', 'N49.2', 'N50.0', 'N50.1', 'N50.3', 'N50.81', 'N50.811', 'N50.812', 'N50.819', 'N50.82', 'N50.89', 'N50.9', 'N52.9', 'N53.12', 'N60.01', 'N60.02', 'N60.1', 'N60.11', 'N60.12', 'N60.19', 'N60.92', 'N64.4', 'N64.89', 'N64.9', 'N76.0', 'N76.5', 'N80.1', 'N80.5', 'N80.8', 'N80.9', 'N81.2', 'N81.4', 'N82.1', 'N83.00', 'N83.9', 'N85.8', 'N85.9', 'N89.5', 'N90.89', 'N91.0', 'N92.1', 'N93.9', 'N94.10', 'N94.89', 'N94.9', 'N97.9', 'O02.0', 'O02.1', 'O03.9', 'O14.20', 'O21.0', 'O24.419', 'O30.00', 'O32.1', 'O36.4XX', 'O40.9XX0', 'O41.0', 'O41.00X', 'O41.129', 'O41.1290', 'O42.912', 'O42.92', 'O45.9', 'O47.9', 'O60.10X', 'O72.1', 'O75.9', 'O92.6', 'O99.11', 'P07.18', 'P07.39', 'P52.3', 'P81.9', 'P83.8', 'P91.60', 'Q03.0', 'Q03.1', 'Q03.9', 'Q04.0', 'Q04.3', 'Q04.8', 'Q04.9', 'Q05.9', 'Q07.00', 'Q07.8', 'Q07.9', 'Q10.0', 'Q12.4', 'Q13.0', 'Q13.4', 'Q14.1', 'Q18.0', 'Q18.8', 'Q21.1', 'Q23.1', 'Q24.8', 'Q24.9', 'Q25.6', 'Q27.30', 'Q27.9', 'Q28.2', 'Q30.9', 'Q31.5', 'Q32.4', 'Q35.3', 'Q37.9', 'Q38.2', 'Q42.9', 'Q43.0', 'Q43.8', 'Q43.9', 'Q44.6', 'Q44.7', 'Q45.0', 'Q51.0', 'Q51.3', 'Q51.811', 'Q52.0', 'Q53.10', 'Q53.20', 'Q53.9', 'Q55.21', 'Q55.22', 'Q55.29', 'Q55.4', 'Q55.69', 'Q60.2', 'Q60.5', 'Q60.6', 'Q61.02', 'Q61.19', 'Q61.3', 'Q61.4', 'Q62.10', 'Q62.11', 'Q62.4', 'Q62.8', 'Q63.1', 'Q63.2', 'Q63.9', 'Q64.0', 'Q64.10', 'Q64.12', 'Q64.4', 'Q64.74', 'Q67.0', 'Q67.3', 'Q68.0', 'Q69.9', 'Q70.9', 'Q71.91', 'Q74.0', 'Q75.0', 'Q75.2', 'Q75.3', 'Q76.49', 'Q78.2', 'Q78.8', 'Q79.1', 'Q79.9', 'Q82.8', 'Q83.9', 'Q85.00', 'Q85.01', 'Q85.1', 'Q85.8', 'Q85.9', 'Q87.2', 'Q87.3', 'Q87.40', 'Q87.81', 'Q87.89', 'Q89.2', 'Q89.3', 'Q90.9', 'Q98.3', 'Q98.4', 'R00.0', 'R00.1', 'R00.2', 'R01.0', 'R01.1', 'R04.0', 'R04.2', 'R04.89', 'R06.00', 'R06.01', 'R06.09', 'R06.1', 'R06.4', 'R06.6', 'R06.82', 'R06.89', 'R07.0', 'R07.1', 'R07.2', 'R07.81', 'R07.89', 'R07.9', 'R09.02', 'R09.2', 'R09.81', 'R10.0', 'R10.10', 'R10.11', 'R10.12', 'R10.13', 'R10.2', 'R10.30', 'R10.31', 'R10.32', 'R10.33', 'R10.811', 'R10.812', 'R10.813', 'R10.814', 'R10.815', 'R10.816', 'R10.817', 'R10.819', 'R10.83', 'R10.84', 'R10.9', 'R11.0', 'R11.10', 'R11.12', 'R11.14', 'R11.2', 'R13.10', 'R14.0', 'R15.9', 'R16.0', 'R16.1', 'R16.2', 'R18.0', 'R18.8', 'R19.0', 'R19.00', 'R19.01', 'R19.02', 'R19.03', 'R19.04', 'R19.06', 'R19.07', 'R19.09', 'R19.2', 'R19.32', 'R19.5', 'R19.7', 'R19.8', 'R20.0', 'R20.1', 'R20.2', 'R20.3', 'R20.8', 'R20.9', 'R22.0', 'R22.1', 'R22.2', 'R22.30', 'R22.9', 'R23.0', 'R23.1', 'R23.3', 'R23.4', 'R23.8', 'R25.1', 'R25.2', 'R25.3', 'R25.8', 'R26.0', 'R26.2', 'R26.81', 'R26.89', 'R26.9', 'R27.0', 'R27.8', 'R29.2', 'R29.6', 'R29.723', 'R29.810', 'R29.818', 'R29.898', 'R30.0', 'R30.1', 'R31.0', 'R31.29', 'R31.9', 'R33.9', 'R35.0', 'R35.1', 'R35.8', 'R36.1', 'R39.12', 'R39.13', 'R39.14', 'R39.15', 'R39.198', 'R39.89', 'R39.9', 'R40.0', 'R40.1', 'R40.20', 'R40.241', 'R40.2410', 'R40.2412', 'R40.242', 'R40.2420', 'R40.2421', 'R40.243', 'R40.2430', 'R40.3', 'R40.4', 'R41.0', 'R41.3', 'R41.840', 'R41.843', 'R43.2', 'R43.9', 'R44.0', 'R44.1', 'R44.3', 'R44.9', 'R45.0', 'R45.1', 'R45.3', 'R45.4', 'R46.0', 'R47.01', 'R47.02', 'R47.1', 'R47.89', 'R47.9', 'R48.2', 'R49.0', 'R49.1', 'R50.81', 'R50.82', 'R50.9', 'R53.1', 'R53.81', 'R53.83', 'R56.00', 'R56.9', 'R57.0', 'R57.1', 'R57.8', 'R57.9', 'R59.0', 'R59.1', 'R59.9', 'R60.0', 'R60.1', 'R60.9', 'R62.50', 'R62.52', 'R63.0', 'R63.1', 'R63.3', 'R63.4', 'R63.5', 'R65.10', 'R65.20', 'R65.21', 'R68.83', 'R68.84', 'R68.89', 'R71.0', 'R71.8', 'R73.01', 'R73.9', 'R74.0', 'R76.0', 'R76.11', 'R78.81', 'R79.82', 'R80.9', 'R82.0', 'R82.2', 'R82.3', 'R82.71', 'R86.9', 'R89.7', 'R90.0', 'R91.1', 'R91.8', 'R97.1', 'R97.20', 'R97.8', 'S00.32X', 'S00.33X', 'S00.522', 'S00.532', 'S01.00X', 'S01.101', 'S01.20X', 'S01.401', 'S01.501', 'S01.502', 'S01.512', 'S01.80X', 'S01.90X', 'S01.95X', 'S02.0XX', 'S02.109', 'S02.19X', 'S02.2XX', 'S02.401', 'S02.40F', 'S02.5XX', 'S02.601', 'S02.609', 'S02.652', 'S02.81X', 'S02.91X', 'S03.00X', 'S03.2XX', 'S05.8X2', 'S05.9', 'S05.90X', 'S05.92X', 'S06.0X0', 'S06.2X', 'S06.36', 'S06.370', 'S06.4X0', 'S06.4X9', 'S06.5X', 'S06.9', 'S06.9X', 'S06.9X0', 'S06.9X9', 'S09.90X', 'S09.92X', 'S09.93X', 'S10.0XX', 'S11.80X', 'S11.90X', 'S11.91X', 'S19.9X', 'S20.92X', 'S21.20', 'S22.20X', 'S22.39X', 'S22.4', 'S22.41X', 'S22.49XA', 'S22.5XX', 'S23.163', 'S25.02X', 'S27.329', 'S27.899', 'S30.0XX', 'S30.1XX', 'S30.23', 'S30.823', 'S31.104', 'S31.109', 'S31.40X', 'S31.809', 'S31.831', 'S32.019', 'S32.10', 'S32.313', 'S32.50', 'S32.501', 'S32.502', 'S32.591', 'S32.602', 'S32.9', 'S34.139', 'S35.229', 'S36.00X', 'S36.029', 'S36.09', 'S36.09X', 'S36.116', 'S37.009', 'S37.039', 'S37.20X', 'S37.29X', 'S37.812', 'S39.92', 'S39.94', 'S39.94X', 'S39.94XA', 'S41.10', 'S42.002', 'S42.009', 'S42.021', 'S42.30', 'S42.302', 'S42.322B', 'S42.402', 'S42.409', 'S42.90X', 'S43.006', 'S43.40', 'S45.10', 'S46.20', 'S47.2XX', 'S49.92X', 'S50.10X', 'S52.002', 'S52.102', 'S52.612', 'S52.92X', 'S53.101', 'S60.229', 'S60.511', 'S61.209', 'S61.402', 'S62.012', 'S62.12', 'S62.92X', 'S63.006', 'S63.071', 'S63.072', 'S69.91X', 'S69.92X', 'S70.312', 'S70.32', 'S71.101', 'S72.002', 'S72.302', 'S72.401', 'S72.401C', 'S80.821', 'S81.001', 'S82.101', 'S82.20', 'S82.40', 'S83.105', 'S83.106', 'S84.02X', 'S85.162', 'S86.102', 'S89.90X', 'S90.42', 'S91.051', 'S92.009', 'S92.312', 'S92.322', 'S93.401', 'S93.402', 'S96.002', 'T14.8', 'T14.90', 'T14.91', 'T17.408', 'T17.810', 'T18.2XX', 'T18.3XX', 'T18.9XX', 'T19.2XX', 'T20.00', 'T20.02X', 'T20.04X', 'T20.30X', 'T21.22X', 'T24.009', 'T24.011', 'T24.012', 'T24.211', 'T30.0', 'T31.2', 'T31.20', 'T36.0X5', 'T38.0X5', 'T48.3X1', 'T50.8X5', 'T56.0', 'T68.XXX', 'T74.2', 'T75.1XX', 'T78.2XX', 'T78.3XX', 'T79.7', 'T79.7XX', 'T79.A0X', 'T80.810', 'T81.30', 'T81.31X', 'T81.4', 'T81.49X', 'T81.509', 'T81.83X', 'T82.01X', 'T82.510', 'T82.7XX', 'T82.858', 'T82.868', 'T85.41X', 'T85.44X', 'T86.11', 'T86.822', 'T86.829', 'V00.121', 'V19.3XX', 'V19.9XX', 'V23.9XX', 'V29.99X', 'V37.0', 'V87.9', 'V87.9XX', 'V89.2XX', 'V89.9XX', 'V99.XXX', 'W01.0XX', 'W16.011', 'W19.XXX', 'W21.00X', 'W32.0XX', 'W34.00', 'W34.00X', 'W40.9XX', 'W55.03', 'W55.22X', 'W56.81X', 'W57.XXX', 'W86.8XX', 'X08.8XX', 'X58.XXX', 'X73.0XX', 'Y84.2', 'Z16.11', 'Z16.19', 'Z16.20', 'Z16.24', 'Z16.35', 'Z17.0', 'Z17.1', 'Z20.818', 'Z37.0', 'Z3A.08', 'Z3A.16', 'Z3A.18', 'Z3A.19', 'Z3A.20', 'Z3A.21', 'Z3A.22', 'Z3A.23', 'Z3A.24', 'Z3A.27', 'Z3A.34', 'Z3A.35', 'Z3A.38', 'Z3A.39', 'Z3A.40', 'Z51.5', 'Z53.20', 'Z53.29', 'Z53.3', 'Z53.31', 'Z53.8', 'Z56.2', 'Z60.2', 'Z65.1', 'Z67.10', 'Z67.11', 'Z68.1', 'Z68.21', 'Z68.22', 'Z68.25', 'Z68.26', 'Z68.27', 'Z68.34', 'Z68.36', 'Z68.4', 'Z68.41', 'Z68.42', 'Z68.43', 'Z68.45', 'Z74.01', 'Z75.1', 'Z76.82', 'Z77.29', 'Z79.01', 'Z79.02', 'Z79.1', 'Z79.3', 'Z79.4', 'Z79.52', 'Z79.810', 'Z79.82', 'Z79.83', 'Z79.84', 'Z79.890', 'Z80.0', 'Z80.1', 'Z80.3', 'Z82.41', 'Z82.49', 'Z83.3', 'Z85.3', 'Z86.11', 'Z86.12', 'Z86.61', 'Z86.718', 'Z86.73', 'Z87.01', 'Z87.11', 'Z87.440', 'Z87.442', 'Z87.81', 'Z87.891', 'Z88.0', 'Z88.1', 'Z88.2', 'Z88.8', 'Z90.3', 'Z90.49', 'Z90.5', 'Z90.710', 'Z90.721', 'Z90.722', 'Z90.79', 'Z90.81', 'Z91.041', 'Z91.048', 'Z91.19', 'Z92.0', 'Z92.21', 'Z92.3', 'Z93.0', 'Z93.2', 'Z94.0', 'Z94.1', 'Z94.2', 'Z95.0', 'Z95.1', 'Z95.2', 'Z96.0', 'Z96.1', 'Z97.4', 'Z97.5', 'Z98.2', 'Z98.52', 'Z98.84', 'Z98.85', 'Z99.2', 'Z99.3', 'Z99.81', 'Z99.89']\n",
      "Percentage of correct parent labels: 100.00% | 100.00%\n",
      "Epoch 1 | Loss: 1.4477 | LR: 1.50E-05\n",
      "F1 Validation | Micro: 0.01454 | Macro: 0.01407 | Best: 0.00000 | Epochs without improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 125/125 [00:09<00:00, 12.50it/s, loss=0.369, lr=3e-5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['A02', 'A06', 'A15', 'A17', 'A23', 'A28', 'A32', 'A42', 'A43', 'A48', 'A53', 'A54', 'A60', 'A74', 'A78', 'A80', 'A87', 'A90', 'B01', 'B06', 'B07', 'B08', 'B10', 'B15', 'B17', 'B19', 'B20', 'B26', 'B30', 'B33', 'B35', 'B44', 'B45', 'B48', 'B57', 'B58', 'B59', 'B95', 'B96', 'B97', 'B99', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C15', 'C16', 'C18', 'C25', 'C30', 'C31', 'C38', 'C44', 'C47', 'C48', 'C49', 'C52', 'C53', 'C54', 'C55', 'C61', 'C62', 'C64', 'C65', 'C67', 'C71', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C81', 'C84', 'C91', 'C92', 'D09', 'D10', 'D11', 'D16', 'D17', 'D18', 'D22', 'D23', 'D25', 'D27', 'D29', 'D30', 'D31', 'D38', 'D3A', 'D40', 'D44', 'D49', 'D50', 'D53', 'D56', 'D61', 'D62', 'D63', 'D64', 'D65', 'D66', 'D68', 'D69', 'D70', 'D72', 'D73', 'D75', 'D80', 'D86', 'D89', 'E04', 'E05', 'E11', 'E13', 'E22', 'E24', 'E27', 'E28', 'E29', 'E31', 'E46', 'E53', 'E55', 'E56', 'E63', 'E66', 'E72', 'E74', 'E75', 'E78', 'E79', 'E80', 'E83', 'E85', 'E86', 'E89', 'F10', 'F12', 'F14', 'F17', 'F19', 'F23', 'F25', 'F29', 'F32', 'F34', 'F40', 'F41', 'F43', 'F45', 'F60', 'F63', 'F65', 'F70', 'F71', 'F80', 'F90', 'F91', 'F98', 'F99', 'G06', 'G12', 'G24', 'G25', 'G30', 'G31', 'G35', 'G37', 'G40', 'G51', 'G60', 'G61', 'G72', 'G82', 'G89', 'G93', 'G97', 'H01', 'H02', 'H04', 'H05', 'H10', 'H16', 'H18', 'H21', 'H25', 'H30', 'H31', 'H44', 'H46', 'H55', 'H57', 'H59', 'H60', 'H66', 'H80', 'H90', 'H91', 'H92', 'H93', 'I07', 'I10', 'I12', 'I15', 'I16', 'I20', 'I21', 'I26', 'I28', 'I33', 'I34', 'I37', 'I38', 'I43', 'I44', 'I46', 'I47', 'I48', 'I49', 'I51', 'I61', 'I63', 'I65', 'I66', 'I71', 'I72', 'I76', 'I77', 'I78', 'I80', 'I83', 'I89', 'I95', 'I96', 'J01', 'J02', 'J03', 'J06', 'J10', 'J11', 'J12', 'J15', 'J16', 'J18', 'J30', 'J32', 'J43', 'J44', 'J45', 'J47', 'J60', 'J62', 'J64', 'J69', 'J80', 'J81', 'J82', 'J84', 'J86', 'J90', 'J91', 'J93', 'J94', 'J98', 'J99', 'K00', 'K01', 'K02', 'K05', 'K06', 'K08', 'K09', 'K11', 'K12', 'K13', 'K20', 'K21', 'K26', 'K31', 'K38', 'K40', 'K44', 'K46', 'K50', 'K56', 'K57', 'K60', 'K61', 'K62', 'K64', 'K65', 'K66', 'K68', 'K70', 'K71', 'K72', 'K74', 'K75', 'K76', 'K81', 'K82', 'K90', 'K92', 'L00', 'L02', 'L08', 'L21', 'L25', 'L27', 'L28', 'L29', 'L30', 'L42', 'L51', 'L57', 'L58', 'L60', 'L72', 'L80', 'L81', 'L85', 'L88', 'L89', 'L90', 'L92', 'L95', 'L98', 'M08', 'M13', 'M15', 'M16', 'M17', 'M21', 'M25', 'M31', 'M32', 'M33', 'M35', 'M45', 'M47', 'M48', 'M50', 'M53', 'M54', 'M60', 'M61', 'M67', 'M70', 'M72', 'M79', 'M80', 'M83', 'M86', 'M87', 'M89', 'M94', 'M95', 'M96', 'N01', 'N03', 'N05', 'N11', 'N12', 'N13', 'N15', 'N17', 'N18', 'N20', 'N26', 'N28', 'N29', 'N30', 'N35', 'N36', 'N40', 'N42', 'N44', 'N46', 'N49', 'N53', 'N60', 'N63', 'N64', 'N76', 'N85', 'N89', 'N90', 'N91', 'N92', 'N93', 'N94', 'N96', 'O14', 'O21', 'O41', 'O45', 'O72', 'O80', 'O92', 'P81', 'P83', 'P91', 'Q02', 'Q03', 'Q07', 'Q12', 'Q13', 'Q14', 'Q18', 'Q24', 'Q25', 'Q27', 'Q30', 'Q35', 'Q37', 'Q38', 'Q42', 'Q43', 'Q44', 'Q51', 'Q52', 'Q55', 'Q60', 'Q62', 'Q63', 'Q67', 'Q69', 'Q71', 'Q74', 'Q82', 'Q87', 'Q90', 'R00', 'R06', 'R07', 'R09', 'R10', 'R11', 'R12', 'R14', 'R15', 'R16', 'R17', 'R18', 'R19', 'R20', 'R21', 'R23', 'R26', 'R27', 'R30', 'R31', 'R33', 'R34', 'R35', 'R39', 'R40', 'R41', 'R42', 'R43', 'R44', 'R45', 'R47', 'R48', 'R50', 'R51', 'R52', 'R53', 'R57', 'R58', 'R59', 'R60', 'R61', 'R63', 'R64', 'R65', 'R68', 'R69', 'R71', 'R74', 'R76', 'R78', 'R79', 'R80', 'R86', 'R89', 'R97', 'R99', 'S01', 'S05', 'S10', 'S11', 'S21', 'S22', 'S23', 'S30', 'S31', 'S32', 'S36', 'S37', 'S41', 'S42', 'S43', 'S45', 'S46', 'S47', 'S49', 'S50', 'S52', 'S53', 'S60', 'S63', 'S69', 'S70', 'S80', 'S82', 'S90', 'S91', 'S92', 'T14', 'T17', 'T18', 'T19', 'T20', 'T21', 'T24', 'T30', 'T31', 'T38', 'T48', 'T68', 'T79', 'T80', 'T81', 'T82', 'T86', 'V23', 'V37', 'V89', 'V99', 'W19', 'W21', 'W32', 'W34', 'W55', 'W86', 'X08', 'X58', 'Y84', 'Z16', 'Z17', 'Z20', 'Z21', 'Z37', 'Z3A', 'Z51', 'Z53', 'Z65', 'Z68', 'Z74', 'Z75', 'Z76', 'Z77', 'Z79', 'Z80', 'Z83', 'Z85', 'Z86', 'Z90', 'Z91', 'Z92', 'Z93', 'Z94', 'Z99']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A01.00', 'A01.4', 'A02.9', 'A03.9', 'A04.5', 'A06.9', 'A08.0', 'A15.0', 'A15.6', 'A15.9', 'A17.81', 'A18.11', 'A18.12', 'A18.2', 'A18.50', 'A19.9', 'A23.9', 'A28.1', 'A32.11', 'A32.9', 'A39.0', 'A39.4', 'A41.1', 'A41.51', 'A41.9', 'A42.9', 'A43.9', 'A44.9', 'A48.1', 'A49.1', 'A49.8', 'A53.9', 'A54.9', 'A60.00', 'A60.9', 'A79.9', 'B00.1', 'B00.9', 'B01.12', 'B01.9', 'B02.21', 'B02.29', 'B02.30', 'B02.39', 'B06.9', 'B07.9', 'B08.3', 'B10.81', 'B15.9', 'B17.9', 'B18.1', 'B18.2', 'B19.10', 'B19.20', 'B25.9', 'B27.00', 'B27.99', 'B30.9', 'B33.3', 'B34.8', 'B34.9', 'B37.0', 'B37.2', 'B37.3', 'B37.7', 'B37.81', 'B37.9', 'B44.9', 'B45.2', 'B48.8', 'B55.9', 'B58.00', 'B58.2', 'B58.9', 'B65.0', 'B67.90', 'B67.99', 'B69.0', 'B74.9', 'B81.0', 'B83.0', 'B85.2', 'B95.0', 'B95.2', 'B95.5', 'B95.6', 'B95.7', 'B95.8', 'B96.0', 'B96.1', 'B96.20', 'B96.3', 'B96.4', 'B96.7', 'B96.81', 'B96.89', 'B97.0', 'B97.12', 'B97.29', 'B97.7', 'B99.9', 'C02.9', 'C05.1', 'C05.9', 'C06.0', 'C06.9', 'C09.9', 'C16.9', 'C18.7', 'C18.9', 'C22.1', 'C22.9', 'C25.9', 'C34.9', 'C34.90', 'C34.91', 'C40.20', 'C41.0', 'C41.1', 'C43.31', 'C43.62', 'C43.9', 'C44.199', 'C44.311', 'C44.42', 'C44.49', 'C47.9', 'C48.2', 'C49.12', 'C49.4', 'C49.5', 'C49.A', 'C49.A2', 'C50.312', 'C50.9', 'C50.91', 'C54.1', 'C56.2', 'C60.1', 'C62.9', 'C62.91', 'C62.92', 'C63.10', 'C63.7', 'C63.8', 'C64.2', 'C64.9', 'C65.9', 'C66.1', 'C67.2', 'C67.7', 'C67.9', 'C69.30', 'C69.40', 'C70.9', 'C71.0', 'C72.32', 'C74.02', 'C74.90', 'C77.2', 'C77.3', 'C77.8', 'C78.00', 'C78.01', 'C78.02', 'C78.1', 'C78.5', 'C78.7', 'C78.89', 'C79.00', 'C79.02', 'C79.2', 'C79.49', 'C79.51', 'C79.70', 'C79.71', 'C79.81', 'C79.89', 'C80.0', 'C81.1', 'C81.10', 'C81.90', 'C83.1', 'C83.30', 'C83.39', 'C83.70', 'C84.49', 'C84.99', 'C84.A', 'C85.1', 'C85.90', 'C88.0', 'C88.4', 'C91.01', 'C91.1', 'C91.10', 'C94.6', 'C96.A', 'D09.0', 'D10.30', 'D12.1', 'D13.1', 'D13.6', 'D15.1', 'D16.10', 'D16.21', 'D18.00', 'D18.02', 'D18.03', 'D18.09', 'D18.1', 'D22.9', 'D23.9', 'D24.1', 'D24.9', 'D27.9', 'D29.1', 'D29.30', 'D29.31', 'D30.01', 'D30.02', 'D30.3', 'D31.62', 'D31.9', 'D32.0', 'D32.9', 'D33.3', 'D35.00', 'D35.02', 'D35.1', 'D35.2', 'D37.030', 'D37.8', 'D38.3', 'D40.1', 'D40.10', 'D40.11', 'D40.8', 'D41.00', 'D41.3', 'D43.1', 'D43.4', 'D44.12', 'D44.4', 'D46.9', 'D47.1', 'D47.2', 'D48.0', 'D49.3', 'D49.4', 'D49.519', 'D49.59', 'D49.6', 'D49.89', 'D53.1', 'D56.1', 'D57.00', 'D58.9', 'D61.818', 'D61.82', 'D63.1', 'D64.9', 'D68.1', 'D68.51', 'D68.59', 'D68.61', 'D68.9', 'D69.2', 'D69.41', 'D69.6', 'D69.9', 'D70.9', 'D72.0', 'D72.1', 'D72.819', 'D72.820', 'D72.821', 'D72.829', 'D73.1', 'D73.4', 'D73.5', 'D73.89', 'D75.81', 'D75.89', 'D76.3', 'D80.0', 'D80.1', 'D86.9', 'D89.2', 'E03.9', 'E04.2', 'E04.9', 'E05.90', 'E05.91', 'E06.1', 'E06.3', 'E07.9', 'E10.10', 'E10.9', 'E11.319', 'E11.42', 'E11.628', 'E11.65', 'E11.9', 'E16.2', 'E21.0', 'E21.3', 'E22.2', 'E23.0', 'E23.2', 'E24.9', 'E27.0', 'E27.40', 'E28.39', 'E29.1', 'E31.20', 'E31.21', 'E44.0', 'E44.1', 'E51.9', 'E53.8', 'E55.9', 'E63.9', 'E66.3', 'E66.9', 'E70.39', 'E72.09', 'E74.00', 'E74.04', 'E75.29', 'E75.5', 'E78.5', 'E80.21', 'E83.01', 'E83.119', 'E83.19', 'E83.39', 'E83.51', 'E83.59', 'E84.9', 'E85.3', 'E85.4', 'E85.9', 'E87.0', 'E87.1', 'E87.2', 'E87.70', 'E89.0', 'F02.80', 'F10.20', 'F10.21', 'F10.23', 'F11.20', 'F11.93', 'F12.10', 'F14.10', 'F17.200', 'F17.210', 'F17.219', 'F17.290', 'F19.10', 'F19.20', 'F20.0', 'F20.2', 'F20.5', 'F31.9', 'F34.1', 'F40.00', 'F40.10', 'F41.9', 'F43.9', 'F45.21', 'F45.29', 'F60.7', 'F65.3', 'F84.5', 'F90.9', 'F91.9', 'G03.9', 'G04.1', 'G12.20', 'G12.21', 'G24.5', 'G24.9', 'G31.84', 'G31.9', 'G40.109', 'G40.309', 'G40.401', 'G40.901', 'G40.909', 'G43.829', 'G44.209', 'G44.89', 'G45.9', 'G47.00', 'G47.32', 'G47.33', 'G50.0', 'G51.0', 'G51.3', 'G54.6', 'G56.20', 'G56.31', 'G57.00', 'G57.42', 'G60.8', 'G61.0', 'G62.81', 'G62.9', 'G70.00', 'G72.2', 'G72.3', 'G72.49', 'G72.9', 'G81.90', 'G81.94', 'G82.20', 'G82.50', 'G83.14', 'G89.29', 'G90.50', 'G90.511', 'G93.2', 'G93.40', 'G93.41', 'G93.5', 'G93.82', 'G95.19', 'G95.20', 'G95.9', 'G96.0', 'G96.19', 'H00.19', 'H01.001', 'H01.009', 'H01.9', 'H02.30', 'H02.40', 'H02.411', 'H02.539', 'H02.726', 'H02.841', 'H02.844', 'H02.845', 'H02.846', 'H02.849', 'H02.89', 'H04.20', 'H04.201', 'H04.202', 'H04.203', 'H04.209', 'H05.111', 'H05.2', 'H05.20', 'H05.221', 'H05.233', 'H10.30', 'H10.32', 'H10.9', 'H11.001', 'H11.002', 'H11.133', 'H11.422', 'H11.423', 'H11.429', 'H11.431', 'H11.432', 'H11.439', 'H15.00', 'H15.032', 'H15.10', 'H16.0', 'H16.001', 'H16.012', 'H16.07', 'H16.442', 'H18.2', 'H18.20', 'H18.891', 'H18.9', 'H20.05', 'H20.9', 'H21.26', 'H21.309', 'H21.42', 'H21.519', 'H21.542', 'H26.9', 'H27.10', 'H30.14', 'H30.9', 'H30.92', 'H31.103', 'H31.309', 'H31.32', 'H33.101', 'H33.21', 'H35.042', 'H35.06', 'H35.30', 'H35.349', 'H35.383', 'H35.52', 'H35.6', 'H35.60', 'H35.61', 'H35.62', 'H35.81', 'H35.9', 'H40.052', 'H40.059', 'H40.10', 'H40.83', 'H40.9', 'H43.1', 'H43.10', 'H43.11', 'H43.13', 'H44.009', 'H46.00', 'H46.10', 'H47.03', 'H47.091', 'H47.092', 'H47.20', 'H47.322', 'H49.01', 'H49.11', 'H49.9', 'H50.012', 'H50.10', 'H50.21', 'H50.52', 'H51.21', 'H52.01', 'H52.02', 'H52.1', 'H53.14', 'H53.142', 'H53.149', 'H53.15', 'H53.2', 'H53.30', 'H53.40', 'H53.411', 'H53.419', 'H53.461', 'H53.462', 'H53.60', 'H53.8', 'H53.9', 'H54.0', 'H54.3', 'H54.62', 'H54.7', 'H55.00', 'H57.03', 'H57.04', 'H57.12', 'H57.13', 'H57.9', 'H59.033', 'H60.91', 'H66.90', 'H80.90', 'H90.5', 'H91.3', 'H91.9', 'H91.90', 'H91.91', 'H92.01', 'H92.09', 'H92.20', 'H92.21', 'H93.11', 'H93.19', 'I05.0', 'I07.1', 'I08.1', 'I08.3', 'I11.9', 'I12.9', 'I20.0', 'I21.09', 'I21.11', 'I21.29', 'I23.0', 'I23.7', 'I24.0', 'I25.10', 'I25.2', 'I25.9', 'I30.9', 'I31.2', 'I31.3', 'I33.0', 'I34.0', 'I34.1', 'I35.0', 'I35.8', 'I35.9', 'I42.6', 'I42.8', 'I42.9', 'I44.30', 'I44.60', 'I45.2', 'I45.6', 'I46.9', 'I47.2', 'I48.91', 'I49.01', 'I49.3', 'I49.9', 'I50.9', 'I51.5', 'I51.7', 'I51.9', 'I60.9', 'I61.3', 'I61.4', 'I61.9', 'I62.00', 'I62.01', 'I62.03', 'I62.9', 'I63.511', 'I63.512', 'I63.519', 'I63.9', 'I65.22', 'I65.8', 'I66.9', 'I67.1', 'I67.2', 'I67.82', 'I70.0', 'I70.90', 'I71.00', 'I72.1', 'I72.2', 'I72.3', 'I72.8', 'I72.9', 'I73.1', 'I73.8', 'I74.3', 'I74.9', 'I75.89', 'I77.0', 'I77.6', 'I77.70', 'I77.71', 'I77.819', 'I78.1', 'I80.8', 'I82.0', 'I82.210', 'I82.3', 'I82.401', 'I82.402', 'I82.411', 'I82.431', 'I82.439', 'I82.619', 'I82.90', 'I82.A19', 'I83.90', 'I85.00', 'I85.01', 'I86.4', 'I87.1', 'I87.8', 'I88.0', 'I88.9', 'I89.0', 'I95.9', 'I99.8', 'J01.30', 'J02.9', 'J06.9', 'J11.0', 'J15.0', 'J16.0', 'J18.9', 'J21.8', 'J21.9', 'J30.2', 'J30.81', 'J32.0', 'J32.3', 'J32.4', 'J33.8', 'J33.9', 'J34.1', 'J34.3', 'J34.9', 'J35.1', 'J38.1', 'J38.4', 'J39.2', 'J39.8', 'J45.909', 'J47.9', 'J81.0', 'J81.1', 'J84.01', 'J84.112', 'J91.0', 'J93.9', 'J94.0', 'J94.2', 'J94.8', 'J95.851', 'J96.90', 'J96.92', 'J98.01', 'J98.11', 'J98.4', 'J98.51', 'J98.8', 'K00.0', 'K04.7', 'K05.6', 'K06.2', 'K06.8', 'K06.9', 'K08.109', 'K08.89', 'K08.9', 'K11.1', 'K11.2', 'K11.20', 'K11.4', 'K11.7', 'K12.2', 'K13.0', 'K13.29', 'K13.70', 'K14.5', 'K14.6', 'K14.9', 'K20.9', 'K21.9', 'K22.0', 'K22.10', 'K22.2', 'K22.6', 'K22.70', 'K25.9', 'K26.9', 'K27.9', 'K31.1', 'K31.5', 'K31.819', 'K31.84', 'K31.9', 'K35.80', 'K38.8', 'K40.20', 'K40.30', 'K40.90', 'K43.9', 'K44.9', 'K46.9', 'K50.10', 'K50.80', 'K51.00', 'K51.90', 'K52.3', 'K52.81', 'K52.82', 'K52.9', 'K55.049', 'K55.20', 'K55.9', 'K56.0', 'K56.1', 'K56.5', 'K56.60', 'K56.69', 'K56.7', 'K57.11', 'K57.30', 'K57.32', 'K57.80', 'K57.90', 'K57.92', 'K59.39', 'K59.8', 'K59.9', 'K60.2', 'K60.3', 'K61.0', 'K62.1', 'K62.4', 'K62.7', 'K62.89', 'K62.9', 'K63.1', 'K63.3', 'K63.5', 'K63.89', 'K63.9', 'K64.8', 'K65.9', 'K66.0', 'K66.1', 'K66.8', 'K68.12', 'K68.9', 'K70.30', 'K72.90', 'K74.0', 'K75.0', 'K75.9', 'K76.0', 'K76.6', 'K76.7', 'K76.81', 'K76.89', 'K76.9', 'K80.20', 'K80.50', 'K80.51', 'K81.9', 'K82.8', 'K83.0', 'K83.3', 'K83.8', 'K86.0', 'K86.1', 'K86.2', 'K86.89', 'K90.0', 'K91.2', 'K91.3', 'K91.7', 'K92.2', 'L02.211', 'L02.31', 'L02.41', 'L03.11', 'L03.115', 'L03.90', 'L11.9', 'L20.9', 'L27.0', 'L28.2', 'L29.0', 'L40.53', 'L40.9', 'L43.9', 'L51.9', 'L53.9', 'L57.0', 'L60.8', 'L60.9', 'L68.0', 'L70.1', 'L74.52', 'L76.3', 'L81.9', 'L85.9', 'L89.150', 'L89.159', 'L90.8', 'L91.0', 'L92.9', 'L94.0', 'L94.3', 'L95.9', 'L98.0', 'L98.49', 'L98.8', 'L98.9', 'M00.071', 'M00.9', 'M04.1', 'M05.9', 'M06.00', 'M08.961', 'M08.99', 'M15.9', 'M16.0', 'M16.11', 'M17.9', 'M19.90', 'M21.059', 'M21.932', 'M21.959', 'M24.641', 'M24.642', 'M25.439', 'M25.451', 'M25.461', 'M25.47', 'M25.50', 'M25.512', 'M25.531', 'M25.532', 'M25.541', 'M25.562', 'M25.569', 'M25.572', 'M25.60', 'M26.02', 'M26.04', 'M26.09', 'M26.601', 'M26.609', 'M26.611', 'M26.619', 'M26.629', 'M27.2', 'M27.49', 'M31.6', 'M32.9', 'M33.90', 'M35.00', 'M35.3', 'M35.9', 'M41.9', 'M46.26', 'M46.34', 'M46.44', 'M46.47', 'M47.816', 'M47.895', 'M48.00', 'M48.04', 'M48.06', 'M48.30', 'M53.2X6', 'M54.2', 'M54.30', 'M54.31', 'M54.41', 'M54.6', 'M54.9', 'M60.009', 'M60.059', 'M60.9', 'M61.10', 'M62.00', 'M62.3', 'M62.50', 'M62.82', 'M62.838', 'M65.9', 'M70.51', 'M71.30', 'M72.0', 'M72.4', 'M75.90', 'M79.0', 'M79.1', 'M79.2', 'M79.5', 'M79.603', 'M79.604', 'M79.605', 'M79.621', 'M79.641', 'M79.643', 'M79.644', 'M79.646', 'M79.652', 'M79.659', 'M79.671', 'M79.673', 'M79.A11', 'M80.88X', 'M81.0', 'M84.30X', 'M84.453', 'M85.60', 'M85.8', 'M86.161', 'M86.60', 'M86.8X5', 'M86.9', 'M87.059', 'M87.80', 'M87.9', 'M88.832', 'M89.061', 'M89.59', 'M89.70', 'M89.8X', 'M89.8X9', 'M89.9', 'M95.0', 'N02.1', 'N02.8', 'N03.9', 'N04.9', 'N05.1', 'N05.5', 'N05.8', 'N05.9', 'N11.9', 'N13.6', 'N13.8', 'N13.9', 'N15.1', 'N17.0', 'N17.9', 'N18.2', 'N18.3', 'N18.5', 'N18.6', 'N20.1', 'N20.9', 'N25.81', 'N26.1', 'N26.9', 'N28.82', 'N28.89', 'N28.9', 'N30.90', 'N31.1', 'N31.9', 'N32.9', 'N34.2', 'N35.9', 'N36.0', 'N36.1', 'N39.0', 'N39.3', 'N39.41', 'N39.43', 'N40.0', 'N41.0', 'N41.1', 'N41.9', 'N42.30', 'N43.3', 'N44.00', 'N44.2', 'N44.8', 'N45.1', 'N45.3', 'N45.4', 'N48.1', 'N48.30', 'N48.5', 'N48.6', 'N48.81', 'N48.9', 'N50.0', 'N50.3', 'N50.81', 'N50.82', 'N50.89', 'N50.9', 'N60.12', 'N60.19', 'N64.89', 'N64.9', 'N76.0', 'N80.5', 'N80.9', 'N81.4', 'N82.1', 'N83.00', 'N85.9', 'N89.5', 'N90.89', 'N91.0', 'N92.1', 'N94.89', 'N94.9', 'O02.1', 'O03.9', 'O30.00', 'O32.1', 'O36.4XX', 'O40.9XX0', 'O41.00X', 'O41.129', 'O41.1290', 'O42.92', 'O47.9', 'O60.10X', 'O92.6', 'O99.11', 'P07.18', 'P83.8', 'Q03.0', 'Q03.9', 'Q04.3', 'Q04.8', 'Q05.9', 'Q07.00', 'Q07.8', 'Q07.9', 'Q10.0', 'Q14.1', 'Q18.0', 'Q23.1', 'Q27.30', 'Q27.9', 'Q28.2', 'Q31.5', 'Q32.4', 'Q35.3', 'Q38.2', 'Q51.0', 'Q51.3', 'Q52.0', 'Q53.10', 'Q53.20', 'Q53.9', 'Q55.21', 'Q55.22', 'Q55.29', 'Q55.4', 'Q60.5', 'Q60.6', 'Q61.02', 'Q61.19', 'Q61.3', 'Q61.4', 'Q62.4', 'Q62.8', 'Q64.0', 'Q64.12', 'Q67.3', 'Q71.91', 'Q75.2', 'Q78.8', 'Q79.9', 'Q82.8', 'Q83.9', 'Q85.00', 'Q85.01', 'Q85.1', 'Q85.9', 'Q87.3', 'Q89.2', 'Q89.3', 'Q90.9', 'Q98.3', 'R00.0', 'R01.0', 'R01.1', 'R04.0', 'R04.2', 'R04.89', 'R06.00', 'R06.01', 'R06.09', 'R06.1', 'R06.4', 'R06.82', 'R07.0', 'R07.1', 'R07.2', 'R07.89', 'R09.2', 'R10.0', 'R10.10', 'R10.11', 'R10.12', 'R10.13', 'R10.2', 'R10.31', 'R10.32', 'R10.33', 'R10.812', 'R10.813', 'R10.817', 'R10.819', 'R10.83', 'R10.84', 'R10.9', 'R11.0', 'R11.10', 'R11.12', 'R11.2', 'R14.0', 'R15.9', 'R16.0', 'R16.1', 'R16.2', 'R18.0', 'R19.0', 'R19.00', 'R19.01', 'R19.02', 'R19.03', 'R19.04', 'R19.06', 'R19.07', 'R19.09', 'R19.2', 'R19.32', 'R19.5', 'R19.8', 'R20.0', 'R20.1', 'R20.2', 'R20.8', 'R20.9', 'R22.30', 'R22.9', 'R23.0', 'R23.1', 'R23.4', 'R23.8', 'R25.3', 'R26.2', 'R26.81', 'R27.0', 'R27.8', 'R29.2', 'R29.6', 'R29.723', 'R29.818', 'R29.898', 'R30.1', 'R31.29', 'R31.9', 'R33.9', 'R35.1', 'R35.8', 'R39.12', 'R39.13', 'R39.15', 'R39.89', 'R40.1', 'R40.2410', 'R40.2412', 'R40.242', 'R40.2420', 'R40.2421', 'R40.3', 'R40.4', 'R41.0', 'R41.840', 'R43.2', 'R43.9', 'R44.9', 'R45.3', 'R46.0', 'R47.01', 'R47.02', 'R47.1', 'R47.89', 'R47.9', 'R49.0', 'R49.1', 'R50.82', 'R50.9', 'R53.1', 'R53.83', 'R56.00', 'R56.9', 'R57.0', 'R57.1', 'R59.0', 'R59.9', 'R60.9', 'R62.52', 'R63.0', 'R63.1', 'R63.4', 'R63.5', 'R65.10', 'R65.20', 'R65.21', 'R68.83', 'R68.89', 'R71.0', 'R73.01', 'R73.9', 'R74.0', 'R76.11', 'R78.81', 'R79.82', 'R80.9', 'R82.0', 'R82.2', 'R82.3', 'R82.71', 'R86.9', 'R90.0', 'R91.8', 'R97.1', 'R97.20', 'R97.8', 'S00.32X', 'S00.33X', 'S00.522', 'S00.532', 'S01.101', 'S01.20X', 'S01.502', 'S01.512', 'S01.80X', 'S01.90X', 'S02.0XX', 'S02.109', 'S02.19X', 'S02.2XX', 'S02.40F', 'S02.5XX', 'S02.601', 'S02.609', 'S02.652', 'S02.81X', 'S03.00X', 'S03.2XX', 'S05.8X2', 'S05.9', 'S05.90X', 'S05.92X', 'S06.0X0', 'S06.2X', 'S06.36', 'S06.370', 'S06.4X0', 'S06.4X9', 'S06.9', 'S06.9X', 'S06.9X9', 'S09.90X', 'S09.93X', 'S10.0XX', 'S11.80X', 'S11.90X', 'S19.9X', 'S21.20', 'S22.39X', 'S22.4', 'S22.41X', 'S22.49XA', 'S23.163', 'S25.02X', 'S27.899', 'S30.0XX', 'S30.1XX', 'S30.23', 'S30.823', 'S31.109', 'S31.809', 'S32.019', 'S32.10', 'S32.313', 'S32.50', 'S32.501', 'S32.502', 'S32.591', 'S32.602', 'S35.229', 'S36.029', 'S36.09', 'S36.09X', 'S36.116', 'S37.009', 'S37.29X', 'S37.812', 'S39.94', 'S39.94X', 'S39.94XA', 'S42.002', 'S42.009', 'S42.30', 'S42.302', 'S42.90X', 'S43.40', 'S46.20', 'S49.92X', 'S52.002', 'S52.102', 'S52.612', 'S52.92X', 'S60.229', 'S60.511', 'S61.209', 'S61.402', 'S63.071', 'S63.072', 'S69.92X', 'S70.312', 'S70.32', 'S71.101', 'S72.002', 'S72.302', 'S72.401C', 'S80.821', 'S81.001', 'S82.20', 'S83.105', 'S83.106', 'S90.42', 'S91.051', 'S92.009', 'S92.322', 'S93.401', 'S93.402', 'S96.002', 'T14.8', 'T14.90', 'T14.91', 'T17.408', 'T17.810', 'T18.2XX', 'T18.3XX', 'T18.9XX', 'T19.2XX', 'T20.00', 'T20.02X', 'T20.04X', 'T21.22X', 'T24.011', 'T24.211', 'T30.0', 'T31.2', 'T36.0X5', 'T38.0X5', 'T48.3X1', 'T56.0', 'T74.2', 'T75.1XX', 'T79.7', 'T81.30', 'T81.31X', 'T81.83X', 'T82.01X', 'T82.858', 'T82.868', 'T85.44X', 'T86.11', 'T86.822', 'V00.121', 'V23.9XX', 'V29.99X', 'V37.0', 'V87.9', 'V87.9XX', 'V89.9XX', 'W01.0XX', 'W16.011', 'W19.XXX', 'W21.00X', 'W32.0XX', 'W34.00', 'W34.00X', 'W40.9XX', 'W55.03', 'X58.XXX', 'Y84.2', 'Z16.19', 'Z16.35', 'Z20.818', 'Z3A.08', 'Z3A.16', 'Z3A.19', 'Z3A.20', 'Z3A.23', 'Z3A.24', 'Z3A.27', 'Z3A.34', 'Z3A.38', 'Z3A.40', 'Z53.20', 'Z53.29', 'Z67.10', 'Z67.11', 'Z68.21', 'Z68.25', 'Z68.26', 'Z68.27', 'Z68.34', 'Z68.36', 'Z68.42', 'Z68.43', 'Z75.1', 'Z76.82', 'Z77.29', 'Z79.01', 'Z79.02', 'Z79.1', 'Z79.3', 'Z79.810', 'Z79.82', 'Z79.83', 'Z80.0', 'Z80.1', 'Z83.3', 'Z86.11', 'Z86.12', 'Z86.718', 'Z86.73', 'Z87.440', 'Z87.442', 'Z87.81', 'Z87.891', 'Z88.1', 'Z88.8', 'Z90.5', 'Z90.710', 'Z90.721', 'Z91.041', 'Z92.3', 'Z94.0', 'Z94.2', 'Z95.1', 'Z96.0', 'Z96.1', 'Z99.2', 'Z99.3', 'Z99.81', 'Z99.89']\n",
      "Percentage of correct parent labels: 70.00% | 54.55%\n",
      "Epoch 2 | Loss: 0.6351 | LR: 3.00E-05\n",
      "Saving best model... 0.00000 -> 0.01086\n",
      "F1 Validation | Micro: 0.01772 | Macro: 0.01086 | Best: 0.01086 | Epochs without improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  32%|███▏      | 40/125 [00:03<00:06, 12.22it/s, loss=0.309, lr=3e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 302\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# ====================\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m#  EJECUCIÓN\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# ====================\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# Cargar datos de test\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mDATA_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 141\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    139\u001b[0m             scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 141\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), lr\u001b[38;5;241m=\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Validación\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hierarchical - V1\n",
    "# ====================\n",
    "\n",
    "import os\n",
    "\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        for codes in df['labels'].apply(eval): # FIXME: Unsafe eval\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        for idx in range(len(self.texts)):\n",
    "            encoding = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO\n",
    "# ====================\n",
    "def train():\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(Config.SAVE_STATE_PATH))\n",
    "        print(\"Loaded previously saved best model\")\n",
    "    except:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                # Línea corregida\n",
    "                loss = hierarchical_loss(\n",
    "                    outputs[0],  # parent_logits\n",
    "                    outputs[1],  # child_logits\n",
    "                    parent_labels,\n",
    "                    child_labels\n",
    "                )\n",
    "\n",
    "                loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device,\n",
    "                              mlb_parent, mlb_child)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "        # Store metrics\n",
    "\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN) and epoch > 0:\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_2\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'model': \"v1\",\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child):\n",
    "    model.eval()\n",
    "    parent_preds_all = []\n",
    "    child_preds_all = []\n",
    "    parent_labels_all = []\n",
    "    child_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            parent_labels = batch['parent_labels'].numpy()\n",
    "            child_labels = batch['child_labels'].numpy()\n",
    "\n",
    "            parent_logits, child_logits, pooled = model(**inputs)\n",
    "\n",
    "            # Convert to predictions\n",
    "            parent_preds = (torch.sigmoid(parent_logits).cpu().numpy() > Config.THRESHOLDS['parent']).astype(int)\n",
    "            child_preds = (torch.sigmoid(child_logits).cpu().numpy() > Config.THRESHOLDS['child']).astype(int)\n",
    "\n",
    "            # Append to lists\n",
    "            parent_preds_all.extend(parent_preds)\n",
    "            child_preds_all.extend(child_preds)\n",
    "            parent_labels_all.extend(parent_labels)\n",
    "            child_labels_all.extend(child_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    parent_preds_all = np.array(parent_preds_all)\n",
    "    child_preds_all = np.array(child_preds_all)\n",
    "    parent_labels_all = np.array(parent_labels_all)\n",
    "    child_labels_all = np.array(child_labels_all)\n",
    "\n",
    "    # Print example comparison for parent level\n",
    "    if len(parent_labels_all) > 0:\n",
    "        parent_true = np.array(mlb_parent.classes_)[parent_labels_all[0].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(parent_true) & set(parent_pred))\n",
    "        total_labels = len(set(parent_true))\n",
    "        accuracy_parent = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[child_labels_all[0].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(child_true) & set(child_pred))\n",
    "        total_labels = len(set(child_true))\n",
    "        accuracy_child = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        print(\"Expected parent labels:\", sorted(parent_true))\n",
    "        print(\"Predicted parent labels:\", sorted(parent_pred))\n",
    "        print(\"Expected child labels:\", sorted(child_true))\n",
    "        print(\"Predicted child labels:\", sorted(child_pred))\n",
    "\n",
    "        print(f\"Percentage of correct parent labels: {accuracy_parent:.2%} | {accuracy_child:.2%}\")\n",
    "\n",
    "    # Calculate F1 scores for each level\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(parent_labels_all, parent_preds_all, average='micro' , zero_division=0),\n",
    "        'f1_macro_parent': f1_score(parent_labels_all, parent_preds_all, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(child_labels_all, child_preds_all, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(child_labels_all, child_preds_all, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Calculate weighted average F1 scores\n",
    "    metrics['f1_micro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    metrics['f1_macro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > Config.THRESHOLDS['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > Config.THRESHOLDS['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.exists(f\"{Config.SAVE_PATH}2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  V0 Entrenamiento con cie10 dataset\n",
    "# ====================\n",
    "\n",
    "# Dataset especializado para pre-entrenamiento\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        self.mlb_parent = mlb_parent\n",
    "        self.mlb_child = mlb_child\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            code = row['code'].strip()\n",
    "            desc = row['description'].strip()\n",
    "            levels = parse_code(code)\n",
    "\n",
    "            # Generar múltiples variantes textuales\n",
    "            # Base variant is the description itself\n",
    "            variants = []\n",
    "\n",
    "            # Extract text inside parentheses and brackets\n",
    "            parentheses_matches = re.findall(r'\\((.*?)\\)', desc)\n",
    "            bracket_matches = re.findall(r'\\[(.*?)\\]', desc)\n",
    "\n",
    "            # Get text outside parentheses and brackets\n",
    "            clean_text = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', desc).strip()\n",
    "            if clean_text != desc:\n",
    "                variants.append(clean_text)\n",
    "\n",
    "            # Add matches from parentheses and brackets\n",
    "            variants.extend(parentheses_matches)\n",
    "            variants.extend(bracket_matches)\n",
    "\n",
    "            for variant in variants:\n",
    "                if len(variant) > 5:\n",
    "                    self.examples.append({\n",
    "                        'text': variant,\n",
    "                        'levels': levels\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping short variant: {variant}\")\n",
    "\n",
    "        tokenized_examples = []\n",
    "        for example in self.examples:\n",
    "            encoding = self.tokenizer(\n",
    "                example['text'],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            levels = example['levels']\n",
    "\n",
    "            tokenized_examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.mlb_parent.transform([[levels[0]]] if levels else [[]])[0]),\n",
    "                'child_labels': torch.FloatTensor(self.mlb_child.transform([[levels[1]]] if len(levels)>1 else [[]])[0])\n",
    "            })\n",
    "        self.examples = tokenized_examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def pretrain():\n",
    "    # Cargar datos de códigos CIE10\n",
    "    df = pd.read_csv(Config.PRETRAIN_DATA_PATH)\n",
    "\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Inicializar componentes\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    dataset = PretrainDataset(df, tokenizer, mlb_parent, mlb_child)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.PRETRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    # Bucle de pre-entrenamiento (similar al entrenamiento normal)\n",
    "    for epoch in range(Config.PRETRAIN_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Pre-train Epoch {epoch+1}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = hierarchical_loss(\n",
    "                outputs[0], outputs[1],\n",
    "                parent_labels, child_labels\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        print(f\"Pre-train Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Guardar modelo pre-entrenado\n",
    "    torch.save(model.state_dict(), Config.SAVE_PATH)\n",
    "    tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "    print(f\"Modelo pre-entrenado guardado en {Config.SAVE_PATH}\")\n",
    "\n",
    "pretrain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
