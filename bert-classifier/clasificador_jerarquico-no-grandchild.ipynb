{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/david/.pyenv/versions/3.12.8/lib/python3.12/site-packages (1.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# ====================\n",
    "#  CONFIGURACIÓN\n",
    "# ====================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class Config:\n",
    "    USE_FP16 = False\n",
    "    STRIDE = 128  # Overlap between windows\n",
    "    MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "    #MODEL_NAME = 'PlanTL-GOB-ES/bsc-bio-ehr-es'\n",
    "    #MODEL_NAME = 'IIC/bsc-bio-ehr-es-caresA'\n",
    "    #MODEL_NAME = 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es'\n",
    "    #MODEL_NAME = 'IIC/bert-base-spanish-wwm-cased-ctebmsp'\n",
    "    #MODEL_NAME = 'IIC/xlm-roberta-large-ehealth_kd'\n",
    "    THRESHOLD_TUNING_INTERVAL = 3  # Cada cuántas épocas ajustar umbrales\n",
    "    USE_FEATURE_PYRAMID = True # Usar Feature Pyramid Network\n",
    "    FEATURE_LAYER_WEIGHTS = [0.1, 0.3, 0.6]  # Pesos para las últimas 3 capas\n",
    "    CLASS_WEIGHT_SMOOTHING = 0.1  # Suavizado para pesos de clases\n",
    "    EARLY_STOP_PATIENCE = 50  # Número de épocas sin mejora para parar\n",
    "    IMPROVEMENT_MARGIN = 0.0005\n",
    "    MAX_LENGTH = 512 # Máxima longitud de secuencia, definida en el BERT pre-entrenado\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    VAL_BATCH_SIZE = 16\n",
    "    TEST_BATCH_SIZE = 32\n",
    "    EPOCHS = 1000\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    WARMUP_EPOCHS = 2\n",
    "    HIERARCHICAL_WEIGHTS = {'parent': 1.5, 'child': 1.0}\n",
    "    LEARNING_RATE = 3e-5 # 2e-2\n",
    "    DATA_PATHS = {\n",
    "        'train': 'codiesp_csvs/codiesp_D_source_train.csv',\n",
    "        'test': 'codiesp_csvs/codiesp_D_source_test.csv',\n",
    "        'val': 'codiesp_csvs/codiesp_D_source_validation.csv'\n",
    "    }\n",
    "    SAVE_TOKENIZER_PATH = 'snapshots/cie10_tokenizer'\n",
    "    SAVE_PATH = 'snapshots/best_hierarchical_model'\n",
    "    SAVE_STATE_PATH = 'snapshots/best_hierarchical_model_state.bin'\n",
    "    SAVE_MLB_PARENT_PATH = 'snapshots/best_hierarchical_model_mlb_parent'\n",
    "    SAVE_MLB_CHILD_PATH = 'snapshots/best_hierarchical_model_mlb_child'\n",
    "    THRESHOLDS = {'parent': 0.041, 'child': 0.12}\n",
    "    PRETRAIN_EPOCHS = 10\n",
    "    PRETRAIN_BATCH_SIZE = 8\n",
    "    PRETRAIN_DATA_PATH = '../csv_import_scripts/cie10-es-diagnoses-expanded.csv'\n",
    "    FORCE_NEW_MODEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PREPROCESAMIENTO\n",
    "# ====================\n",
    "def parse_code(code):\n",
    "    \"\"\"Divide el código en niveles jerárquicos\"\"\"\n",
    "    parts = code.split('.')\n",
    "    hierarchy = []\n",
    "    if len(parts) >= 1:\n",
    "        parent = parts[0]  # Primera categoría (ej: S62)\n",
    "        hierarchy.append(parent)\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            child_part = parts[1]\n",
    "            child = f\"{parent}.{child_part}\"  # Segunda categoría (ej: S62.14S)\n",
    "            hierarchy.append(child)\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def calculate_mlb_classes():\n",
    "    df = pd.read_csv(Config.PRETRAIN_DATA_PATH)\n",
    "    # Try to load saved MLBs first\n",
    "    try:\n",
    "        mlb_parent = joblib.load(Config.SAVE_MLB_PARENT_PATH)\n",
    "        mlb_child = joblib.load(Config.SAVE_MLB_CHILD_PATH)\n",
    "        print(\"Loaded saved MLBs\")\n",
    "        return mlb_parent, mlb_child\n",
    "    except:\n",
    "        print(\"Creating new MLBs\")\n",
    "\n",
    "        # Construir jerarquía de códigos\n",
    "        all_parents = set()\n",
    "        all_children = set()\n",
    "\n",
    "        for code in df['code']:\n",
    "            levels = parse_code(code.strip())\n",
    "            if len(levels) >= 1: all_parents.add(levels[0])\n",
    "            if len(levels) >= 2: all_children.add(levels[1])\n",
    "\n",
    "        # Inicializar MLB\n",
    "        mlb_parent = MultiLabelBinarizer().fit([all_parents])\n",
    "        mlb_child = MultiLabelBinarizer().fit([all_children])\n",
    "\n",
    "        # Save MLBs\n",
    "        joblib.dump(mlb_parent, Config.SAVE_MLB_PARENT_PATH)\n",
    "        joblib.dump(mlb_child, Config.SAVE_MLB_CHILD_PATH)\n",
    "\n",
    "    print(f\"Padres: {len(mlb_parent.classes_)} - Hijos: {len(mlb_child.classes_)}\")\n",
    "    return mlb_parent, mlb_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PLT DE MÉTRICAS\n",
    "# ====================\n",
    "\n",
    "def plot_metrics():\n",
    "    # Load metrics\n",
    "    metrics_history = pd.read_csv('training_metrics.csv')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['loss'], label='Loss')\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['f1_micro'], label='F1 Micro')\n",
    "    plt.plot(metrics_history['epoch'], metrics_history['f1_macro'], label='F1 Macro')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training Metrics Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MODELO JERÁRQUICO\n",
    "# ====================\n",
    "\n",
    "class HierarchicalBERT(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            self.bert = AutoModel.from_pretrained(Config.SAVE_PATH)\n",
    "            print(\"Loaded saved BERT model\")\n",
    "        except:\n",
    "            self.bert = AutoModel.from_pretrained(Config.MODEL_NAME)\n",
    "            print(\"Using default BERT model\")\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Clasificación padre\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "\n",
    "        # Clasificación hijo con contexto de padres\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "# Función de pérdida\n",
    "# ====================\n",
    "def hierarchical_loss(parent_logits, child_logits,\n",
    "                      parent_labels, child_labels):\n",
    "\n",
    "    loss_parent = torch.nn.BCEWithLogitsLoss()(parent_logits, parent_labels)\n",
    "    loss_child = torch.nn.BCEWithLogitsLoss()(child_logits, child_labels)\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved MLBs\n",
      "Loaded saved tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:  22%|██▏       | 135/603 [00:17<01:01,  7.61it/s, loss=3.46, lr=3.36e-6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 511\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     best_thresholds \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mTHRESHOLDS\n\u001b[0;32m--> 511\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_thresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Cargar datos de test\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mDATA_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 298\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(best_thresholds)\u001b[0m\n\u001b[1;32m    295\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    296\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 298\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), lr\u001b[38;5;241m=\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# After epoch completes, calculate aggregated loss\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v3 (Sliding Window)\n",
    "# ====================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  Modelo v3\n",
    "# ====================\n",
    "class HierarchicalBERTv2(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(Config.MODEL_NAME, output_hidden_states=True)\n",
    "        self.bert = AutoModel.from_pretrained(Config.MODEL_NAME, config=config)\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if Config.USE_FEATURE_PYRAMID:\n",
    "            # Combine last 3 layers' [CLS] embeddings\n",
    "            hidden_states = outputs.hidden_states[-3:]  # Get last 3 layers\n",
    "            # Stack [CLS] embeddings (shape: [3, batch_size, hidden_size])\n",
    "            pooled = torch.stack([state[:, 0] for state in hidden_states])\n",
    "            # Weighted combination of layers (weights should sum to 1)\n",
    "            if sum(Config.FEATURE_LAYER_WEIGHTS) != 1:\n",
    "                raise ValueError(\"FEATURE_LAYER_WEIGHTS must sum to 1\")\n",
    "\n",
    "            # Apply weights to layers\n",
    "            pooled = torch.einsum('lbd,l->bd', pooled,\n",
    "                                torch.tensor(Config.FEATURE_LAYER_WEIGHTS).to(pooled.device))\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Jerarquía de clasificación\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "#  FUNCIÓN DE PÉRDIDA MEJORADA\n",
    "# ====================\n",
    "def hierarchical_lossv2(parent_logits, child_logits,\n",
    "                     parent_labels, child_labels,\n",
    "                     parent_weights, child_weights):\n",
    "\n",
    "    loss_parent = F.binary_cross_entropy_with_logits(\n",
    "        parent_logits,\n",
    "        parent_labels,\n",
    "        pos_weight=parent_weights\n",
    "    )\n",
    "\n",
    "    loss_child = F.binary_cross_entropy_with_logits(\n",
    "        child_logits,\n",
    "        child_labels,\n",
    "        pos_weight=child_weights\n",
    "    )\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)\n",
    "\n",
    "# ====================\n",
    "#  AJUSTE DINÁMICO DE UMBRALES\n",
    "# ====================\n",
    "def calculate_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = {}\n",
    "    for i in range(y_probs.shape[1]):\n",
    "        if np.sum(y_true[:, i]) > 0:  # Solo clases presentes\n",
    "            precision, recall, threshs = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_idx = np.nanargmax(f1_scores)\n",
    "            thresholds[i] = threshs[best_idx]\n",
    "    return thresholds\n",
    "\n",
    "# ====================\n",
    "#  DATASET v3 (Sliding Window)\n",
    "# ====================\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        # Same label processing as before\n",
    "        for codes in df['labels'].apply(eval):\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                code = code.strip().upper()\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        # Generate sliding windows for each text\n",
    "        for idx, text in enumerate(self.texts):\n",
    "            # Tokenize whole text\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                truncation=False,\n",
    "                return_offsets_mapping=True,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            # Generate sliding windows\n",
    "            window_size = Config.MAX_LENGTH - 2  # Account for [CLS] and [SEP]\n",
    "            stride = Config.STRIDE\n",
    "\n",
    "            for i in range(0, len(tokens['input_ids']), stride):\n",
    "                # Extract window\n",
    "                window_start = i\n",
    "                window_end = min(i + window_size, len(tokens['input_ids']))\n",
    "\n",
    "                # Add special tokens\n",
    "                input_ids = (\n",
    "                    [self.tokenizer.cls_token_id] +\n",
    "                    tokens['input_ids'][window_start:window_end] +\n",
    "                    [self.tokenizer.sep_token_id]\n",
    "                )\n",
    "\n",
    "                attention_mask = [1] * len(input_ids)\n",
    "\n",
    "                # Pad if necessary\n",
    "                padding_length = Config.MAX_LENGTH - len(input_ids)\n",
    "                if padding_length > 0:\n",
    "                    input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "                    attention_mask += [0] * padding_length\n",
    "\n",
    "                self.examples.append({\n",
    "                    'input_ids': torch.tensor(input_ids),\n",
    "                    'attention_mask': torch.tensor(attention_mask),\n",
    "                    'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                    'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "                    'text_id': idx  # To group windows later\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO con Sliding Window\n",
    "# ====================\n",
    "def train(best_thresholds=Config.THRESHOLDS):\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERTv2(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    if not Config.FORCE_NEW_MODEL:\n",
    "        if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "            model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "            print(\"Loaded best model - 2\")\n",
    "        elif os.path.exists(Config.SAVE_PATH):\n",
    "            model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "            print(\"Loaded best model\")\n",
    "        else:\n",
    "            print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "\n",
    "    # Calcular pesos de clases\n",
    "    parent_counts = np.sum(train_dataset.parent_labels, axis=0)\n",
    "    parent_weights = (len(train_dataset) - parent_counts) / (parent_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    parent_weights = torch.tensor(parent_weights).to(device)\n",
    "\n",
    "    child_counts = np.sum(train_dataset.child_labels, axis=0)\n",
    "    child_weights = (len(train_dataset) - child_counts) / (child_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    child_weights = torch.tensor(child_weights).to(device)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Ajuste periódico de umbrales\n",
    "        if (epoch + 1) % Config.THRESHOLD_TUNING_INTERVAL == 0:\n",
    "            val_probs, val_labels = get_validation_probabilities(model, val_loader, device)\n",
    "\n",
    "            # Calcular mejores umbrales por clase\n",
    "            parent_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['parent'], val_probs['parent']\n",
    "            )\n",
    "            child_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['child'], val_probs['child']\n",
    "            )\n",
    "\n",
    "            # Actualizar umbrales globales\n",
    "            best_thresholds['parent'] = np.mean(list(parent_thresholds.values()))\n",
    "            best_thresholds['child'] = np.mean(list(child_thresholds.values()))\n",
    "            print(f\"Nuevos umbrales: Parent={best_thresholds['parent']:.3f}, Child={best_thresholds['child']:.3f}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        text_predictions = defaultdict(lambda: {'parent': [], 'child': []})\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                parent_logits, child_logits, _ = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask']\n",
    "                )\n",
    "\n",
    "                # Store predictions by original text\n",
    "                for i, text_id in enumerate(batch['text_id'].cpu().numpy()):\n",
    "                    text_predictions[text_id]['parent'].append(parent_logits[i])\n",
    "                    text_predictions[text_id]['child'].append(child_logits[i])\n",
    "\n",
    "                # Immediate window-level loss\n",
    "                loss = hierarchical_lossv2(\n",
    "                    parent_logits,\n",
    "                    child_logits,\n",
    "                    batch['parent_labels'],  # Use batch labels directly\n",
    "                    batch['child_labels'],   # Not the dataset's labels\n",
    "                    parent_weights,\n",
    "                    child_weights\n",
    "                )\n",
    "\n",
    "            # Backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # After epoch completes, calculate aggregated loss\n",
    "        agg_loss = 0\n",
    "        for text_id in text_predictions:\n",
    "            if text_id >= len(train_dataset.parent_labels):\n",
    "                continue  # Skip invalid text_ids\n",
    "\n",
    "            # Aggregate predictions\n",
    "            parent_agg = torch.stack(text_predictions[text_id]['parent']).max(dim=0)[0]\n",
    "            child_agg = torch.stack(text_predictions[text_id]['child']).max(dim=0)[0]\n",
    "\n",
    "            # Get true labels from dataset\n",
    "            parent_label = torch.FloatTensor(train_dataset.parent_labels[text_id]).to(device)\n",
    "            child_label = torch.FloatTensor(train_dataset.child_labels[text_id]).to(device)\n",
    "\n",
    "            # Calculate aggregated loss\n",
    "            agg_loss += hierarchical_lossv2(\n",
    "                parent_agg.unsqueeze(0),\n",
    "                child_agg.unsqueeze(0),\n",
    "                parent_label.unsqueeze(0),\n",
    "                child_label.unsqueeze(0),\n",
    "                parent_weights,\n",
    "                child_weights\n",
    "            )\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss += agg_loss.item() / len(text_predictions)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device, mlb_parent, mlb_child, best_thresholds)\n",
    "\n",
    "        # Store metrics\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if epoch == 0:\n",
    "            best_f1 = loss_metric\n",
    "\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN):\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_3\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'parent_threshold': best_thresholds['parent'],\n",
    "            'child_threshold': best_thresholds['child']\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "# ====================\n",
    "#  FUNCIONES AUXILIARES (Con sliding windows)\n",
    "# ====================\n",
    "def get_validation_probabilities(model, dataloader, device):\n",
    "    model.eval()\n",
    "    text_predictions = defaultdict(lambda: {'parent': [], 'child': []})\n",
    "    parent_labels = {}\n",
    "    child_labels = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            text_ids = batch['text_id'].numpy()\n",
    "\n",
    "            # Store labels by text_id\n",
    "            for i, text_id in enumerate(text_ids):\n",
    "                if text_id not in parent_labels:\n",
    "                    parent_labels[text_id] = batch['parent_labels'][i].numpy()\n",
    "                    child_labels[text_id] = batch['child_labels'][i].numpy()\n",
    "\n",
    "            # Get predictions\n",
    "            p_logits, c_logits, _ = model(**inputs)\n",
    "\n",
    "            # Store predictions by text_id\n",
    "            for i, text_id in enumerate(text_ids):\n",
    "                text_predictions[text_id]['parent'].append(p_logits[i].cpu())\n",
    "                text_predictions[text_id]['child'].append(c_logits[i].cpu())\n",
    "\n",
    "    # Aggregate predictions per text\n",
    "    parent_probs, child_probs = [], []\n",
    "    final_parent_labels, final_child_labels = [], []\n",
    "\n",
    "    for text_id in text_predictions:\n",
    "        # Aggregate using max pooling (same as training)\n",
    "        parent_agg = torch.stack(text_predictions[text_id]['parent']).max(dim=0)[0]\n",
    "        child_agg = torch.stack(text_predictions[text_id]['child']).max(dim=0)[0]\n",
    "\n",
    "        parent_probs.append(torch.sigmoid(parent_agg).numpy())\n",
    "        child_probs.append(torch.sigmoid(child_agg).numpy())\n",
    "\n",
    "        # Get original labels\n",
    "        final_parent_labels.append(parent_labels[text_id])\n",
    "        final_child_labels.append(child_labels[text_id])\n",
    "\n",
    "    return {\n",
    "        'parent': np.array(parent_probs),\n",
    "        'child': np.array(child_probs)\n",
    "    }, {\n",
    "        'parent': np.array(final_parent_labels),\n",
    "        'child': np.array(final_child_labels)\n",
    "    }\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN Con sliding windows\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child, thresholds):\n",
    "    val_probs, val_labels = get_validation_probabilities(model, dataloader, device)\n",
    "\n",
    "    # Convert probabilities to predictions\n",
    "    parent_preds = (val_probs['parent'] > thresholds['parent']).astype(int)\n",
    "    child_preds = (val_probs['child'] > thresholds['child']).astype(int)\n",
    "\n",
    "    # Print example comparison\n",
    "    if len(val_labels['parent']) > 0:\n",
    "        idx = 0  # First example\n",
    "        parent_true = np.array(mlb_parent.classes_)[val_labels['parent'][idx].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds[idx].astype(bool)]\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[val_labels['child'][idx].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds[idx].astype(bool)]\n",
    "\n",
    "        print(\"\\nExample Validation Results:\")\n",
    "        print(f\"Expected parent: {sorted(parent_true)}\")\n",
    "        print(f\"Predicted parent: {sorted(parent_pred)}\")\n",
    "        print(f\"Expected child: {sorted(child_true)}\")\n",
    "        print(f\"Predicted child: {sorted(child_pred)}\")\n",
    "\n",
    "        common_parent = len(set(parent_true) & set(parent_pred))\n",
    "        common_child = len(set(child_true) & set(child_pred))\n",
    "        print(f\"Parent Accuracy: {common_parent/len(parent_true):.2%} | Child Accuracy: {common_child/len(child_true):.2%}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(val_labels['parent'], parent_preds, average='micro', zero_division=0),\n",
    "        'f1_macro_parent': f1_score(val_labels['parent'], parent_preds, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(val_labels['child'], child_preds, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(val_labels['child'], child_preds, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Weighted averages\n",
    "    total_weight = sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "    metrics['f1_micro'] = (Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "                          Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']) / total_weight\n",
    "\n",
    "    metrics['f1_macro'] = (Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "                          Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']) / total_weight\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device, thresholds):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > thresholds['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > thresholds['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    best_thresholds = Config.THRESHOLDS\n",
    "\n",
    "    train(best_thresholds)\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if os.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "        model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "        print(\"Loaded best model - 2\")\n",
    "    elif os.exists(Config.SAVE_PATH):\n",
    "        model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "        print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device, best_thresholds)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(Config\u001b[38;5;241m.\u001b[39mSAVE_TOKENIZER_PATH)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo pre-entrenado guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig\u001b[38;5;241m.\u001b[39mSAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m \u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mpretrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpretrain\u001b[39m():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Cargar datos de códigos CIE10\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mConfig\u001b[49m\u001b[38;5;241m.\u001b[39mPRETRAIN_DATA_PATH)\n\u001b[1;32m     72\u001b[0m     mlb_parent, mlb_child \u001b[38;5;241m=\u001b[39m calculate_mlb_classes()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Inicializar componentes\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "#  V0 Preentrenamiento con cie10 dataset\n",
    "# ====================\n",
    "\n",
    "# Dataset especializado para pre-entrenamiento\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        self.mlb_parent = mlb_parent\n",
    "        self.mlb_child = mlb_child\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            code = row['code'].strip()\n",
    "            desc = row['description'].strip()\n",
    "            levels = parse_code(code)\n",
    "\n",
    "            # Generar múltiples variantes textuales\n",
    "            # Base variant is the description itself\n",
    "            variants = []\n",
    "\n",
    "            # Extract text inside parentheses and brackets\n",
    "            parentheses_matches = re.findall(r'\\((.*?)\\)', desc)\n",
    "            bracket_matches = re.findall(r'\\[(.*?)\\]', desc)\n",
    "\n",
    "            # Get text outside parentheses and brackets\n",
    "            clean_text = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', desc).strip()\n",
    "            if clean_text != desc:\n",
    "                variants.append(clean_text)\n",
    "\n",
    "            # Add matches from parentheses and brackets\n",
    "            variants.extend(parentheses_matches)\n",
    "            variants.extend(bracket_matches)\n",
    "\n",
    "            for variant in variants:\n",
    "                if len(variant) > 5:\n",
    "                    self.examples.append({\n",
    "                        'text': variant,\n",
    "                        'levels': levels\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping short variant: {variant}\")\n",
    "\n",
    "        tokenized_examples = []\n",
    "        for example in self.examples:\n",
    "            encoding = self.tokenizer(\n",
    "                example['text'],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            levels = example['levels']\n",
    "\n",
    "            tokenized_examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.mlb_parent.transform([[levels[0]]] if levels else [[]])[0]),\n",
    "                'child_labels': torch.FloatTensor(self.mlb_child.transform([[levels[1]]] if len(levels)>1 else [[]])[0])\n",
    "            })\n",
    "        self.examples = tokenized_examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def pretrain():\n",
    "    # Cargar datos de códigos CIE10\n",
    "    df = pd.read_csv(Config.PRETRAIN_DATA_PATH)\n",
    "\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Inicializar componentes\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    dataset = PretrainDataset(df, tokenizer, mlb_parent, mlb_child)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.PRETRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    # Bucle de pre-entrenamiento (similar al entrenamiento normal)\n",
    "    for epoch in range(Config.PRETRAIN_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Pre-train Epoch {epoch+1}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = hierarchical_loss(\n",
    "                outputs[0], outputs[1],\n",
    "                parent_labels, child_labels\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        print(f\"Pre-train Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Guardar modelo pre-entrenado\n",
    "    torch.save(model.state_dict(), Config.SAVE_PATH)\n",
    "    tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "    print(f\"Modelo pre-entrenado guardado en {Config.SAVE_PATH}\")\n",
    "\n",
    "pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 326) (2590137000.py, line 326)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 326\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif os.exists(Config.SAVE_PATH\"):\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 326)\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical - V1\n",
    "# ====================\n",
    "\n",
    "import os\n",
    "\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        for codes in df['labels'].apply(eval): # FIXME: Unsafe eval\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                code = code.strip().upper()\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        for idx in range(len(self.texts)):\n",
    "            encoding = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO\n",
    "# ====================\n",
    "def train():\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(Config.SAVE_STATE_PATH))\n",
    "        print(\"Loaded previously saved best model\")\n",
    "    except:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                # Línea corregida\n",
    "                loss = hierarchical_loss(\n",
    "                    outputs[0],  # parent_logits\n",
    "                    outputs[1],  # child_logits\n",
    "                    parent_labels,\n",
    "                    child_labels\n",
    "                )\n",
    "\n",
    "                loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device,\n",
    "                              mlb_parent, mlb_child)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "        # Store metrics\n",
    "\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN) and epoch > 0:\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_2\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child):\n",
    "    model.eval()\n",
    "    parent_preds_all = []\n",
    "    child_preds_all = []\n",
    "    parent_labels_all = []\n",
    "    child_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            parent_labels = batch['parent_labels'].numpy()\n",
    "            child_labels = batch['child_labels'].numpy()\n",
    "\n",
    "            parent_logits, child_logits = model(**inputs)\n",
    "\n",
    "            # Convert to predictions\n",
    "            parent_preds = (torch.sigmoid(parent_logits).cpu().numpy() > Config.THRESHOLDS['parent']).astype(int)\n",
    "            child_preds = (torch.sigmoid(child_logits).cpu().numpy() > Config.THRESHOLDS['child']).astype(int)\n",
    "\n",
    "            # Append to lists\n",
    "            parent_preds_all.extend(parent_preds)\n",
    "            child_preds_all.extend(child_preds)\n",
    "            parent_labels_all.extend(parent_labels)\n",
    "            child_labels_all.extend(child_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    parent_preds_all = np.array(parent_preds_all)\n",
    "    child_preds_all = np.array(child_preds_all)\n",
    "    parent_labels_all = np.array(parent_labels_all)\n",
    "    child_labels_all = np.array(child_labels_all)\n",
    "\n",
    "    # Print example comparison for parent level\n",
    "    if len(parent_labels_all) > 0:\n",
    "        parent_true = np.array(mlb_parent.classes_)[parent_labels_all[0].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(parent_true) & set(parent_pred))\n",
    "        total_labels = len(set(parent_true))\n",
    "        accuracy_parent = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[child_labels_all[0].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(child_true) & set(child_pred))\n",
    "        total_labels = len(set(child_true))\n",
    "        accuracy_child = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        print(\"Expected parent labels:\", sorted(parent_true))\n",
    "        print(\"Predicted parent labels:\", sorted(parent_pred))\n",
    "        print(\"Expected child labels:\", sorted(child_true))\n",
    "        print(\"Predicted child labels:\", sorted(child_pred))\n",
    "\n",
    "        print(f\"Percentage of correct parent labels: {accuracy_parent:.2%} | {accuracy_child:.2%}\")\n",
    "\n",
    "    # Calculate F1 scores for each level\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(parent_labels_all, parent_preds_all, average='micro' , zero_division=0),\n",
    "        'f1_macro_parent': f1_score(parent_labels_all, parent_preds_all, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(child_labels_all, child_preds_all, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(child_labels_all, child_preds_all, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Calculate weighted average F1 scores\n",
    "    metrics['f1_micro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    metrics['f1_macro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > Config.THRESHOLDS['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > Config.THRESHOLDS['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if os.exists(f\"{Config.SAVE_PATH}2\"):\n",
    "        model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "        print(\"Loaded best model - 2\")\n",
    "    elif os.exists(Config.SAVE_PATH\"):\n",
    "        model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "        print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved MLBs\n",
      "Loaded saved tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 63/63 [00:11<00:00,  5.30it/s, loss=0.00711, lr=1e-5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'D49', 'I10', 'N13', 'N28', 'N32', 'R10', 'R31', 'R58']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B99.9', 'C64.9', 'C67.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D64.9', 'D72.829', 'E11.9', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I48.91', 'I82.90', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'N13.30', 'N18.9', 'N20.0', 'N28.1', 'N28.89', 'N28.9', 'N32.9', 'N39.0', 'R10.9', 'R23.1', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R59.0', 'R59.9', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90']\n",
      "Percentage of correct parent labels: 30.00% | 45.45%\n",
      "Epoch 1 | Loss: 0.0056 | LR: 1.00E-05\n",
      "F1 Validation | Micro: 0.27902 | Macro: 0.02075 | Best: 0.02075 | Epochs without improvement: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 63/63 [00:11<00:00,  5.42it/s, loss=0.00477, lr=2e-5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'C79', 'D49', 'I10', 'N13', 'N28', 'N32', 'R31', 'R58', 'R59']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B99.9', 'C64.9', 'C67.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D64.9', 'E11.9', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I48.91', 'I82.90', 'K65.9', 'M19.90', 'M54.5', 'N13.30', 'N18.9', 'N20.0', 'N28.1', 'N28.89', 'N28.9', 'N32.9', 'N39.0', 'R10.9', 'R19.00', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R59.0', 'R59.9', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90']\n",
      "Percentage of correct parent labels: 30.00% | 45.45%\n",
      "Epoch 2 | Loss: 0.0052 | LR: 2.00E-05\n",
      "F1 Validation | Micro: 0.28390 | Macro: 0.02119 | Best: 0.02075 | Epochs without improvement: 3\n",
      "Nuevos umbrales: Parent=0.036, Child=0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 63/63 [00:11<00:00,  5.45it/s, loss=0.00471, lr=2e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'D49', 'I10', 'N13', 'N28', 'N32', 'N39', 'R31', 'R52', 'R58']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A15.9', 'A41.9', 'B19.20', 'B99.9', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'F32.9', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K65.9', 'L53.9', 'L98.9', 'M19.90', 'M54.5', 'M89.9', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'Q61.3', 'R10.9', 'R11.10', 'R14.0', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R57.9', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z87.891', 'Z90.710']\n",
      "Percentage of correct parent labels: 40.00% | 45.45%\n",
      "Epoch 3 | Loss: 0.0042 | LR: 2.00E-05\n",
      "Saving best model... 0.02075 -> 0.02176\n",
      "F1 Validation | Micro: 0.26579 | Macro: 0.02176 | Best: 0.02176 | Epochs without improvement: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 63/63 [00:11<00:00,  5.35it/s, loss=0.00283, lr=2e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'C79', 'D49', 'I10', 'N13', 'N20', 'N23', 'N28', 'N32', 'N39', 'R10', 'R31', 'R52', 'R58', 'R59']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A15.9', 'A41.9', 'B19.20', 'B96.20', 'B99.9', 'C16.9', 'C64.2', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.2', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'F32.9', 'I12.9', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K59.00', 'K65.9', 'L92.9', 'L98.9', 'M19.90', 'M54.5', 'M89.9', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'N50.89', 'N80.9', 'Q61.3', 'R10.13', 'R10.9', 'R11.10', 'R14.0', 'R18.8', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R35.0', 'R50.9', 'R53.1', 'R53.81', 'R57.9', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z87.891', 'Z90.710']\n",
      "Percentage of correct parent labels: 60.00% | 54.55%\n",
      "Epoch 4 | Loss: 0.0045 | LR: 2.00E-05\n",
      "Saving best model... 0.02176 -> 0.02378\n",
      "F1 Validation | Micro: 0.25922 | Macro: 0.02378 | Best: 0.02378 | Epochs without improvement: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 63/63 [00:11<00:00,  5.36it/s, loss=0.00487, lr=1.99e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'C79', 'D49', 'I10', 'M54', 'N13', 'N20', 'N23', 'N28', 'N32', 'N39', 'R10', 'R31', 'R52', 'R58', 'R59']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['A15.9', 'A41.9', 'B19.20', 'B96.20', 'B99.9', 'C64.2', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.2', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'F32.9', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K59.00', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'M89.9', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'N80.9', 'Q61.3', 'R10.13', 'R10.9', 'R11.10', 'R14.0', 'R16.0', 'R18.8', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R35.0', 'R50.9', 'R53.1', 'R53.81', 'R57.9', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z87.891', 'Z90.710']\n",
      "Percentage of correct parent labels: 60.00% | 54.55%\n",
      "Epoch 5 | Loss: 0.0043 | LR: 1.99E-05\n",
      "F1 Validation | Micro: 0.24928 | Macro: 0.02421 | Best: 0.02378 | Epochs without improvement: 4\n",
      "Nuevos umbrales: Parent=0.045, Child=0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 63/63 [00:11<00:00,  5.46it/s, loss=0.00658, lr=1.99e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'D49', 'I10', 'N13', 'N28', 'N32', 'R31', 'R58']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B19.20', 'B99.9', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'Q61.3', 'R10.9', 'R11.10', 'R19.00', 'R19.7', 'R23.1', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5']\n",
      "Percentage of correct parent labels: 30.00% | 45.45%\n",
      "Epoch 6 | Loss: 0.0035 | LR: 1.99E-05\n",
      "F1 Validation | Micro: 0.27719 | Macro: 0.02001 | Best: 0.02378 | Epochs without improvement: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 63/63 [00:11<00:00,  5.37it/s, loss=0.00589, lr=1.99e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'C79', 'D49', 'I10', 'N13', 'N28', 'N32', 'R10', 'R31', 'R58', 'R59']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B19.20', 'B99.9', 'C64.2', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.2', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I12.9', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K59.00', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'M89.9', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'Q61.3', 'R10.13', 'R10.9', 'R11.10', 'R14.0', 'R18.8', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R35.0', 'R50.9', 'R53.1', 'R57.9', 'R59.0', 'R59.9', 'R60.0', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z87.891', 'Z90.710']\n",
      "Percentage of correct parent labels: 30.00% | 54.55%\n",
      "Epoch 7 | Loss: 0.0039 | LR: 1.99E-05\n",
      "F1 Validation | Micro: 0.26606 | Macro: 0.02273 | Best: 0.02378 | Epochs without improvement: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 63/63 [00:11<00:00,  5.41it/s, loss=0.0034, lr=1.99e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'D49', 'I10', 'M54', 'N13', 'N28', 'N32', 'R10', 'R31', 'R52', 'R59']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B99.9', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K59.00', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'Q61.3', 'R10.13', 'R10.9', 'R11.10', 'R14.0', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R57.9', 'R59.0', 'R59.9', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z90.710']\n",
      "Percentage of correct parent labels: 30.00% | 54.55%\n",
      "Epoch 8 | Loss: 0.0037 | LR: 1.99E-05\n",
      "F1 Validation | Micro: 0.27141 | Macro: 0.02268 | Best: 0.02378 | Epochs without improvement: 7\n",
      "Nuevos umbrales: Parent=0.038, Child=0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 63/63 [00:11<00:00,  5.62it/s, loss=0.00407, lr=1.99e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected parent labels: ['D18', 'K26', 'K59', 'N13', 'N20', 'N23', 'N28', 'N39', 'Q62', 'R31']\n",
      "Predicted parent labels: ['C64', 'D49', 'I10', 'N13', 'N28', 'N32', 'N39', 'R31', 'R58']\n",
      "Expected child labels: ['D18.09', 'K26.9', 'K59.00', 'N13.5', 'N20.0', 'N28.0', 'N28.89', 'N28.9', 'N39.0', 'Q62.11', 'R31.9']\n",
      "Predicted child labels: ['B99.9', 'C64.2', 'C64.9', 'C67.9', 'C77.9', 'C78.7', 'C79.51', 'D49.0', 'D49.4', 'D49.59', 'D50.9', 'D64.9', 'D72.829', 'E11.9', 'E78.00', 'E78.5', 'E80.7', 'F17.200', 'F17.210', 'F17.290', 'I48.91', 'I82.90', 'J44.9', 'K52.9', 'K65.9', 'L98.9', 'M19.90', 'M54.5', 'N13.30', 'N18.9', 'N20.0', 'N26.1', 'N28.1', 'N28.89', 'N28.9', 'N32.89', 'N32.9', 'N39.0', 'N40.0', 'Q61.3', 'R10.9', 'R11.10', 'R19.00', 'R19.7', 'R23.1', 'R30.0', 'R31.0', 'R31.29', 'R31.9', 'R50.9', 'R53.1', 'R57.9', 'R59.0', 'R59.9', 'R60.9', 'R63.0', 'R63.4', 'R80.9', 'T14.8', 'T14.90', 'Z51.5', 'Z90.710']\n",
      "Percentage of correct parent labels: 40.00% | 45.45%\n",
      "Epoch 9 | Loss: 0.0030 | LR: 1.99E-05\n",
      "F1 Validation | Micro: 0.27826 | Macro: 0.02228 | Best: 0.02378 | Epochs without improvement: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:  38%|███▊      | 24/63 [00:04<00:07,  5.43it/s, loss=0.00448, lr=1.99e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 458\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     best_thresholds \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mTHRESHOLDS\n\u001b[0;32m--> 458\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_thresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Cargar datos de test\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mDATA_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 240\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(best_thresholds)\u001b[0m\n\u001b[1;32m    237\u001b[0m child_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, enabled\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mUSE_FP16):\n\u001b[0;32m--> 240\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Línea corregida\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     loss \u001b[38;5;241m=\u001b[39m hierarchical_loss(\n\u001b[1;32m    244\u001b[0m         outputs[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# parent_logits\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         outputs[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# child_logits\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         parent_labels,\n\u001b[1;32m    247\u001b[0m         child_labels\n\u001b[1;32m    248\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m, in \u001b[0;36mHierarchicalBERTv2.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([state[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m hidden_states])\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Weighted combination of layers (weights should sum to 1)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbd,l->bd\u001b[39m\u001b[38;5;124m'\u001b[39m, pooled,\n\u001b[0;32m---> 41\u001b[0m                         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFEATURE_LAYER_WEIGHTS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# V2 modelo jerárquico\n",
    "# # ====================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  Modelo v2\n",
    "# ====================\n",
    "class HierarchicalBERTv2(torch.nn.Module):\n",
    "    def __init__(self, num_parents, num_children):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(Config.MODEL_NAME, output_hidden_states=True)\n",
    "        self.bert = AutoModel.from_pretrained(Config.MODEL_NAME, config=config)\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size  # This will be 768 for base models\n",
    "\n",
    "        self.parent_classifier = torch.nn.Linear(hidden_size, num_parents)\n",
    "        self.child_classifier = torch.nn.Linear(hidden_size + num_parents, num_children)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if Config.USE_FEATURE_PYRAMID:\n",
    "            # Combine last 3 layers' [CLS] embeddings\n",
    "            hidden_states = outputs.hidden_states[-3:]  # Get last 3 layers\n",
    "            # Stack [CLS] embeddings (shape: [3, batch_size, hidden_size])\n",
    "            pooled = torch.stack([state[:, 0] for state in hidden_states])\n",
    "            # Weighted combination of layers (weights should sum to 1)\n",
    "            pooled = torch.einsum('lbd,l->bd', pooled,\n",
    "                                torch.tensor(Config.FEATURE_LAYER_WEIGHTS).to(pooled.device))\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # Jerarquía de clasificación\n",
    "        parent_logits = self.parent_classifier(pooled)\n",
    "        parent_probs = torch.sigmoid(parent_logits)\n",
    "        child_input = torch.cat([pooled, parent_probs], dim=1)\n",
    "        child_logits = self.child_classifier(child_input)\n",
    "\n",
    "        return parent_logits, child_logits, pooled\n",
    "\n",
    "# ====================\n",
    "#  FUNCIÓN DE PÉRDIDA MEJORADA\n",
    "# ====================\n",
    "def hierarchical_lossv2(parent_logits, child_logits,\n",
    "                     parent_labels, child_labels,\n",
    "                     parent_weights, child_weights):\n",
    "\n",
    "    loss_parent = F.binary_cross_entropy_with_logits(\n",
    "        parent_logits,\n",
    "        parent_labels,\n",
    "        pos_weight=parent_weights\n",
    "    )\n",
    "\n",
    "    loss_child = F.binary_cross_entropy_with_logits(\n",
    "        child_logits,\n",
    "        child_labels,\n",
    "        pos_weight=child_weights\n",
    "    )\n",
    "\n",
    "    return (Config.HIERARCHICAL_WEIGHTS['parent'] * loss_parent +\n",
    "            Config.HIERARCHICAL_WEIGHTS['child'] * loss_child)\n",
    "\n",
    "# ====================\n",
    "#  AJUSTE DINÁMICO DE UMBRALES\n",
    "# ====================\n",
    "def calculate_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = {}\n",
    "    for i in range(y_probs.shape[1]):\n",
    "        if np.sum(y_true[:, i]) > 0:  # Solo clases presentes\n",
    "            precision, recall, threshs = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_idx = np.nanargmax(f1_scores)\n",
    "            thresholds[i] = threshs[best_idx]\n",
    "    return thresholds\n",
    "\n",
    "# ====================\n",
    "#  Dataset\n",
    "# ====================\n",
    "class HierarchicalMedicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb_parent, mlb_child):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        # Procesar etiquetas\n",
    "        self.parent_labels = []\n",
    "        self.child_labels = []\n",
    "\n",
    "        for codes in df['labels'].apply(eval): # FIXME: Unsafe eval\n",
    "            parents, children = set(), set()\n",
    "            for code in codes:\n",
    "                code = code.strip().upper()\n",
    "                levels = parse_code(code)\n",
    "                if len(levels) >= 1: parents.add(levels[0])\n",
    "                if len(levels) >= 2: children.add(levels[1])\n",
    "\n",
    "            self.parent_labels.append(mlb_parent.transform([parents])[0])\n",
    "            self.child_labels.append(mlb_child.transform([children])[0])\n",
    "\n",
    "        for idx in range(len(self.texts)):\n",
    "            encoding = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=Config.MAX_LENGTH,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'parent_labels': torch.FloatTensor(self.parent_labels[idx]),\n",
    "                'child_labels': torch.FloatTensor(self.child_labels[idx]),\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# ====================\n",
    "#  ENTRENAMIENTO\n",
    "# ====================\n",
    "def train(best_thresholds=Config.THRESHOLDS):\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop = False\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Cargar datos\n",
    "    train_df = pd.read_csv(Config.DATA_PATHS['train'])\n",
    "    val_df = pd.read_csv(Config.DATA_PATHS['val'])\n",
    "\n",
    "    # Construir binarizadores\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Preparar datasets\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        tokenizer.save_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Created new tokenizer\")\n",
    "\n",
    "    train_dataset = HierarchicalMedicalDataset(train_df, tokenizer, mlb_parent, mlb_child)\n",
    "    val_dataset = HierarchicalMedicalDataset(val_df, tokenizer, mlb_parent, mlb_child)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
    "\n",
    "    # Modelo y optimizador\n",
    "    model = HierarchicalBERTv2(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    # Load best model if available\n",
    "    if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "        model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "        print(\"Loaded best model - 2\")\n",
    "    elif os.path.exists(Config.SAVE_PATH):\n",
    "        model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "        print(\"Loaded best model\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = Config.WARMUP_EPOCHS * len(train_loader),\n",
    "        num_training_steps = Config.EPOCHS * len(train_loader)\n",
    "    )\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=Config.USE_FP16)\n",
    "\n",
    "    # Calcular pesos de clases\n",
    "    parent_counts = np.sum(train_dataset.parent_labels, axis=0)\n",
    "    parent_weights = (len(train_dataset) - parent_counts) / (parent_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    parent_weights = torch.tensor(parent_weights).to(device)\n",
    "\n",
    "    child_counts = np.sum(train_dataset.child_labels, axis=0)\n",
    "    child_weights = (len(train_dataset) - child_counts) / (child_counts + Config.CLASS_WEIGHT_SMOOTHING)\n",
    "    child_weights = torch.tensor(child_weights).to(device)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Ajuste periódico de umbrales\n",
    "        if (epoch + 1) % Config.THRESHOLD_TUNING_INTERVAL == 0:\n",
    "            val_probs, val_labels = get_validation_probabilities(model, val_loader, device)\n",
    "\n",
    "            # Calcular mejores umbrales por clase\n",
    "            parent_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['parent'], val_probs['parent']\n",
    "            )\n",
    "            child_thresholds = calculate_optimal_thresholds(\n",
    "                val_labels['child'], val_probs['child']\n",
    "            )\n",
    "\n",
    "            # Actualizar umbrales globales\n",
    "            best_thresholds['parent'] = np.mean(list(parent_thresholds.values()))\n",
    "            best_thresholds['child'] = np.mean(list(child_thresholds.values()))\n",
    "            print(f\"Nuevos umbrales: Parent={best_thresholds['parent']:.3f}, Child={best_thresholds['child']:.3f}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            parent_labels = batch['parent_labels'].to(device)\n",
    "            child_labels = batch['child_labels'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=Config.USE_FP16):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                # Línea corregida\n",
    "                loss = hierarchical_loss(\n",
    "                    outputs[0],  # parent_logits\n",
    "                    outputs[1],  # child_logits\n",
    "                    parent_labels,\n",
    "                    child_labels\n",
    "                )\n",
    "\n",
    "                loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (step + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = evaluate(model, val_loader, device, mlb_parent, mlb_child, best_thresholds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | LR: {scheduler.get_last_lr()[0]:.2E}\")\n",
    "        # Store metrics\n",
    "\n",
    "        loss_metric = val_metrics['f1_macro']\n",
    "        if epoch == 0:\n",
    "            best_f1 = loss_metric\n",
    "\n",
    "        if loss_metric > (best_f1 + Config.IMPROVEMENT_MARGIN):\n",
    "            print(f\"Saving best model... {best_f1:.5f} -> {loss_metric:.5f}\")\n",
    "            torch.save(model.state_dict(), f\"{Config.SAVE_PATH}_3\")\n",
    "            best_f1 = loss_metric\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= Config.EARLY_STOP_PATIENCE:\n",
    "                early_stop = True\n",
    "\n",
    "        metrics_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': total_loss/len(train_loader),\n",
    "            'f1_micro': val_metrics['f1_micro'],\n",
    "            'f1_macro': val_metrics['f1_macro'],\n",
    "            'f1_micro_parent': val_metrics['f1_micro_parent'],\n",
    "            'f1_macro_parent': val_metrics['f1_macro_parent'],\n",
    "            'f1_micro_child': val_metrics['f1_micro_child'],\n",
    "            'f1_macro_child': val_metrics['f1_macro_child'],\n",
    "            'lr': scheduler.get_last_lr()[0],\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'parent_threshold': best_thresholds['parent'],\n",
    "            'child_threshold': best_thresholds['child']\n",
    "        }\n",
    "\n",
    "        # Write metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        if epoch == 0:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', index=False)\n",
    "        else:\n",
    "            metrics_df.to_csv('training_metrics.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"F1 Validation | Micro: {val_metrics['f1_micro']:.5f} | Macro: {val_metrics['f1_macro']:.5f} | Best: {best_f1:.5f} | Epochs without improvement: {epochs_without_improvement + 1}\")\n",
    "\n",
    "# ====================\n",
    "#  FUNCIONES AUXILIARES\n",
    "# ====================\n",
    "def get_validation_probabilities(model, dataloader, device):\n",
    "    model.eval()\n",
    "    parent_probs, child_probs = [], []\n",
    "    parent_labels, child_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            p_logits, c_logits, _ = model(**inputs)\n",
    "\n",
    "            parent_probs.append(torch.sigmoid(p_logits).cpu().numpy())\n",
    "            child_probs.append(torch.sigmoid(c_logits).cpu().numpy())\n",
    "\n",
    "            parent_labels.append(batch['parent_labels'].numpy())\n",
    "            child_labels.append(batch['child_labels'].numpy())\n",
    "\n",
    "    return {\n",
    "        'parent': np.concatenate(parent_probs),\n",
    "        'child': np.concatenate(child_probs)\n",
    "    }, {\n",
    "        'parent': np.concatenate(parent_labels),\n",
    "        'child': np.concatenate(child_labels)\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================\n",
    "#  EVALUACIÓN\n",
    "# ====================\n",
    "def evaluate(model, dataloader, device, mlb_parent, mlb_child, thresholds):\n",
    "    model.eval()\n",
    "    parent_preds_all = []\n",
    "    child_preds_all = []\n",
    "    parent_labels_all = []\n",
    "    child_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                k: v.to(device)\n",
    "                for k, v in batch.items()\n",
    "                if k in ['input_ids', 'attention_mask']\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            parent_labels = batch['parent_labels'].numpy()\n",
    "            child_labels = batch['child_labels'].numpy()\n",
    "\n",
    "            parent_logits, child_logits, pooled = model(**inputs)\n",
    "\n",
    "            # Convert to predictions\n",
    "            parent_preds = (torch.sigmoid(parent_logits).cpu().numpy() > thresholds['parent']).astype(int)\n",
    "            child_preds = (torch.sigmoid(child_logits).cpu().numpy() > thresholds['child']).astype(int)\n",
    "\n",
    "            # Append to lists\n",
    "            parent_preds_all.extend(parent_preds)\n",
    "            child_preds_all.extend(child_preds)\n",
    "            parent_labels_all.extend(parent_labels)\n",
    "            child_labels_all.extend(child_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    parent_preds_all = np.array(parent_preds_all)\n",
    "    child_preds_all = np.array(child_preds_all)\n",
    "    parent_labels_all = np.array(parent_labels_all)\n",
    "    child_labels_all = np.array(child_labels_all)\n",
    "\n",
    "    # Print example comparison for parent level\n",
    "    if len(parent_labels_all) > 0:\n",
    "        parent_true = np.array(mlb_parent.classes_)[parent_labels_all[0].astype(bool)]\n",
    "        parent_pred = np.array(mlb_parent.classes_)[parent_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(parent_true) & set(parent_pred))\n",
    "        total_labels = len(set(parent_true))\n",
    "        accuracy_parent = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        child_true = np.array(mlb_child.classes_)[child_labels_all[0].astype(bool)]\n",
    "        child_pred = np.array(mlb_child.classes_)[child_preds_all[0].astype(bool)]\n",
    "        common_labels = len(set(child_true) & set(child_pred))\n",
    "        total_labels = len(set(child_true))\n",
    "        accuracy_child = common_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "        print(\"Expected parent labels:\", sorted(parent_true))\n",
    "        print(\"Predicted parent labels:\", sorted(parent_pred))\n",
    "        print(\"Expected child labels:\", sorted(child_true))\n",
    "        print(\"Predicted child labels:\", sorted(child_pred))\n",
    "\n",
    "        print(f\"Percentage of correct parent labels: {accuracy_parent:.2%} | {accuracy_child:.2%}\")\n",
    "\n",
    "    # Calculate F1 scores for each level\n",
    "    metrics = {\n",
    "        'f1_micro_parent': f1_score(parent_labels_all, parent_preds_all, average='micro' , zero_division=0),\n",
    "        'f1_macro_parent': f1_score(parent_labels_all, parent_preds_all, average='macro', zero_division=0),\n",
    "        'f1_micro_child': f1_score(child_labels_all, child_preds_all, average='micro', zero_division=0),\n",
    "        'f1_macro_child': f1_score(child_labels_all, child_preds_all, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "    # Calculate weighted average F1 scores\n",
    "    metrics['f1_micro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_micro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_micro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    metrics['f1_macro'] = (\n",
    "        Config.HIERARCHICAL_WEIGHTS['parent'] * metrics['f1_macro_parent'] +\n",
    "        Config.HIERARCHICAL_WEIGHTS['child'] * metrics['f1_macro_child']\n",
    "    ) / sum(Config.HIERARCHICAL_WEIGHTS.values())\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ====================\n",
    "#  PREDICCIÓN\n",
    "# ====================\n",
    "def predict(text, model, tokenizer, mlb_parent, mlb_child, device, thresholds):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        parent_logits, child_logits = model(**encoding)\n",
    "\n",
    "    # Obtener predicciones\n",
    "    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()\n",
    "    child_probs = torch.sigmoid(child_logits).cpu().numpy()\n",
    "\n",
    "    # Decodificar etiquetas\n",
    "    parent_preds = mlb_parent.inverse_transform((parent_probs > thresholds['parent']).astype(int))\n",
    "    child_preds = mlb_child.inverse_transform((child_probs > thresholds['child']).astype(int))\n",
    "\n",
    "    # Combinar y asegurar jerarquía\n",
    "    final_codes = set()\n",
    "    for parent in parent_preds[0]:\n",
    "        final_codes.add(parent)\n",
    "        for child in child_preds[0]:\n",
    "            if child.startswith(parent):\n",
    "                final_codes.add(child)\n",
    "\n",
    "    return sorted(final_codes)\n",
    "\n",
    "# ====================\n",
    "#  EJECUCIÓN\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    best_thresholds = Config.THRESHOLDS\n",
    "\n",
    "    train(best_thresholds)\n",
    "\n",
    "    # Cargar datos de test\n",
    "    test_df = pd.read_csv(Config.DATA_PATHS['test'])\n",
    "    mlb_parent, mlb_child = calculate_mlb_classes()\n",
    "\n",
    "    # Cargar modelo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.SAVE_TOKENIZER_PATH)\n",
    "        print(\"Loaded saved tokenizer\")\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        print(\"Using default tokenizer\")\n",
    "\n",
    "    model = HierarchicalBERT(\n",
    "        len(mlb_parent.classes_),\n",
    "        len(mlb_child.classes_)\n",
    "    ).to(device)\n",
    "\n",
    "    if os.path.exists(f\"{Config.SAVE_PATH}_2\"):\n",
    "        model.load_state_dict(torch.load(f\"{Config.SAVE_PATH}_2\"))\n",
    "        print(\"Loaded best model - 2\")\n",
    "    elif os.path.exists(Config.SAVE_PATH):\n",
    "        model.load_state_dict(torch.load(Config.SAVE_PATH))\n",
    "        print(\"Loaded best model\")\n",
    "\n",
    "    # Evaluar en test\n",
    "    test_dataset = HierarchicalMedicalDataset(test_df, tokenizer, mlb_parent, mlb_child)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.TEST_BATCH_SIZE)\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader, device, mlb_parent, mlb_child)\n",
    "    print(\"\\nResultados en Test:\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Ejemplo de predicción\n",
    "    sample_text = \"Paciente con diabetes mellitus tipo 2 y complicaciones renales...\"\n",
    "    prediction = predict(sample_text, model, tokenizer, mlb_parent, mlb_child, device, best_thresholds)\n",
    "    print(\"\\nPredicción de ejemplo:\", prediction)\n",
    "\n",
    "    plot_metrics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
