{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CIE10 Code Classifier - All-in-One Implementation con Validación\n",
    "\n",
    "Estructura de datos requerida:\n",
    "- train.csv y test.csv deben tener columnas:\n",
    "  * text: Texto descriptivo del diagnóstico\n",
    "  * labels: Lista de códigos CIE10 asociados (formato: \"['cod1', 'cod2', ...]\")\n",
    "- cie10_codes.csv debe tener columnas:\n",
    "  * codigo: Código CIE10\n",
    "  * descripcion: Descripción oficial del código\n",
    "\"\"\"\n",
    "\n",
    "!pip install pandas spacy sentence-transformers scikit-learn transformers deep-translator torch numpy joblib transformers[torch]\n",
    "\n",
    "#!python -m spacy download es_core_news_lg\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import joblib\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm.auto import tqdm  # Barra de progreso mejorada\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Configuración global\n",
    "CIE10_DATA_PATH = '../csv_import_scripts/cie10-es-diagnoses.csv'\n",
    "TRAIN_DATA_PATH = 'codiesp_csvs/codiesp_D_source_train.csv'\n",
    "TEST_DATA_PATH = 'codiesp_csvs/codiesp_D_source_test.csv'\n",
    "VALIDATION_DATA_PATH = 'codiesp_csvs/codiesp_D_source_validation.csv'\n",
    "\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "cie10_df = pd.read_csv(CIE10_DATA_PATH)\n",
    "\n",
    "# Convertir labels a listas\n",
    "print(\"\\nPreprocesando etiquetas...\")\n",
    "train_df['labels'] = train_df['labels'].progress_apply(ast.literal_eval)\n",
    "test_df['labels'] = test_df['labels'].progress_apply(ast.literal_eval)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "class MultiLabelClassificationEvaluator:\n",
    "    def __init__(self, texts, true_labels, model, label_embeddings, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Custom evaluator for multi-label classification.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of input texts.\n",
    "            true_labels (list): List of true multi-label lists.\n",
    "            model (SentenceTransformer): The SentenceTransformer model.\n",
    "            label_embeddings (dict): Dictionary mapping label codes to their embeddings.\n",
    "            threshold (float): Similarity threshold for assigning labels.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.true_labels = true_labels\n",
    "        self.model = model\n",
    "        self.label_embeddings = label_embeddings\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the provided data.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing accuracy, precision, recall, and F1 score.\n",
    "        \"\"\"\n",
    "        # Generate embeddings for the input texts\n",
    "        text_embeddings = model.encode(self.texts)\n",
    "\n",
    "        # Predict labels for each text\n",
    "        predicted_labels = []\n",
    "        for text_embed in tqdm(text_embeddings, desc=\"Predicting labels\", leave=False):\n",
    "            similarities = {\n",
    "                code: cosine_similarity([text_embed], [code_embed])[0][0]\n",
    "                for code, code_embed in self.label_embeddings.items()\n",
    "            }\n",
    "            # Assign labels based on the threshold\n",
    "            predicted_codes = [code for code, sim in similarities.items() if sim >= self.threshold]\n",
    "            predicted_labels.append(predicted_codes)\n",
    "\n",
    "        # Flatten the true and predicted labels for evaluation\n",
    "        true_labels_flattened = [label for sublist in self.true_labels for label in sublist]\n",
    "        predicted_labels_flattened = [label for sublist in predicted_labels for label in sublist]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        print(\"calculando scores\")\n",
    "        accuracy = accuracy_score(true_labels_flattened, predicted_labels_flattened)\n",
    "        precision = precision_score(true_labels_flattened, predicted_labels_flattened, average='micro')\n",
    "        recall = recall_score(true_labels_flattened, predicted_labels_flattened, average='micro')\n",
    "        f1 = f1_score(true_labels_flattened, predicted_labels_flattened, average='micro')\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción 1: Sentence Transformers con Pairwise Training\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "class SentenceTransformerCIE10:\n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.cie10_df, self.code_embeddings = self._prepare_code_embeddings()\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "\n",
    "    def _prepare_code_embeddings(self):\n",
    "        print(\"Prepara embeddings para los códigos CIE10\")\n",
    "        cie10_df = pd.read_csv(CIE10_DATA_PATH)\n",
    "        cie10_df['clean_desc'] = cie10_df['description'].str.replace('NEOM', '').str.strip()\n",
    "        code_embeddings = {}\n",
    "\n",
    "        # Process descriptions in batches for better performance\n",
    "        batch_size = 32  # Adjust batch size based on available memory\n",
    "        descriptions = cie10_df['clean_desc'].tolist()\n",
    "        embeddings = []\n",
    "\n",
    "        for i in tqdm(range(0, len(descriptions), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch = descriptions[i:i + batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        # Create dictionary mapping codes to embeddings\n",
    "        code_embeddings = dict(zip(cie10_df['code'], embeddings))\n",
    "\n",
    "        return cie10_df, code_embeddings\n",
    "\n",
    "\n",
    "    def _generate_triplets(self, texts, labels):\n",
    "        print(\"Genera tripletas (anchor, positive, negative)\")\n",
    "        triplets = []\n",
    "        all_codes = list(self.code_embeddings.keys())\n",
    "\n",
    "        for text, codes in zip(texts, labels):\n",
    "            for pos_code in codes:\n",
    "                print(f\"Generando tripletas para código {pos_code}\")\n",
    "                pos_desc = self.cie10_df[self.cie10_df['code'] == pos_code.upper()]['clean_desc'].values[0]\n",
    "                triplets.append(InputExample(texts=[text, pos_desc], label=1.0))\n",
    "\n",
    "                negative_codes = [c for c in all_codes if c not in codes]\n",
    "                neg_code = np.random.choice(negative_codes)\n",
    "                neg_desc = self.cie10_df[self.cie10_df['code'] == neg_code.upper()]['clean_desc'].values[0]\n",
    "                triplets.append(InputExample(texts=[text, neg_desc], label=0.0))\n",
    "\n",
    "        return triplets\n",
    "\n",
    "    def train(self, train_df, val_df=None, epochs=5, batch_size=32):\n",
    "        print(\"Entrena el modelo con validación\")\n",
    "        # Preparar datos\n",
    "        self.mlb.fit(train_df['labels'])\n",
    "\n",
    "        # Dividir datos si no se proporciona val_df\n",
    "        if val_df is None:\n",
    "            train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Generar tripletas\n",
    "        train_triplets = self._generate_triplets(train_df['text'].tolist(), train_df['labels'].tolist())\n",
    "        #val_triplets = self._generate_triplets(val_df['text'].tolist(), val_df['labels'].tolist())\n",
    "\n",
    "        # Prepare custom evaluator\n",
    "        val_texts = val_df['text'].tolist()\n",
    "        val_true_labels = val_df['labels'].tolist()\n",
    "        evaluator = MultiLabelClassificationEvaluator(val_texts, val_true_labels, self.model, self.code_embeddings)\n",
    "\n",
    "        # Entrenamiento con barra de progreso\n",
    "        train_loader = DataLoader(train_triplets, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.CosineSimilarityLoss(self.model)\n",
    "\n",
    "        best_score = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            self.model.fit(\n",
    "                train_objectives=[(train_loader, train_loss)],\n",
    "                #evaluator=evaluator,\n",
    "                epochs=1,\n",
    "                warmup_steps=100,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "\n",
    "            # Evaluación\n",
    "            score = evaluator(self.model)\n",
    "            print(f\"Validation Score: {score:.4f}\")\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                self.model.save('./best_sbert_model')\n",
    "\n",
    "        # Cargar mejor modelo\n",
    "        self.model = SentenceTransformer('./best_sbert_model')\n",
    "\n",
    "    def predict(self, text, threshold=0.4, top_k=5):\n",
    "        print(\"Predice códigos para un texto dado\")\n",
    "        text_embed = self.model.encode([text])\n",
    "        similarities = {\n",
    "            code: cosine_similarity([text_embed], [code_embed])[0][0]\n",
    "            for code, code_embed in self.code_embeddings.items()\n",
    "        }\n",
    "\n",
    "        sorted_codes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        results = []\n",
    "        for code, sim in sorted_codes:\n",
    "            if sim >= threshold and len(results) < top_k:\n",
    "                results.append({\n",
    "                    'codigo': code,\n",
    "                    'descripcion': self.cie10_df[self.cie10_df['code'] == code]['description'].values[0],\n",
    "                    'similitud': float(sim)\n",
    "                })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformerCIE10()\n",
    "st_model.train(train_df, epochs=5)\n",
    "\n",
    "# Predicción\n",
    "test_text = \"Paciente con fiebre alta y dolor articular\"\n",
    "resultados = st_model.predict(test_text)\n",
    "for res in resultados:\n",
    "    print(f\"Código: {res['codigo']} - Similitud: {res['similitud']:.2f}\")\n",
    "    print(f\"Descripción: {res['descripcion']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Opción 2: BERT Clasificador Multi-Etiqueta\n",
    "# ==================================================================\n",
    "class BERTMultiLabelCIE10:\n",
    "    def __init__(self, model_name='dccuchile/bert-base-spanish-wwm-cased'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self._prepare_labels()\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        \"\"\"Prepara el binarizador de etiquetas\"\"\"\n",
    "        cie10_df = pd.read_csv(CIE10_DATA_PATH)\n",
    "        self.mlb.fit([cie10_df['codigo'].tolist()])\n",
    "\n",
    "    def train(self, train_path=TRAIN_DATA_PATH, epochs=3, batch_size=16):\n",
    "        \"\"\"Entrena el modelo BERT\"\"\"\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        train_df['codigos'] = train_df['codigos'].apply(ast.literal_eval)\n",
    "        y_train = self.mlb.transform(train_df['codigos'])\n",
    "\n",
    "        class Cie10Dataset(Dataset):\n",
    "            def __init__(self, texts, labels, tokenizer):\n",
    "                self.texts = texts\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "\n",
    "            def __len__(self): return len(self.texts)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                encoding = self.tokenizer(\n",
    "                    self.texts[idx],\n",
    "                    max_length=128,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.FloatTensor(self.labels[idx])\n",
    "                }\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.tokenizer.name_or_path,\n",
    "            num_labels=len(self.mlb.classes_),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "        train_dataset = Cie10Dataset(train_df['descripcion'].tolist(), y_train, self.tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./bert_results',\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            save_strategy='epoch'\n",
    "        )\n",
    "\n",
    "        Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset\n",
    "        ).train()\n",
    "\n",
    "    def predict(self, text, threshold=0.3):\n",
    "        \"\"\"Realiza predicciones\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits)\n",
    "        predicted_labels = (probs > threshold).int().flatten().tolist()\n",
    "        return self.mlb.inverse_transform([predicted_labels])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Opción 3: TF-IDF + Random Forest\n",
    "# ==================================================================\n",
    "class TfidfRFCIE10:\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "\n",
    "    def train(self, train_path=TRAIN_DATA_PATH):\n",
    "        \"\"\"Entrena el pipeline TF-IDF + Random Forest\"\"\"\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        train_df['codigos'] = train_df['codigos'].apply(ast.literal_eval)\n",
    "        y_train = self.mlb.fit_transform(train_df['codigos'])\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "            ('clf', OneVsRestClassifier(RandomForestClassifier(n_estimators=100)))\n",
    "        ])\n",
    "\n",
    "        self.pipeline.fit(train_df['descripcion'], y_train)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Guarda el modelo\"\"\"\n",
    "        joblib.dump({'pipeline': self.pipeline, 'mlb': self.mlb}, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Carga el modelo\"\"\"\n",
    "        data = joblib.load(path)\n",
    "        self.pipeline = data['pipeline']\n",
    "        self.mlb = data['mlb']\n",
    "\n",
    "    def predict(self, text, threshold=0.2):\n",
    "        \"\"\"Predice códigos\"\"\"\n",
    "        probs = self.pipeline.predict_proba([text])\n",
    "        return self.mlb.inverse_transform(probs > threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Opción 4: Ensemble Híbrido\n",
    "# ==================================================================\n",
    "class EnsembleCIE10:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(max_features=2000)\n",
    "        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.model = RandomForestClassifier(n_estimators=100)\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "\n",
    "    def train(self, train_path=TRAIN_DATA_PATH):\n",
    "        \"\"\"Entrena el modelo ensemble\"\"\"\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        train_df['codigos'] = train_df['codigos'].apply(ast.literal_eval)\n",
    "        y_train = self.mlb.fit_transform(train_df['codigos'])\n",
    "\n",
    "        # Generar features\n",
    "        X_tfidf = self.tfidf.fit_transform(train_df['descripcion']).toarray()\n",
    "        X_semantic = self.embedder.encode(train_df['descripcion'])\n",
    "        X_combined = np.concatenate([X_tfidf, X_semantic], axis=1)\n",
    "\n",
    "        self.model.fit(X_combined, y_train)\n",
    "\n",
    "    def predict(self, text, threshold=0.25):\n",
    "        \"\"\"Realiza predicciones\"\"\"\n",
    "        tfidf_feat = self.tfidf.transform([text]).toarray()\n",
    "        semantic_feat = self.embedder.encode([text])\n",
    "        combined = np.concatenate([tfidf_feat, semantic_feat], axis=1)\n",
    "        probs = self.model.predict_proba(combined)\n",
    "        return self.mlb.inverse_transform(probs > threshold)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Opción 5: Modelo Biomédico con Traducción\n",
    "# ==================================================================\n",
    "class BioTranslatedCIE10:\n",
    "    def __init__(self):\n",
    "        self.translator = GoogleTranslator(source='es', target='en')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.code_descs = pd.read_csv(CIE10_DATA_PATH).set_index('codigo')['descripcion'].to_dict()\n",
    "\n",
    "    def _translate(self, text):\n",
    "        \"\"\"Traduce texto médico español→inglés\"\"\"\n",
    "        try: return self.translator.translate(text)\n",
    "        except: return text  # Fallback\n",
    "\n",
    "    def train(self, train_path=TRAIN_DATA_PATH, epochs=3):\n",
    "        \"\"\"Entrena el modelo con datos traducidos\"\"\"\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        train_df['codigos'] = train_df['codigos'].apply(ast.literal_eval)\n",
    "        train_df['text_en'] = train_df['descripcion'].apply(self._translate)\n",
    "\n",
    "        self.mlb.fit(train_df['codigos'])\n",
    "        y_train = self.mlb.transform(train_df['codigos'])\n",
    "\n",
    "        class MedicalDataset(Dataset):\n",
    "            def __init__(self, texts, labels, tokenizer):\n",
    "                self.texts = texts\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "\n",
    "            def __len__(self): return len(self.texts)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                encoding = self.tokenizer(\n",
    "                    self.texts[idx],\n",
    "                    max_length=128,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.FloatTensor(self.labels[idx])\n",
    "                }\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "            num_labels=len(self.mlb.classes_),\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=TrainingArguments(\n",
    "                output_dir='./biobert',\n",
    "                num_train_epochs=epochs,\n",
    "                per_device_train_batch_size=16,\n",
    "                fp16=True\n",
    "            ),\n",
    "            train_dataset=MedicalDataset(train_df['text_en'].tolist(), y_train, self.tokenizer)\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "    def predict(self, text, threshold=0.3):\n",
    "        \"\"\"Predice traduciendo al inglés\"\"\"\n",
    "        translated = self._translate(text)\n",
    "        inputs = self.tokenizer(\n",
    "            translated,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        outputs = self.model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits)\n",
    "        return self.mlb.inverse_transform((probs > threshold).int().numpy())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Opción 2\n",
    "    bert_model = BERTMultiLabelCIE10()\n",
    "    bert_model.train()\n",
    "    print(\"Opción 2:\", bert_model.predict(test_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Opción 3\n",
    "    tfidf_model = TfidfRFCIE10()\n",
    "    tfidf_model.train()\n",
    "    print(\"Opción 3:\", tfidf_model.predict(test_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Opción 4\n",
    "    ensemble_model = EnsembleCIE10()\n",
    "    ensemble_model.train()\n",
    "    print(\"Opción 4:\", ensemble_model.predict(test_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Opción 5 (Requiere API Key de Google Translate)\n",
    "    bio_model = BioTranslatedCIE10()\n",
    "    bio_model.train()\n",
    "    print(\"Opción 5:\", bio_model.predict(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultima opción?\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Cargar los datos\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "val_df = pd.read_csv(VALIDATION_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Convertir las etiquetas alfanuméricas a listas\n",
    "def convert_labels_to_list(label_str):\n",
    "    return label_str.split(\",\")  # Suponiendo que las etiquetas están separadas por comas\n",
    "\n",
    "train_df['labels'] = train_df['labels'].apply(convert_labels_to_list)\n",
    "val_df['labels'] = val_df['labels'].apply(convert_labels_to_list)\n",
    "test_df['labels'] = test_df['labels'].apply(convert_labels_to_list)\n",
    "\n",
    "# Convertir las etiquetas a codificación one-hot\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_df['labels'])\n",
    "val_labels = mlb.transform(val_df['labels'])\n",
    "test_labels = mlb.transform(test_df['labels'])\n",
    "\n",
    "# Cargar el modelo de Sentence Transformers (usamos un modelo multilingüe)\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"  # Modelo en español\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Preparar los datos para Pairwise Training\n",
    "def create_pairwise_examples(df, labels):\n",
    "    examples = []\n",
    "    for i, text in enumerate(df['text']):\n",
    "        # Crear pares positivos (mismo texto consigo mismo)\n",
    "        examples.append(InputExample(texts=[text, text], label=1.0))\n",
    "        # Crear pares negativos (texto con otro texto aleatorio)\n",
    "        if len(df) > 1:\n",
    "            random_idx = torch.randint(0, len(df), (1,)).item()\n",
    "            while random_idx == i:\n",
    "                random_idx = torch.randint(0, len(df), (1,)).item()\n",
    "            examples.append(InputExample(texts=[text, df.iloc[random_idx]['text']], label=0.0))\n",
    "    return examples\n",
    "\n",
    "train_examples = create_pairwise_examples(train_df, train_labels)\n",
    "val_examples = create_pairwise_examples(val_df, val_labels)\n",
    "\n",
    "# Crear DataLoader para Pairwise Training\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "# Definir la función de pérdida (Pairwise Loss)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Crear un evaluador personalizado\n",
    "class CustomEvaluator:\n",
    "    def __init__(self, texts, labels, model, mlb):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.model = model\n",
    "        self.mlb = mlb\n",
    "\n",
    "    def __call__(self, model, output_path=None, epoch=None, steps=None):\n",
    "        # Generar embeddings para los textos\n",
    "        embeddings = model.encode(self.texts, convert_to_tensor=True)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        # Usar un clasificador para predecir las etiquetas\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(embeddings, self.labels)\n",
    "        preds = classifier.predict(embeddings)\n",
    "        # Calcular métricas\n",
    "        accuracy = accuracy_score(self.labels, preds)\n",
    "        f1 = f1_score(self.labels, preds, average=\"micro\")\n",
    "        precision = precision_score(self.labels, preds, average=\"micro\")\n",
    "        recall = recall_score(self.labels, preds, average=\"micro\")\n",
    "        # Mostrar métricas\n",
    "        print(f\"Epoch {epoch}, Steps {steps}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "        return accuracy\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator = CustomEvaluator(val_df['text'].tolist(), val_labels, model, mlb)\n",
    "\n",
    "# Entrenar el modelo con Pairwise Training\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% de warmup steps\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,  # Añadir el evaluador\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=\"./pairwise_model\",\n",
    "    evaluation_steps=100,  # Evaluar cada 100 pasos\n",
    "    show_progress_bar=True,  # Mostrar barra de progreso\n",
    ")\n",
    "\n",
    "# Generar embeddings con el modelo fine-tuneado\n",
    "train_embeddings = model.encode(train_df['text'].tolist(), convert_to_tensor=True)\n",
    "val_embeddings = model.encode(val_df['text'].tolist(), convert_to_tensor=True)\n",
    "test_embeddings = model.encode(test_df['text'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Entrenar un clasificador (Logistic Regression)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_embeddings.cpu().numpy(), train_labels)\n",
    "\n",
    "# Evaluar en el conjunto de validación\n",
    "val_preds = classifier.predict(val_embeddings.cpu().numpy())\n",
    "print(\"Resultados en validación:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=mlb.classes_))\n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "test_preds = classifier.predict(test_embeddings.cpu().numpy())\n",
    "print(\"Resultados en prueba:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=mlb.classes_))\n",
    "\n",
    "# Guardar el modelo y el clasificador\n",
    "import joblib\n",
    "model.save(\"./pairwise_model\")\n",
    "joblib.dump(classifier, \"./classifier.pkl\")\n",
    "joblib.dump(mlb, \"./mlb.pkl\")  # Guardar el MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuración inicial\n",
    "# = 'PlanTL-GOB-ES/bsc-bio-ehr-es-quasimodo'\n",
    "MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "TEST_BATCH_SIZE = 32\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VAL_BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 200\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "HIERARCHICAL_WEIGHTS = {'parent': 1.5, 'child': 1.0}  # Peso mayor para categorías padre\n",
    "TRAIN_DATA_PATH = 'codiesp_csvs/codiesp_D_source_train.csv'\n",
    "TEST_DATA_PATH = 'codiesp_csvs/codiesp_D_source_test.csv'\n",
    "VALIDATION_DATA_PATH = 'codiesp_csvs/codiesp_D_source_validation.csv'\n",
    "\n",
    "# Cargar datos\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "val_df = pd.read_csv(VALIDATION_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Preprocesamiento jerárquico de etiquetas\n",
    "def extract_hierarchy(code):\n",
    "    if not isinstance(code, str):\n",
    "        return '', ''  # Return empty strings for non-string inputs\n",
    "    match = re.match(r\"([A-Z0-9])[.]?(\\d+[A-Z]*)?\", code.upper())\n",
    "    if match:\n",
    "        parent = match.group(1)\n",
    "        child = match.group(2)\n",
    "        return parent, child\n",
    "    return code, \"*\"  # Return the code as parent and child as *\n",
    "\n",
    "all_labels = set()\n",
    "hierarchy_map = defaultdict(set)\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for labels in df['labels'].apply(eval):\n",
    "        for code in labels:\n",
    "            parent, child = extract_hierarchy(code)\n",
    "            all_labels.add(child)\n",
    "            all_labels.add(parent)  # Incluir padres como etiquetas independientes\n",
    "            hierarchy_map[child].add(parent)\n",
    "\n",
    "# Crear mapeo de etiquetas con estructura jerárquica\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "\n",
    "# Dataset mejorado con jerarquía\n",
    "class HierarchicalClinicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mlb, max_length):\n",
    "        self.texts = df['text'].values.tolist()\n",
    "        self.labels = [self._process_labels(eval(l)) for l in df['labels']]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mlb = mlb\n",
    "\n",
    "    def _process_labels(self, labels):\n",
    "        # Añadir padres automáticamente\n",
    "        processed = set()\n",
    "        for code in labels:\n",
    "            parent, child = extract_hierarchy(code)\n",
    "            processed.add(child)\n",
    "            processed.add(parent)\n",
    "        return list(processed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.mlb.transform([self.labels[idx]]).astype(float).flatten()\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }\n",
    "\n",
    "# Inicializar modelo y componentes\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Función de pérdida con pesos jerárquicos\n",
    "class HierarchicalBCELoss(torch.nn.Module):\n",
    "    def __init__(self, hierarchy_map, mlb, weight=None):\n",
    "        super().__init__()\n",
    "        self.base_loss = torch.nn.BCEWithLogitsLoss(weight=weight)\n",
    "        self.hierarchy_map = hierarchy_map\n",
    "        self.classes = mlb.classes_\n",
    "        self.class_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        base_loss = self.base_loss(outputs, targets)\n",
    "\n",
    "        # Penalización adicional por errores jerárquicos\n",
    "        batch_size = targets.size(0)\n",
    "        hierarchical_loss = 0.0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            predicted = torch.sigmoid(outputs[i]) > 0.5\n",
    "            true_labels = targets[i].bool()\n",
    "\n",
    "            for idx, label in enumerate(self.classes):\n",
    "                if true_labels[idx]:\n",
    "                    # Verificar si los padres están presentes\n",
    "                    parents = self.hierarchy_map.get(label, set())\n",
    "                    for parent in parents:\n",
    "                        parent_idx = self.class_index.get(parent)\n",
    "                        if parent_idx is not None and not predicted[parent_idx]:\n",
    "                            hierarchical_loss += HIERARCHICAL_WEIGHTS['parent']\n",
    "\n",
    "        return base_loss + hierarchical_loss / batch_size\n",
    "\n",
    "# Preparar datasets\n",
    "train_dataset = HierarchicalClinicalDataset(train_df, tokenizer, mlb, MAX_LENGTH)\n",
    "val_dataset = HierarchicalClinicalDataset(val_df, tokenizer, mlb, MAX_LENGTH)\n",
    "test_dataset = HierarchicalClinicalDataset(test_df, tokenizer, mlb, MAX_LENGTH)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "# Configurar entrenamiento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "# Calculate warmup steps and total steps\n",
    "warmup_epochs = 100\n",
    "boost_epochs = 25\n",
    "warmup_steps = len(train_loader) * warmup_epochs\n",
    "boost_steps = len(train_loader) * boost_epochs\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < boost_steps:\n",
    "        lr = 10e-5 + 5e-5 * (1- (current_step / boost_steps))\n",
    "    elif current_step < warmup_steps:\n",
    "        lr = 5e-5\n",
    "    else:\n",
    "        # After warmup: decrease from 3e-5 to 2e-5 linearly\n",
    "        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        lr = 3e-5 - (1e-5 * progress)\n",
    "    return lr\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "loss_fn = HierarchicalBCELoss(hierarchy_map, mlb)\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.logits).cpu().detach().numpy()\n",
    "            predictions.extend(preds > 0.5)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "# Bucle de entrenamiento con barra de progreso y evaluación\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Entrenando época {epoch+1}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluación después de cada época\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    print(f\"\\nÉpoca {epoch+1} - Loss: {epoch_loss / len(train_loader):.4f} - LR: {scheduler.get_last_lr()[0]}\")\n",
    "    print(f\"Validación - Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"F1 Micro: {val_metrics['f1_micro']:.4f}, F1 Macro: {val_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if val_metrics['f1_micro'] > best_f1:\n",
    "        best_f1 = val_metrics['f1_micro']\n",
    "        print(f\"Guardando mejor modelo... BEST ONE!!!!!! {val_metrics['f1_micro']:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch.bin\")\n",
    "\n",
    "\n",
    "# Evaluación final en test\n",
    "print(\"\\nEvaluando en conjunto de test...\")\n",
    "test_metrics = evaluate(model, test_loader)\n",
    "print(f\"Test - Loss: {test_metrics['loss']:.4f} - LR: {scheduler.get_last_lr()[0]}\")\n",
    "print(f\"F1 Micro: {test_metrics['f1_micro']:.4f}, F1 Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "# Generar reporte de clasificación con etiquetas reales\n",
    "y_true = test_dataset.labels\n",
    "y_pred = model.predict(test_loader)  # Necesitarías implementar esta función\n",
    "print(classification_report(y_true, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
