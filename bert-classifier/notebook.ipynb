{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a BERT Model for Medical Diagnoses Classification\n",
    "This notebook demonstrates how to train a BERT model to classify medical diagnoses based on their descriptions and CIE-10 codes. It includes steps for loading data, preprocessing, training, evaluation, and querying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn torch transformers datasets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "We need several libraries for data manipulation, model training, and evaluation.\n",
    "- `pandas` and `numpy` for data manipulation.\n",
    "- `scikit-learn` for data splitting.\n",
    "- `torch` for PyTorch, the deep learning framework.\n",
    "- `transformers` for BERT model and tokenizer.\n",
    "- `datasets` for handling datasets.\n",
    "- `matplotlib` for plotting graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training Dataset\n",
    "We load the training dataset containing medical diagnoses descriptions and their corresponding CIE-10 codes.\n",
    "- `pd.read_csv` is used to read the CSV file into a DataFrame.\n",
    "- We select only the relevant columns (`description` and `code`).\n",
    "- We rename the columns to `text` and `label` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "df_train = pd.read_csv('../csv_import_scrips/cie10-es-diagnoses.csv')\n",
    "df_train = df_train[['description', 'code']]\n",
    "df_train = df_train.rename(columns={'description': 'text', 'code': 'label'})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Dataset\n",
    "We load a separate evaluation dataset to validate the model's performance.\n",
    "- Similar steps are followed as for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-eval-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "df_eval = pd.read_csv('../generated-diagnoses/diagnosticos_medicos_10000.csv')\n",
    "df_eval = df_eval[['Diagnóstico', 'CIE-10']]\n",
    "df_eval = df_eval.rename(columns={'Diagnóstico': 'text', 'CIE-10': 'label'})\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "We convert the CIE-10 codes to categorical labels and create a mapping from label indices back to CIE-10 codes.\n",
    "- Convert the `label` column to a categorical type.\n",
    "- Create a dictionary to map label indices to CIE-10 codes.\n",
    "- Convert the categorical labels to numerical codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_train['label'] = df_train['label'].astype('category')\n",
    "df_eval['label'] = df_eval['label'].astype('category')\n",
    "label_to_code = dict(enumerate(df_train['label'].cat.categories))\n",
    "df_train['label'] = df_train['label'].cat.codes\n",
    "df_eval['label'] = df_eval['label'].cat.codes\n",
    "df_train.head(), df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode Data\n",
    "We use the BERT tokenizer to tokenize and encode the text data.\n",
    "- Load the BERT tokenizer.\n",
    "- Define a function to tokenize the text data.\n",
    "- Convert the DataFrame to a Dataset object.\n",
    "- Apply the tokenizer to the dataset.\n",
    "- Use `DataCollatorWithPadding` to handle padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or Train the Model\n",
    "We check if a trained model already exists. If it does, we load it. Otherwise, we train a new model.\n",
    "- Check if the model directory exists.\n",
    "- If it exists, load the model and tokenizer from the saved files.\n",
    "- If it doesn't exist, load a pre-trained BERT model and fine-tune it on our dataset.\n",
    "- Save the trained model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-or-train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is already trained and saved\n",
    "model_path = './trained_model'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load the saved model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    print('Model loaded from saved files.')\n",
    "else:\n",
    "    # Load pre-trained BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df_train['label'].unique()))\n",
    "    \n",
    "    # Fine-tune BERT model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print('Model trained and saved.')\n",
    "\n",
    "# Define the trainer for evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "We evaluate the model's performance on the evaluation dataset.\n",
    "- Use the `evaluate` method of the `Trainer` class to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training and Validation Loss\n",
    "We plot the training and validation loss to visualize the model's performance over epochs.\n",
    "- Extract the training and validation loss from the training metrics.\n",
    "- Plot the losses using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "training_metrics = trainer.state.log_history\n",
    "losses = [x['loss'] for x in training_metrics if 'loss' in x]\n",
    "eval_losses = [x['eval_loss'] for x in training_metrics if 'eval_loss' in x]\n",
    "epochs = range(1, len(losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Model\n",
    "We provide examples of how to query the model with new diagnoses descriptions and get the predicted CIE-10 codes.\n",
    "- Define a `predict` function to process the input text and get the model's prediction.\n",
    "- Ensure the inputs are on the same device as the model.\n",
    "- Map the predicted label index back to the CIE-10 code using the `label_to_code` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries to the model\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Ensure inputs are on the same device as the model\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return label_to_code[predictions.item()]\n",
    "\n",
    "# Example queries\n",
    "examples = [\n",
    "    'Cambio en cerebro, de dispositivo de drenaje, abordaje externo',\n",
    "    'Escisión de cerebro, diagnóstico, abordaje abierto'\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    label = predict(example)\n",
    "    print(f'Text: {example}\\nPredicted Label: {label}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
